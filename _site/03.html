<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Investing in AI - UKRI - Topic Modeling - Part 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Investing in AI - UKRI</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02.html">Analysis</a></li><li class="breadcrumb-item"><a href="./03.html">Topic Modeling - Part 1</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Getting Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Data</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Trend and Elite Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Topic Modeling - Part 1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#cleaning" id="toc-cleaning" class="nav-link" data-scroll-target="#cleaning">Cleaning</a>
  <ul class="collapse">
  <li><a href="#how-well-does-the-two-data-set-join" id="toc-how-well-does-the-two-data-set-join" class="nav-link" data-scroll-target="#how-well-does-the-two-data-set-join">How well does the two data set join</a></li>
  <li><a href="#linkedduplicated-project" id="toc-linkedduplicated-project" class="nav-link" data-scroll-target="#linkedduplicated-project">Linked/Duplicated project</a></li>
  </ul></li>
  <li><a href="#word-count-and-keyword-summary" id="toc-word-count-and-keyword-summary" class="nav-link" data-scroll-target="#word-count-and-keyword-summary">Word Count and Keyword Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02.html">Analysis</a></li><li class="breadcrumb-item"><a href="./03.html">Topic Modeling - Part 1</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Topic Modeling - Part 1</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<div class="cell">
<details class="code-fold">
<summary>set up and load data</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(readr)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(tidyr)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(arrow)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(tidytext)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggrepel)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(gghighlight)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>gtr_desc <span class="ot">=</span> <span class="fu">read_parquet</span>(<span class="st">"data/gtr.parquet"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>gtr_meta <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"data/projectsearch-1709481069771.csv"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">ends_with</span>(<span class="st">"Date"</span>), <span class="sc">~</span><span class="fu">as.Date</span>(.x,<span class="st">"%d/%m/%Y"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="cleaning" class="level2">
<h2 class="anchored" data-anchor-id="cleaning">Cleaning</h2>
<p><strong>overview</strong></p>
<ul>
<li>Majority of data and link can join</li>
<li>Description and Title contains duplication:
<ul>
<li>This is due to <strong>project migration</strong> when person of interests changes occupation;</li>
<li>When this happens the <code>ProjectReference</code> will ends with a <code>slash</code>.</li>
</ul></li>
</ul>
<section id="how-well-does-the-two-data-set-join" class="level3">
<h3 class="anchored" data-anchor-id="how-well-does-the-two-data-set-join">How well does the two data set join</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>JOIN_META_KEY <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"id"</span><span class="ot">=</span><span class="st">"ProjectId"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>desc_key <span class="ot">=</span> gtr_desc[[<span class="fu">names</span>(JOIN_META_KEY)]]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>meta_key <span class="ot">=</span> gtr_meta[[JOIN_META_KEY[[<span class="dv">1</span>]]]]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">stopifnot</span>(</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="sc">!</span><span class="fu">any</span>(<span class="fu">duplicated</span>(desc_key)),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="sc">!</span><span class="fu">any</span>(<span class="fu">duplicated</span>(meta_key))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>can_join <span class="ot">=</span> <span class="fu">intersect</span>(desc_key, meta_key)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>desc_cant_join <span class="ot">=</span> <span class="fu">setdiff</span>(desc_key, meta_key)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>meta_cant_join<span class="ot">=</span> <span class="fu">setdiff</span>(meta_key, desc_key)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>pcg <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">length</span>(can_join) <span class="sc">/</span> <span class="fu">nrow</span>(gtr_desc) <span class="sc">*</span> <span class="dv">100</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(glue<span class="sc">::</span><span class="fu">glue</span>(</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">"{ pcg } % of description can find matching porject id in the csv export;</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  <span class="st">"</span><span class="sc">\n</span><span class="st">Numbers breakdown:</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="st">"</span><span class="sc">\t</span><span class="st"> - {length(can_join)} can join;</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  <span class="st">"</span><span class="sc">\t</span><span class="st"> - {length(desc_cant_join)} description will be taken out;</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="st">"</span><span class="sc">\t</span><span class="st"> - {length(meta_cant_join)} from csv file will be taken out;</span><span class="sc">\n</span><span class="st">"</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>86 % of description can find matching porject id in the csv export;

Numbers breakdown:
     - 4453 can join;
     - 722 description will be taken out;
     - 722 from csv file will be taken out;</code></pre>
</div>
</div>
</section>
<section id="linkedduplicated-project" class="level3">
<h3 class="anchored" data-anchor-id="linkedduplicated-project">Linked/Duplicated project</h3>
<div class="cell">
<details class="code-fold">
<summary>partial project or migrated project</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## migrated project consist of this pattern</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>PARTIAL_PTN_DCT<span class="ot">=</span><span class="st">'(/[1-9])$'</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="do">## waggle some columns for analytics</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>gtr_pj <span class="ot">=</span> gtr_meta <span class="sc">|&gt;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">is_partial =</span> <span class="fu">str_detect</span>(ProjectReference, PARTIAL_PTN_DCT),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">project_ref =</span> <span class="fu">str_replace</span>(ProjectReference,PARTIAL_PTN_DCT,<span class="st">""</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">part =</span> <span class="fu">str_extract</span>(ProjectReference, PARTIAL_PTN_DCT) <span class="sc">|&gt;</span> </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">str_extract</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">d+"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.numeric</span>() <span class="sc">|&gt;</span> </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">coalesce</span>(<span class="dv">0</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span> </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># filter(is_partial) |&gt; </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(project_ref) <span class="sc">|&gt;</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">occurance =</span> <span class="fu">n</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">|&gt;</span> </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">relocate</span>(ProjectReference, FundingOrgName, LeadROName, ProjectId, </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>           is_partial,project_ref,part,occurance</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>           )</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="do">## early stop if this is no longer true</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="fu">stopifnot</span>(</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Project Reference is Unique!"</span><span class="ot">=</span><span class="fu">length</span>(<span class="fu">unique</span>(gtr_pj<span class="sc">$</span>ProjectReference)) <span class="sc">==</span> <span class="fu">nrow</span>(gtr_pj),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Project Refrence contain null!"</span><span class="ot">=</span><span class="sc">!</span><span class="fu">any</span>(<span class="fu">is.na</span>(gtr_pj<span class="sc">$</span>ProjectReference))</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="do">## a lot of these are false duplicate? or have they simply not been included in </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="do">## the project? </span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>gtr_pj <span class="sc">|&gt;</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(occurance) <span class="sc">|&gt;</span> </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">n_projects=</span><span class="fu">n</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["occurance"],"name":[1],"type":["int"],"align":["right"]},{"label":["n_projects"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"1","2":"4984"},{"1":"2","2":"182"},{"1":"3","2":"9"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<details class="code-fold">
<summary>partial project or migrated project</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## actually majority of these project will false alert</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>examples of inheritance projects</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>smp<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">94</span>,<span class="dv">1</span>)<span class="co"># 9/3 + 183/2 </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>gtr_pj <span class="sc">|&gt;</span> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(occurance <span class="sc">!=</span><span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(project_ref) <span class="sc">|&gt;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">cur_group_id</span>() <span class="sc">==</span> smp) <span class="sc">|&gt;</span> </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(<span class="fu">select</span>(gtr_desc, id,abstractText,title), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"ProjectId"</span><span class="ot">=</span><span class="st">"id"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["ProjectReference"],"name":[1],"type":["chr"],"align":["left"]},{"label":["FundingOrgName"],"name":[2],"type":["chr"],"align":["left"]},{"label":["LeadROName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["ProjectId"],"name":[4],"type":["chr"],"align":["left"]},{"label":["is_partial"],"name":[5],"type":["lgl"],"align":["right"]},{"label":["project_ref"],"name":[6],"type":["chr"],"align":["left"]},{"label":["part"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["occurance"],"name":[8],"type":["int"],"align":["right"]},{"label":["Department"],"name":[9],"type":["chr"],"align":["left"]},{"label":["ProjectCategory"],"name":[10],"type":["chr"],"align":["left"]},{"label":["PISurname"],"name":[11],"type":["chr"],"align":["left"]},{"label":["PIFirstName"],"name":[12],"type":["chr"],"align":["left"]},{"label":["PIOtherNames"],"name":[13],"type":["lgl"],"align":["right"]},{"label":["PI ORCID iD"],"name":[14],"type":["chr"],"align":["left"]},{"label":["StudentSurname"],"name":[15],"type":["chr"],"align":["left"]},{"label":["StudentFirstName"],"name":[16],"type":["chr"],"align":["left"]},{"label":["StudentOtherNames"],"name":[17],"type":["lgl"],"align":["right"]},{"label":["Student ORCID iD"],"name":[18],"type":["chr"],"align":["left"]},{"label":["Title"],"name":[19],"type":["chr"],"align":["left"]},{"label":["StartDate"],"name":[20],"type":["date"],"align":["right"]},{"label":["EndDate"],"name":[21],"type":["date"],"align":["right"]},{"label":["AwardPounds"],"name":[22],"type":["dbl"],"align":["right"]},{"label":["ExpenditurePounds"],"name":[23],"type":["dbl"],"align":["right"]},{"label":["Region"],"name":[24],"type":["chr"],"align":["left"]},{"label":["Status"],"name":[25],"type":["chr"],"align":["left"]},{"label":["GTRProjectUrl"],"name":[26],"type":["chr"],"align":["left"]},{"label":["FundingOrgId"],"name":[27],"type":["chr"],"align":["left"]},{"label":["LeadROId"],"name":[28],"type":["chr"],"align":["left"]},{"label":["PIId"],"name":[29],"type":["chr"],"align":["left"]},{"label":["abstractText"],"name":[30],"type":["chr"],"align":["left"]},{"label":["title"],"name":[31],"type":["chr"],"align":["left"]}],"data":[{"1":"BB/E014372/2","2":"BBSRC","3":"University of St Andrews","4":"B186507A-DC65-4FC1-858C-902A2B1B63AA","5":"TRUE","6":"BB/E014372","7":"2","8":"2","9":"Biology","10":"Research Grant","11":"Shuker","12":"David","13":"NA","14":"NA","15":"NA","16":"NA","17":"NA","18":"NA","19":"Understanding the constraints on sex ratio adaptation using artificial neural networks","20":"2009-04-01","21":"2010-11-30","22":"5101","23":"NA","24":"Scotland","25":"Closed","26":"http://internal-gtr-tomcat-alb-611010599.eu-west-2.elb.amazonaws.com:8080/gtr.portal/projects?ref=BB/E014372/2","27":"2512EF1C-401B-4222-9869-A770D4C5FAC7","28":"09D41C19-6E04-4937-902C-C3CD52E0683F","29":"F27CBAA9-3CD0-48F9-B9D6-942862A3FF72","30":"How perfect should behaviour be? There is little doubt that, given sufficient time, and genetic variation, natural selection can produce organisms displaying startlingly precise adaptations to their surroundings. At the same time, a variety of processes can constrain the ability of populations to reach adaptive peaks. Determining the relative importance of these processes is a major challenge still facing evolutionary biology. We will work on a model system that will allow us to say how obtaining and processing information can limit adaptation. We will work on parasitic wasps, which can choose the sex of their offspring by deciding whether or not to fertilize their eggs. The choice of offspring sex in this group has been the subject of much previous work, and the wasps generally show a good but imperfect fit to predicted behaviour. We will ask if we can better understand the behaviour shown in this group of organisms by incorporating the limitations of sensory and nervous systems into our predictions using a modeling approach known as artificial neural networks. The new predictions will then be tested against existing datasets and against new data, obtained by manipulating the information available to a parasitic wasp in the laboratory.","31":"Understanding the constraints on sex ratio adaptation using artificial neural networks"},{"1":"BB/E014372/1","2":"BBSRC","3":"University of Edinburgh","4":"6691CBAC-2089-4A2A-8C35-92CA45FBB1F4","5":"TRUE","6":"BB/E014372","7":"1","8":"2","9":"Inst of Evolutionary Biology","10":"Research Grant","11":"Shuker","12":"David","13":"NA","14":"NA","15":"NA","16":"NA","17":"NA","18":"NA","19":"Understanding the constraints on sex ratio adaptation using artificial neural networks","20":"2007-10-01","21":"2009-01-31","22":"9467","23":"NA","24":"Scotland","25":"Closed","26":"http://internal-gtr-tomcat-alb-611010599.eu-west-2.elb.amazonaws.com:8080/gtr.portal/projects?ref=BB/E014372/1","27":"2512EF1C-401B-4222-9869-A770D4C5FAC7","28":"A4735E0B-ED6E-4FC5-9497-CFE564317F54","29":"F27CBAA9-3CD0-48F9-B9D6-942862A3FF72","30":"How perfect should behaviour be? There is little doubt that, given sufficient time, and genetic variation, natural selection can produce organisms displaying startlingly precise adaptations to their surroundings. At the same time, a variety of processes can constrain the ability of populations to reach adaptive peaks. Determining the relative importance of these processes is a major challenge still facing evolutionary biology. We will work on a model system that will allow us to say how obtaining and processing information can limit adaptation. We will work on parasitic wasps, which can choose the sex of their offspring by deciding whether or not to fertilize their eggs. The choice of offspring sex in this group has been the subject of much previous work, and the wasps generally show a good but imperfect fit to predicted behaviour. We will ask if we can better understand the behaviour shown in this group of organisms by incorporating the limitations of sensory and nervous systems into our predictions using a modeling approach known as artificial neural networks. The new predictions will then be tested against existing datasets and against new data, obtained by manipulating the information available to a parasitic wasp in the laboratory.","31":"Understanding the constraints on sex ratio adaptation using artificial neural networks"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Although the project will have different <code>id</code>. The content is actually the same. So it is important these are taken out before input for analysis.</p>
<p>For the analysis, we only need to take the first project which end with /1</p>
<p>We will need to know which rows to keep which rows to delete</p>
<div class="cell">
<details class="code-fold">
<summary>keep only the first project</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>unique_prj <span class="ot">=</span> gtr_pj <span class="sc">|&gt;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">relocate</span>(ProjectReference, project_ref, ProjectId) <span class="sc">|&gt;</span> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(project_ref) <span class="sc">|&gt;</span> </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">rn=</span><span class="fu">row_number</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(rn<span class="sc">==</span><span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>rn) <span class="sc">|&gt;</span> </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(<span class="fu">select</span>(gtr_desc, id,abstractText,title), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"ProjectId"</span><span class="ot">=</span><span class="st">"id"</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>check further duplication</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>repeated_text <span class="ot">=</span> unique_prj <span class="sc">|&gt;</span> </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(abstractText) <span class="sc">|&gt;</span> </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n=</span><span class="fu">n</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(n<span class="sc">!=</span><span class="dv">1</span>) <span class="sc">|&gt;</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(abstractText)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>repeated_text <span class="sc">|&gt;</span> </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(ProjectReference,title,abstractText, ProjectId)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["ProjectReference"],"name":[1],"type":["chr"],"align":["left"]},{"label":["title"],"name":[2],"type":["chr"],"align":["left"]},{"label":["abstractText"],"name":[3],"type":["chr"],"align":["left"]},{"label":["ProjectId"],"name":[4],"type":["chr"],"align":["left"]}],"data":[{"1":"27744","2":"Enhancing Type 2 Diabetes treatment through peer-learning AI: Creating a support system for healthcare professionals in primary care to source and share optimal self-care interventions and best practice in order to improve care and efficiency","3":"**NEED:** The management of Type2 Diabetes costs the NHS &pound;8.8bn per year with patients estimated to account for 15-25% of all appointments at a local surgery level.\\n\\nThe 'General Practice Forward View' identifies that all patients with long-term conditions should have a personalised plan of care that includes self-care, social prescribing and active signposting as part of its 10 high impact actions within 5 years.\\n\\nInnovations such as Artificial Intelligence (AI) can play a key role in this - however at present there is no market-ready AI-system to assist Healthcare Professionals (HCP) to do this effectively within their time pressures, in part due to the difficulties associated with deriving, machine taught (machine learning), clinically-safe recommendations from the huge amounts of patient data available to digital health companies.\\n\\n**APPROACH:** The project brings together Healum Ltd (digital health platform developer), Vernova CIC Foundation (NHS), Manchester University (Subcontractor) to develop a peer2peer Learning AI and AI platform to support the self-management of T2D patients through personalised plans of care, support, behaviour change and education.\\n\\nWorking with 11 GP Practices and with SBRI support (Project Number:8625456) we have already established feasibility of approach, developed an a prototype collaborative self-care platform and innovative peer-learning algorithm.\\n\\n**FOCUS:** The project's focuses on the refinement of peer2peer algorithms and development of AI, machine learning (ML) algorthims, SnoMed code integration, content classifier and collective intelligence recommendation engine as well as aligning and evaluating the platform in the treatment of Type2 Diabetes (T2D).\\n\\nBy harnessing the collective peer2peer intelligence of HCP inputting into the software, combined with aggregate anonymised clinical audit data,we are able to reduce the critical mass of data inputs required to train ML algorithms and provide automatic clinical recommendations with a greater level of data confidence.\\n\\n**IMPACT:** The platform will be integrated with and make use of new SnoMed codes and historical Read codes to act as a classifier - supporting the recommendation of the right content, service and plan to the right patient at the right time - and trialled with 21 GP practices during the project to to form a peer2peer community for providing personalised plans of care, support, behaviour change and education to patients with or at risk of T2D.\\n\\nBy improving the management of T2D care workload, we estimate potential for cost saving per practice per annum of &pound;11,304, increasing to &pound;61,254 by Y5 once reductions in microvascular complications are factored in.","4":"6A378D2F-B4B5-4766-B3AB-ADB254A3213B"},{"1":"10025411","2":"Research and Development support for financial and economic impacts for project 27744","3":"**NEED:** The management of Type2 Diabetes costs the NHS &pound;8.8bn per year with patients estimated to account for 15-25% of all appointments at a local surgery level.\\n\\nThe 'General Practice Forward View' identifies that all patients with long-term conditions should have a personalised plan of care that includes self-care, social prescribing and active signposting as part of its 10 high impact actions within 5 years.\\n\\nInnovations such as Artificial Intelligence (AI) can play a key role in this - however at present there is no market-ready AI-system to assist Healthcare Professionals (HCP) to do this effectively within their time pressures, in part due to the difficulties associated with deriving, machine taught (machine learning), clinically-safe recommendations from the huge amounts of patient data available to digital health companies.\\n\\n**APPROACH:** The project brings together Healum Ltd (digital health platform developer), Vernova CIC Foundation (NHS), Manchester University (Subcontractor) to develop a peer2peer Learning AI and AI platform to support the self-management of T2D patients through personalised plans of care, support, behaviour change and education.\\n\\nWorking with 11 GP Practices and with SBRI support (Project Number:8625456) we have already established feasibility of approach, developed an a prototype collaborative self-care platform and innovative peer-learning algorithm.\\n\\n**FOCUS:** The project's focuses on the refinement of peer2peer algorithms and development of AI, machine learning (ML) algorthims, SnoMed code integration, content classifier and collective intelligence recommendation engine as well as aligning and evaluating the platform in the treatment of Type2 Diabetes (T2D).\\n\\nBy harnessing the collective peer2peer intelligence of HCP inputting into the software, combined with aggregate anonymised clinical audit data,we are able to reduce the critical mass of data inputs required to train ML algorithms and provide automatic clinical recommendations with a greater level of data confidence.\\n\\n**IMPACT:** The platform will be integrated with and make use of new SnoMed codes and historical Read codes to act as a classifier - supporting the recommendation of the right content, service and plan to the right patient at the right time - and trialled with 21 GP practices during the project to to form a peer2peer community for providing personalised plans of care, support, behaviour change and education to patients with or at risk of T2D.\\n\\nBy improving the management of T2D care workload, we estimate potential for cost saving per practice per annum of &pound;11,304, increasing to &pound;61,254 by Y5 once reductions in microvascular complications are factored in.","4":"B9B20477-E9DF-4291-B9FA-D7EAD48B9F8A"},{"1":"ST/G003467/1","2":"High speed imaging with diamond dynode detectors: a technological advance with major commercial applications","3":"1. The purpose of the project Development and commercialization of imaging detectors using artificial diamond technology to provide greatly enhanced performance. 2. Introduction The need to detect fast signals is crucial in many disciplines. Very high speed, low amplitude light signals need signal amplification. The photomultiplier tube (PMT) was the first device to use electronic signal amplification in a vacuum tube for optical light and has been a workhorse detector since. Though silicon chips have replaced vacuum tubes as the technology of choice in most imaging applications they have limited high speed and sensitivity performance compared with devices such as the PMT. The aim of this project is to apply detector technology and know-how from the Space Research Centre (SRC), Leicester, developed through space science R &amp; D, together with recent developments in diamond chemistry at the Diamond Group, Bristol, to the commercialization of an imaging PMT with ground-breaking performance for widespread commercial application and specific relevance to the defence sector / fusion plasma diagnostics at the Atomic Weapons Establishment, Aldermaston. 3. Advantages of Diamond as an electron amplification material a) High gain: Diamond is one of a small number of materials which has high electron gain when correctly treated. b) Simplified design: Diamond can have a higher gain per amplification stage, resulting in a lower number of stages being required for a given gain. c) Enhanced timing: The amplification properties of diamond allow improved signal timing and reduced background. d) Lower gain variability: The higher gain of diamond reduces the variability in the gain. e) Low noise: Diamond is less susceptible to thermal noise so it can operate with lower noise levels or at higher temperatures. f) Large area: Synthetic diamond offers low cost, large area coating and is easily grown on shaped surfaces. g) Stability: Synthetic diamond has a stable performance over long periods. Its performance remains high after exposure to air. The electron gain properties of synthetic diamond promises to greatly expand the usage of PMTs in many fields. 4. Application of synthetic Diamond to Detectors We have already measured the performance of synthetic diamond and our measured data supports published results and demonstrates the potential benefits of synthetic diamond as a detector material. This project will transfer the technology from proof-of-concept to prototype, beginning with optimization of manufacturing processes. Firstly we will manufacture two demonstrator detectors to provide data on process optimization. The next stage of the project will be development of a single transmissive gain stage. Transmissive dynodes can operate in two modes: - a) Transmission: input electrons enter through one surface of a thin film of diamond, and output electrons exit through the other. b) Refection: diamond is deposited on an open conductive wire mesh. Input and output electrons enter and exit through the same diamond surface. The transmission technique is superior, providing better detector performance, but is more demanding because of the need to produce very thin films, however we have already demonstrated manufacture. We will investigate both techniques and choose the optimum technology based on performance, manufacturability, developmental and manufacturing costs, and development timescale. We will initially demonstrate a single stage transmissive gain stage to provide comprehensive device diagnostics. The final stage of the project is to design, build and demonstrate a detector using a stack of gain stages with fast response and high gain and incorporating an imaging capability. Performance evaluation will involve testing with AWE collaborators at Aldermaston and field trials in a laser fusion facility at Los Alamos, and in photon counting mode at Photek and SRC.","4":"382C54EA-2248-422F-AEBC-07D2D56630B9"},{"1":"ST/G003475/1","2":"High speed imaging with diamond dynode detectors: a technological advance with major commercial applications","3":"1. The purpose of the project Development and commercialization of imaging detectors using artificial diamond technology to provide greatly enhanced performance. 2. Introduction The need to detect fast signals is crucial in many disciplines. Very high speed, low amplitude light signals need signal amplification. The photomultiplier tube (PMT) was the first device to use electronic signal amplification in a vacuum tube for optical light and has been a workhorse detector since. Though silicon chips have replaced vacuum tubes as the technology of choice in most imaging applications they have limited high speed and sensitivity performance compared with devices such as the PMT. The aim of this project is to apply detector technology and know-how from the Space Research Centre (SRC), Leicester, developed through space science R &amp; D, together with recent developments in diamond chemistry at the Diamond Group, Bristol, to the commercialization of an imaging PMT with ground-breaking performance for widespread commercial application and specific relevance to the defence sector / fusion plasma diagnostics at the Atomic Weapons Establishment, Aldermaston. 3. Advantages of Diamond as an electron amplification material a) High gain: Diamond is one of a small number of materials which has high electron gain when correctly treated. b) Simplified design: Diamond can have a higher gain per amplification stage, resulting in a lower number of stages being required for a given gain. c) Enhanced timing: The amplification properties of diamond allow improved signal timing and reduced background. d) Lower gain variability: The higher gain of diamond reduces the variability in the gain. e) Low noise: Diamond is less susceptible to thermal noise so it can operate with lower noise levels or at higher temperatures. f) Large area: Synthetic diamond offers low cost, large area coating and is easily grown on shaped surfaces. g) Stability: Synthetic diamond has a stable performance over long periods. Its performance remains high after exposure to air. The electron gain properties of synthetic diamond promises to greatly expand the usage of PMTs in many fields. 4. Application of synthetic Diamond to Detectors We have already measured the performance of synthetic diamond and our measured data supports published results and demonstrates the potential benefits of synthetic diamond as a detector material. This project will transfer the technology from proof-of-concept to prototype, beginning with optimization of manufacturing processes. Firstly we will manufacture two demonstrator detectors to provide data on process optimization. The next stage of the project will be development of a single transmissive gain stage. Transmissive dynodes can operate in two modes: - a) Transmission: input electrons enter through one surface of a thin film of diamond, and output electrons exit through the other. b) Refection: diamond is deposited on an open conductive wire mesh. Input and output electrons enter and exit through the same diamond surface. The transmission technique is superior, providing better detector performance, but is more demanding because of the need to produce very thin films, however we have already demonstrated manufacture. We will investigate both techniques and choose the optimum technology based on performance, manufacturability, developmental and manufacturing costs, and development timescale. We will initially demonstrate a single stage transmissive gain stage to provide comprehensive device diagnostics. The final stage of the project is to design, build and demonstrate a detector using a stack of gain stages with fast response and high gain and incorporating an imaging capability. Performance evaluation will involve testing with AWE collaborators at Aldermaston and field trials in a laser fusion facility at Los Alamos, and in photon counting mode at Photek and SRC.","4":"BBD58755-5EF9-4281-9EC2-177637F10D28"},{"1":"NE/V012908/1","2":"Dry deposition processes of volatile organic compounds (VOCDep)","3":"A large range of different volatile organic chemical compounds (VOCs) are released to the atmosphere from vegetation and human activities such as fossil fuel burning and the use of consumer care products. Together with nitrogen oxides, VOCs are one of the two key ingredients for the formation of tropospheric ground-level ozone pollution, causing impacts on human health, biodiversity decline and crop losses. Similarly, through a series of chemical reactions in the atmosphere, many VOCs end up forming the organic fraction of particulate matter (PM) with major impacts on human health around the globe. VOC emissions are needed in models to predict air quality and its impacts and climate change. These models are used to assess measures to reduce emissions to safeguard the health of humans and the environment. For these model predictions to be reliable, the models need to accurately represent VOC emissions, chemical transformation, but also the deposition to vegetation. Whilst considerable effort has gone into the first two aspects, very little is known about the deposition of the various VOC compounds, with deposition rates usually estimated from the behaviour of other (inorganic) compounds with no validation through actual measurements.\\n\\nThis project will make use of recent improvements in VOC measurement technology to perform the first comprehensive study of the rate and processes that control the deposition of a wide range of different VOC compounds of environmental concern. This will be achieved through three different, complementary experimental approaches: (a) the study of VOC uptake to vegetation in the laboratory using gas exchange chambers, (b) the study of VOC uptake to natural and artificial liquid water films and (c) two measurement campaigns of VOC exchange with vegetation, focussing on urban parkland and forest. The results from these measurements and existing datasets from project partners will be used to derive improved model descriptions of the deposition process for incorporation into the numerical models. We will then use two atmospheric chemistry and transport models, a simpler model that can be operated at high spatial resolution and is used to support European and UK policy, and a model with a more detailed description of the chemistry of isoprene, the compound that dominates plant emissions globally to assess the impacts of the new deposition rates on model performance with emphasis on ozone formation, PM formation and the particular role of isoprene in PM formation.","4":"BD2B6410-487F-4640-B8B2-29DE1C4E1A4C"},{"1":"NE/V01272X/1","2":"Dry deposition processes of volatile organic compounds (VOCDep)","3":"A large range of different volatile organic chemical compounds (VOCs) are released to the atmosphere from vegetation and human activities such as fossil fuel burning and the use of consumer care products. Together with nitrogen oxides, VOCs are one of the two key ingredients for the formation of tropospheric ground-level ozone pollution, causing impacts on human health, biodiversity decline and crop losses. Similarly, through a series of chemical reactions in the atmosphere, many VOCs end up forming the organic fraction of particulate matter (PM) with major impacts on human health around the globe. VOC emissions are needed in models to predict air quality and its impacts and climate change. These models are used to assess measures to reduce emissions to safeguard the health of humans and the environment. For these model predictions to be reliable, the models need to accurately represent VOC emissions, chemical transformation, but also the deposition to vegetation. Whilst considerable effort has gone into the first two aspects, very little is known about the deposition of the various VOC compounds, with deposition rates usually estimated from the behaviour of other (inorganic) compounds with no validation through actual measurements.\\n\\nThis project will make use of recent improvements in VOC measurement technology to perform the first comprehensive study of the rate and processes that control the deposition of a wide range of different VOC compounds of environmental concern. This will be achieved through three different, complementary experimental approaches: (a) the study of VOC uptake to vegetation in the laboratory using gas exchange chambers, (b) the study of VOC uptake to natural and artificial liquid water films and (c) two measurement campaigns of VOC exchange with vegetation, focussing on urban parkland and forest. The results from these measurements and existing datasets from project partners will be used to derive improved model descriptions of the deposition process for incorporation into the numerical models. We will then use two atmospheric chemistry and transport models, a simpler model that can be operated at high spatial resolution and is used to support European and UK policy, and a model with a more detailed description of the chemistry of isoprene, the compound that dominates plant emissions globally to assess the impacts of the new deposition rates on model performance with emphasis on ozone formation, PM formation and the particular role of isoprene in PM formation.","4":"B7C88680-EAD3-458A-B921-D222C336E284"},{"1":"EP/F047770/1","2":"Carbon Dioxide and Alkanes as Electron-sink and Source in a Solar Nanocell: towards Tandem Photosynthesis of Carbon Monoxide and Methanol","3":"A major solar energy challenge is the goal of artificial synthesis in which sunlight is used to generate fuels or high energy chemicals. Natural photosynthesis uses solar energy to generate dioxygen and carbohydrates from carbon dioxide and water, but the targets of artificial photosynthesis can be more diverse. Our vision is to create a solar nano-device which will drive the coupled photo-conversion of methane and carbon dioxide into methanol and carbon monoxide respectively. This challenging target differs fundamentally from the familiar one of splitting water into hydrogen and oxygen. Our target offers products both on the oxidation and the reduction sides that are significant fuels or feedstocks. The photocatalytic reduction of CO2 and oxidation of alkanes represent long-standing goals of great complexity, but we base our concepts on well-established principles. We break down the goals into individual components, each of which is highly challenging within its own right and delivery of each would constitute a major breakthrough. The challenges will be met by a team of scientists, integrated across the four centres of Manchester, Nottingham, York and Norwich, who lead teams with expertise in photophysics, nanoscience, photochemistry, electrochemistry and synthesis. Thus these researchers will seek to establish the science required to underpin technologies that will allow the conversion of abundant and environmentally damaging feedstocks into products of high economic value by constructing a new class of solar device capable of driving green chemical reactions.","4":"041635B4-35F6-4AA5-A7E4-5FEEA3A1202C"},{"1":"EP/F047789/1","2":"Carbon Dioxide and Alkanes as Electron-sink and Source in a Solar Nanocell: towards Tandem Photosynthesis of Carbon Monoxide and Methanol","3":"A major solar energy challenge is the goal of artificial synthesis in which sunlight is used to generate fuels or high energy chemicals. Natural photosynthesis uses solar energy to generate dioxygen and carbohydrates from carbon dioxide and water, but the targets of artificial photosynthesis can be more diverse. Our vision is to create a solar nano-device which will drive the coupled photo-conversion of methane and carbon dioxide into methanol and carbon monoxide respectively. This challenging target differs fundamentally from the familiar one of splitting water into hydrogen and oxygen. Our target offers products both on the oxidation and the reduction sides that are significant fuels or feedstocks. The photocatalytic reduction of CO2 and oxidation of alkanes represent long-standing goals of great complexity, but we base our concepts on well-established principles. We break down the goals into individual components, each of which is highly challenging within its own right and delivery of each would constitute a major breakthrough. The challenges will be met by a team of scientists, integrated across the four centres of Manchester, Nottingham, York and Norwich, who lead teams with expertise in photophysics, nanoscience, photochemistry, electrochemistry and synthesis. Thus these researchers will seek to establish the science required to underpin technologies that will allow the conversion of abundant and environmentally damaging feedstocks into products of high economic value by constructing a new class of solar device capable of driving green chemical reactions.","4":"7FF78CAF-FB31-419E-83AE-FAADDB7562F8"},{"1":"EP/F047878/1","2":"Carbon Dioxide and Alkanes as Electron-sink and Source in a Solar Nanocell: towards Tandem Photosynthesis of Carbon Monoxide and Methanol","3":"A major solar energy challenge is the goal of artificial synthesis in which sunlight is used to generate fuels or high energy chemicals. Natural photosynthesis uses solar energy to generate dioxygen and carbohydrates from carbon dioxide and water, but the targets of artificial photosynthesis can be more diverse. Our vision is to create a solar nano-device which will drive the coupled photo-conversion of methane and carbon dioxide into methanol and carbon monoxide respectively. This challenging target differs fundamentally from the familiar one of splitting water into hydrogen and oxygen. Our target offers products both on the oxidation and the reduction sides that are significant fuels or feedstocks. The photocatalytic reduction of CO2 and oxidation of alkanes represent long-standing goals of great complexity, but we base our concepts on well-established principles. We break down the goals into individual components, each of which is highly challenging within its own right and delivery of each would constitute a major breakthrough. The challenges will be met by a team of scientists, integrated across the four centres of Manchester, Nottingham, York and Norwich, who lead teams with expertise in photophysics, nanoscience, photochemistry, electrochemistry and synthesis. Thus these researchers will seek to establish the science required to underpin technologies that will allow the conversion of abundant and environmentally damaging feedstocks into products of high economic value by constructing a new class of solar device capable of driving green chemical reactions.","4":"C595D2BE-8EDC-4321-B948-1140E19078A0"},{"1":"EP/F04772X/1","2":"Carbon Dioxide and Alkanes as Electron-sink and Source in a Solar Nanocell: towards Tandem Photosynthesis of Carbon Monoxide and Methanol","3":"A major solar energy challenge is the goal of artificial synthesis in which sunlight is used to generate fuels or high energy chemicals. Natural photosynthesis uses solar energy to generate dioxygen and carbohydrates from carbon dioxide and water, but the targets of artificial photosynthesis can be more diverse. Our vision is to create a solar nano-device which will drive the coupled photo-conversion of methane and carbon dioxide into methanol and carbon monoxide respectively. This challenging target differs fundamentally from the familiar one of splitting water into hydrogen and oxygen. Our target offers products both on the oxidation and the reduction sides that are significant fuels or feedstocks. The photocatalytic reduction of CO2 and oxidation of alkanes represent long-standing goals of great complexity, but we base our concepts on well-established principles. We break down the goals into individual components, each of which is highly challenging within its own right and delivery of each would constitute a major breakthrough. The challenges will be met by a team of scientists, integrated across the four centres of Manchester, Nottingham, York and Norwich, who lead teams with expertise in photophysics, nanoscience, photochemistry, electrochemistry and synthesis. Thus these researchers will seek to establish the science required to underpin technologies that will allow the conversion of abundant and environmentally damaging feedstocks into products of high economic value by constructing a new class of solar device capable of driving green chemical reactions.","4":"60F0EBC9-2FB9-4249-A84B-023969CD4705"},{"1":"84293","2":"Move3 -Home workouts with Artificial Intelligence","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"58CC3C71-FA0E-4F25-953D-A0250C8BEA9E"},{"1":"107512","2":"Fashion Business Guide Platform with Artificial Intelligence","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"9362F98E-DCEC-45E0-B839-1B57EB044373"},{"1":"2736907","2":"SANTA: Smart and Automated Nanomanufacturing Technologies with Artificial Intelligence","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"A143C625-0EEF-439F-85BA-71E00EA74CED"},{"1":"10004471","2":"Using Data Integration and Artificial Intelligence to improve HS2 Project Success","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"C5C4AF2B-82CA-4488-B59A-C224A1618B97"},{"1":"MC_PC_21014","2":"Warwick-KU Collaboration on Domain-Invariant Artificial Intelligence for Robust Analysis of Pathology Images","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"C51B855E-0A97-40CA-8196-476534F2FCCF"},{"1":"2750918","2":"Interpretation of animal behaviours using novel video processing techniques in Artificial Intelligence","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"272CE517-FE95-4C0E-871B-3E026F91C64D"},{"1":"2765246","2":"Performance-Based Earthquake Engineering 2.0: Machine-Learning and Artificial Intelligence Algorithms for seismic hazard and vulnerability.","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"8B3EDF0C-AFE8-441A-8662-99131BB97D0B"},{"1":"2749500","2":"Integration of Artificial Intelligence (AI) and Distributed Ledger Technologies to Improve Interpretability and Reportability for Point-of-Care Medica","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"F94F273E-2ACB-4E72-9831-64BC6476C1B6"},{"1":"BB/T019980/1","2":"Taiwan Partnering Award: Artificial intelligence applications to identify regulatory genomic signatures of diet and lifestyle disease risk factors","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"555233F9-45D4-4E4E-A55B-89BE908BDF22"},{"1":"120028","2":"Development of a test framework to investigate the the applicability of Artificial Intelligence techniques to Strategy Games","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"7CA04F52-C494-4B2C-AA5D-BCB924B25332"},{"1":"BB/V509267/1","2":"Business Case for a Catalyst Partnership in Artificial Intelligence between the Alan Turing Institute and the Norwich Biosciences Institutes","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"AFED7ACC-10AF-421E-96A9-98F989AC3482"},{"1":"EP/T000481/1","2":"An Artificial Ribosome","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"1744A629-8FA9-4202-8CF9-DA55A8FB20C9"},{"1":"2741774","2":"Assembly of Artificial Metalloenzymes for Biocatalysis","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"E0A907B0-84F0-4A2E-8018-50F1BFD1BEEA"},{"1":"10032173","2":"An intelligence vulnerability management (i-VulMan) platform","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"BF9C97D3-AD2F-4EF0-9D61-0405971CB611"},{"1":"EP/R002584/1","2":"Controlling membrane translocation for artificial signal transduction","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"63F5AFBE-D19C-478B-982D-CAC2272A0989"},{"1":"BB/S009817/1","2":"In situ generated artificial immunity against Campylobacter","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"C4B44E54-4A1B-46CC-A5F6-0AEA160077A7"},{"1":"2780559","2":"Collective animal behaviour and artificial swarm systems","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"2A91E89D-08AB-4142-8326-A8F61B0E432E"},{"1":"ES/E000231/1","2":"Social engagement, emotional intelligence and loneliness among school-children","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"3F63CBE9-913D-4A51-94B1-2032E2CFBACE"},{"1":"EP/F062222/1","2":"Advanced Analysis of Building Energy Performance using Computational Intelligence Approaches","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"682686B6-9BD2-447E-827E-1E5DFB6E9A67"},{"1":"10002618","2":"An Intelligence Cyber Security Incident Handling (i-CSIH)","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"4612F3C7-9BF0-4F7C-864C-EF137D0EDB10"},{"1":"EP/L00285X/1","2":"Artificial Spin Ice: Designer Matter Far From Equilibrium","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"23213008-63F4-4A2C-8719-2870BA1B7CA4"},{"1":"EP/G010897/1","2":"Current-driven Domain Wall Motion in Artificial Magnetic Domain Structures","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"3F55619A-448A-4298-8FFA-B7AC5673D3C4"},{"1":"EP/L003090/1","2":"Artificial Spin Ice: Designer Matter Far From Equilibrium","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"D78C0D17-10CF-420D-A6D4-0913B0FC5491"},{"1":"74284","2":"Maci-D: An AI-powered tool to accelerate the identification and extraction of business intelligence data","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"4A2B0E3B-80A3-41AF-A634-63192716FAA1"},{"1":"ES/I010173/1","2":"Postdoctoral Fellowship: The relationship of intelligence and personality, and its effect on adult intellect.","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"D4E781FB-7A99-4E13-AEEA-CE865E7666FD"},{"1":"EP/M007065/1","2":"Testing Quantumness: From Artificial Quantum Arrays to Lattice Spin Models and Spin Liquids","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"F04C89B3-7FC8-44B2-9E25-D0A87BF68042"},{"1":"EP/P015875/1","2":"Detecting bladder volume and pressure from sacral nerve signals: the key to future artificial control","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"45F0BBBA-3C47-42D1-9EE2-E8F99CCD7BE2"},{"1":"2749892","2":"Computational Intelligence for algorithmic support of teams in data-intensive contexts","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"BCC35640-E0B9-445C-9F89-84B18B223836"},{"1":"120001","2":"HISTORIC - Building Emotional Social Intelligence in a Virtual Playground for Autistic Children (BESIVPAC)","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"1BECF116-1917-4F8E-87B4-B2264FC5F452"},{"1":"EP/P015999/1","2":"Detecting bladder volume and pressure from sacral nerve signals: the key to future artificial control","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"ED1A4589-E4E2-44BD-A032-0ABEAE39D9B2"},{"1":"2767061","2":"Polymer coated surfaces for the target-agnostic diagnosis of diseases - an artificial nose approach","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"6FA61AE2-25D7-4576-90E0-237FD0A27CD0"},{"1":"EP/J010618/1","2":"Generation, Imaging and Control of Novel Coherent Electronic States in Artificial Ferromagnetic-Superconducting Hybrid Metamaterials and Devices","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"E5CB0216-BC28-4745-86CD-0BE536B69508"},{"1":"RES-193-25-0010","2":"Improving engagement with communities: exploring the potential of community intelligence utilising contemporary geographical analysis and geographic i","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"56316701-F8DF-4397-A9DE-518D6B3358FB"},{"1":"BB/R012474/1","2":"Creating artificial oligonucleotides by chemical synthesis - applications in life science research, crop protection and as novel therapeutics","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"A10B4AF1-AB07-4DE3-9546-26F95BFCE146"},{"1":"EP/J010634/1","2":"Generation, Imaging and Control of Novel Coherent Electronic States in Artificial Ferromagnetic-Superconducting Hybrid Metamaterials and Devices","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"82F8E0B2-F648-427A-A42C-A8203C0C7A3F"},{"1":"EP/J010626/1","2":"Generation, Imaging and Control of Novel Coherent Electronic States in Artificial Ferromagnetic-Superconducting Hybrid Metamaterials and Devices","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"B0F2692D-B59F-460D-989F-84810462873F"},{"1":"EP/J010650/1","2":"Generation, Imaging and Control of Novel Coherent Electronic States in Artificial Ferromagnetic-Superconducting Hybrid Metamaterials and Devices","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"FA9CDF17-364C-41CB-BC13-DEF71A21F184"},{"1":"BB/S015191/1","2":"NEC06796 See and be seen: Understanding trade-offs in bioluminescent signalling and how it is affected by artificial lighting at night","3":"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.","4":"6A3458BA-057D-42F4-93BE-E05F2CC58DDD"},{"1":"NE/T003952/2","2":"Explainable AI for UK agricultural land use decision-making","3":"Agricultural land use dynamics and their associated driving factors represent highly complex systems of flows that are subject to non-linearities, sensitivities, and uncertainties across spatial and temporal scales. They are therefore challenging to represent using traditional statistical modelling approaches. Existing process-based modelling has enabled advances in understanding of individual biophysical processes underpinning agricultural land use systems (e.g. crop, livestock and biogeochemical models). However, these tend to focus on individual processes in detail or link a limited number of processes at large scales, thereby mostly ignoring the complex interdependencies between the multiple interacting biophysical and socio-economic components of land use systems. Artificial intelligence (AI) techniques offer great potential to complement such modelling approaches by mining the deep knowledge (e.g. farming patterns and behaviours) encapsulated in 'big' data from ground-based sensors (such as frequently used for precision farming) and Earth Observation satellites. This will deliver enhanced insight on the past and current state and spatio-temporal dynamics of agricultural land use system flows and how they can be influenced by decisions on agricultural policies and related farm management practices. \\n\\nOur proposal aims to develop a novel explainable AI framework that is transparent, data-driven and spatially-explicit by using probabilistic inference and explicit &quot;if-then&quot; rules. We will demonstrate proof-of-concept for two pilot regions of the UK (Oxfordshire and Lincolnshire), and the framework will be set up in a way that can be readily expanded to the whole UK. Specifically, we will draw on time-series of agricultural land use and production datasets (in-kind support from industry project partner SOYL) to identify the key socio-economic and environmental driving factors that have led to historic agricultural land use changes in the pilot regions. We will then establish explainable AI-rules for the characterisation of these agricultural land use changes and refine them within the framework through machine learning and parameter optimisation.\\n\\nWe will demonstrate and test the potential of the explainable AI framework for providing a new and robust method for predicting changing patterns of agricultural land use in the two pilot regions. This will include testing the ability of the AI framework for improving understanding of past and present agricultural land use dynamics across multiple temporal and spatial scales from 'big' data. It will also assess the potential for continually updating the predictions of land use dynamics in real-time using data from sensors. This could provide early warning when certain driving conditions are triggered or used to repeatedly refine short-term projections of land use change and their estimates of uncertainty.","4":"90EBE26F-1251-49FA-BFA0-1C9ECA42D11B"},{"1":"NE/T004002/1","2":"Explainable AI for UK agricultural land use decision-making","3":"Agricultural land use dynamics and their associated driving factors represent highly complex systems of flows that are subject to non-linearities, sensitivities, and uncertainties across spatial and temporal scales. They are therefore challenging to represent using traditional statistical modelling approaches. Existing process-based modelling has enabled advances in understanding of individual biophysical processes underpinning agricultural land use systems (e.g. crop, livestock and biogeochemical models). However, these tend to focus on individual processes in detail or link a limited number of processes at large scales, thereby mostly ignoring the complex interdependencies between the multiple interacting biophysical and socio-economic components of land use systems. Artificial intelligence (AI) techniques offer great potential to complement such modelling approaches by mining the deep knowledge (e.g. farming patterns and behaviours) encapsulated in 'big' data from ground-based sensors (such as frequently used for precision farming) and Earth Observation satellites. This will deliver enhanced insight on the past and current state and spatio-temporal dynamics of agricultural land use system flows and how they can be influenced by decisions on agricultural policies and related farm management practices. \\n\\nOur proposal aims to develop a novel explainable AI framework that is transparent, data-driven and spatially-explicit by using probabilistic inference and explicit &quot;if-then&quot; rules. We will demonstrate proof-of-concept for two pilot regions of the UK (Oxfordshire and Lincolnshire), and the framework will be set up in a way that can be readily expanded to the whole UK. Specifically, we will draw on time-series of agricultural land use and production datasets (in-kind support from industry project partner SOYL) to identify the key socio-economic and environmental driving factors that have led to historic agricultural land use changes in the pilot regions. We will then establish explainable AI-rules for the characterisation of these agricultural land use changes and refine them within the framework through machine learning and parameter optimisation.\\n\\nWe will demonstrate and test the potential of the explainable AI framework for providing a new and robust method for predicting changing patterns of agricultural land use in the two pilot regions. This will include testing the ability of the AI framework for improving understanding of past and present agricultural land use dynamics across multiple temporal and spatial scales from 'big' data. It will also assess the potential for continually updating the predictions of land use dynamics in real-time using data from sensors. This could provide early warning when certain driving conditions are triggered or used to repeatedly refine short-term projections of land use change and their estimates of uncertainty.","4":"07A4D48B-25A1-4732-AEE6-BEEEB0DC6E4C"},{"1":"BB/I022791/1","2":"Detecting cytosine methylation at the single DNA molecule level","3":"Although the genomes of many organisms (humans, plants, invertebrates and vertebrates) have been sequenced and many of the genes identified, our understanding of the regulation of the genes is limited due to lack of analysis technology. DNA is composed of four nucleic acid bases, adenine, guanine, cytosine and thymine. Some of these nucleic acid bases can be modified by enzymes and as a result have an additional methyl group; here we will investigate new technologies for the detection of methylcytosine and unmethylated cytosine within single DNA molecules. The methylation of cytosine nucleic acid bases is associated with gene silencing. In humans DNA methylation is considered to play a critical role in development and is aberrant in many diseases, but as yet the complete role remains unclear. There are numerous techniques for the detection of methylated cytosine in DNA, but the current methodologies do not yet provide a simple, fast, reliable cheap approach. A major problem is the need to evaluate DNA from cell samples that will contain the same DNA sequence but which are heterogeneous with respect to the cytosine residues that are methylated. So an average is often obtained. Those techniques that do allow single DNA strands to be evaluated are highly laborious and limited. Here we will develop a new approach for detecting sequences containing methylated cytosines at the single molecule level. There are currently other groups working in the field of DNA sequencing of single molecules, but these methods are slow and the DNA is investigated as a single strand. We will interrogate double-stranded DNA and this will allow us to detect methylated or unmethylated cytosine molecules on each strand, called hemi-methylation. Our approach is to create an artificial form of DNA, an oligonucleotide, that associates and wraps within the major groove of double-stranded DNA molecule at specific sequences. This artificial form of DNA when associated is called a triplex and the molecules synthesised will also contain a fluorophore. When the DNA sample has been treated with these triplex forming oligonucleotides the helix will contain fluorophores at different points along it. We will inject the DNA sample into a small channel that will result in unravelling and straightening of the strand so that it is then threaded into an optical interrogation channel. The fluorophores will be excited with light, which in the presence of nanostructures within the nanochannel will result in fluorescence intensity changes. The change in intensity will provide a code that indicates the methylation status of the different cytosine containing sequences (unmethylated, hemi-methylated, doubly methylated). A simple technique to detect the methylated and unmethylated cytosines within DNA sequences will be important for a wide academic, clinical and industrial research community, since this will allow a greater understanding of gene regulation. There are many research areas where cytosine methylation is considered to play a significant role in humans, such as diet related disease, inflammatory diseases, embryonic development to name a few, or in plants for understanding the effect of environmental stress. But as noted above, cytosine methylation is important for many organisms, and a technique that allows for the analysis of the patterns of methylation within genes has the potential to be commercially valuable in the longer-term. First a better understanding of DNA methylation is required, but it is possible that a form of the approach proposed here will yield a diagnostic tool.","4":"C5DA86BF-9490-438E-8326-CB6E89912F3B"},{"1":"BB/I022686/1","2":"Detecting cytosine methylation at the single DNA molecule level","3":"Although the genomes of many organisms (humans, plants, invertebrates and vertebrates) have been sequenced and many of the genes identified, our understanding of the regulation of the genes is limited due to lack of analysis technology. DNA is composed of four nucleic acid bases, adenine, guanine, cytosine and thymine. Some of these nucleic acid bases can be modified by enzymes and as a result have an additional methyl group; here we will investigate new technologies for the detection of methylcytosine and unmethylated cytosine within single DNA molecules. The methylation of cytosine nucleic acid bases is associated with gene silencing. In humans DNA methylation is considered to play a critical role in development and is aberrant in many diseases, but as yet the complete role remains unclear. There are numerous techniques for the detection of methylated cytosine in DNA, but the current methodologies do not yet provide a simple, fast, reliable cheap approach. A major problem is the need to evaluate DNA from cell samples that will contain the same DNA sequence but which are heterogeneous with respect to the cytosine residues that are methylated. So an average is often obtained. Those techniques that do allow single DNA strands to be evaluated are highly laborious and limited. Here we will develop a new approach for detecting sequences containing methylated cytosines at the single molecule level. There are currently other groups working in the field of DNA sequencing of single molecules, but these methods are slow and the DNA is investigated as a single strand. We will interrogate double-stranded DNA and this will allow us to detect methylated or unmethylated cytosine molecules on each strand, called hemi-methylation. Our approach is to create an artificial form of DNA, an oligonucleotide, that associates and wraps within the major groove of double-stranded DNA molecule at specific sequences. This artificial form of DNA when associated is called a triplex and the molecules synthesised will also contain a fluorophore. When the DNA sample has been treated with these triplex forming oligonucleotides the helix will contain fluorophores at different points along it. We will inject the DNA sample into a small channel that will result in unravelling and straightening of the strand so that it is then threaded into an optical interrogation channel. The fluorophores will be excited with light, which in the presence of nanostructures within the nanochannel will result in fluorescence intensity changes. The change in intensity will provide a code that indicates the methylation status of the different cytosine containing sequences (unmethylated, hemi-methylated, doubly methylated). A simple technique to detect the methylated and unmethylated cytosines within DNA sequences will be important for a wide academic, clinical and industrial research community, since this will allow a greater understanding of gene regulation. There are many research areas where cytosine methylation is considered to play a significant role in humans, such as diet related disease, inflammatory diseases, embryonic development to name a few, or in plants for understanding the effect of environmental stress. But as noted above, cytosine methylation is important for many organisms, and a technique that allows for the analysis of the patterns of methylation within genes has the potential to be commercially valuable in the longer-term. First a better understanding of DNA methylation is required, but it is possible that a form of the approach proposed here will yield a diagnostic tool.","4":"C18DE419-2F59-49AC-80DC-8155A2468A67"},{"1":"TS/G002835/1","2":"CAD-GAME: Computer-Aided Game Design","3":"As a form of media - increasingly, mass media - games have much in common with film, particularly in terms of business models. One reason for the high levels of risk in film is that only limited market testing can be done prior to product release, and the product is essentially fixed from then on. Games have traditionally followed this model, but it is important to remember that games are not just media - they are also pieces of software. One major lesson from the software industry is that capturing users' interactions with a product can be used to refine market offerings in several ways: by adapting it after release (e.g. adaptive interfaces), by using that data to iteratively refine products in future releases (e.g. error reporting in Microsoft and Apple), and to understand the various user groups in more depth. In essence, we are suggesting that this approach can be directly applied to games. The difficulty, however, is that we cannot use simple measures of task completion, because there is no task: we are simply trying to entertain. The main research challenge is to understand better just what the word 'entertain' really means in games, and how this might be reflected in patterns of player behaviour. To do this, we will undertake a study of enjoyment and immersion in video games, in order to derive a concrete methodology for closely estimating the amount of enjoyment and immersion a player has, at fairly fine-grained intervals of a game. We will also capture a multitude of different forms of data from players actually playing Rebellion's games, and we will use advanced techniques from Artificial Intelligence to search for correlations with the player's overall enjoyment. These techniques could be used during design to speed up the process of tweaking the various game parameters - where the enemies are, how effective weapons are, etc. More interesting, however, is the idea that data analysis tools could run within the game itself, adapting the game in response to the player's behaviour. In this way, we hope to pioneer a new age of user-adaptive video games, within a one game, many gameplays paradigm, i.e., where each player has a unique game experience which is tailored to their personality, level of experience, playing style and mood.","4":"781819E9-B378-48EB-942D-23DAB639E1EC"},{"1":"TS/G002843/1","2":"CAD-GAME: Computer-Aided Game Design","3":"As a form of media - increasingly, mass media - games have much in common with film, particularly in terms of business models. One reason for the high levels of risk in film is that only limited market testing can be done prior to product release, and the product is essentially fixed from then on. Games have traditionally followed this model, but it is important to remember that games are not just media - they are also pieces of software. One major lesson from the software industry is that capturing users' interactions with a product can be used to refine market offerings in several ways: by adapting it after release (e.g. adaptive interfaces), by using that data to iteratively refine products in future releases (e.g. error reporting in Microsoft and Apple), and to understand the various user groups in more depth. In essence, we are suggesting that this approach can be directly applied to games. The difficulty, however, is that we cannot use simple measures of task completion, because there is no task: we are simply trying to entertain. The main research challenge is to understand better just what the word 'entertain' really means in games, and how this might be reflected in patterns of player behaviour. To do this, we will undertake a study of enjoyment and immersion in video games, in order to derive a concrete methodology for closely estimating the amount of enjoyment and immersion a player has, at fairly fine-grained intervals of a game. We will also capture a multitude of different forms of data from players actually playing Rebellion's games, and we will use advanced techniques from Artificial Intelligence to search for correlations with the player's overall enjoyment. These techniques could be used during design to speed up the process of tweaking the various game parameters - where the enemies are, how effective weapons are, etc. More interesting, however, is the idea that data analysis tools could run within the game itself, adapting the game in response to the player's behaviour. In this way, we hope to pioneer a new age of user-adaptive video games, within a one game, many gameplays paradigm, i.e., where each player has a unique game experience which is tailored to their personality, level of experience, playing style and mood.","4":"A3159B12-CD84-46A0-A6D7-22646196C02D"},{"1":"2635643","2":"Inferring ancestry and relatedness of human genomes using ancient DNA samples' and falls within the EPSRC Artificial Intelligence and Healthcare Techn","3":"At every genomic position, two individuals are connected through genealogical relationships that lead to a common ancestor. The chronological distance from the individuals to this ancestor is termed time to the most recent common ancestor (TMRCA). This can be generalized to a set of individuals by representing their genealogical relationships by a tree. Moving along the genome, the topology of the trees can change as the genome is broken up by recombination during meiosis. Hence, the evolutionary history of a set of samples can be compactly represented by a graph, called the ancestral recombination graph (ARG), comprised by the individual trees spanning different chunks of the genome. \\n\\nThere are multiple computationally intensive methods to reconstruct the ARG from high-quality sequencing data of modern DNA samples. Ancient samples are of degraded quality due to environmental conditions and contamination; usually they can only be sequenced at very low coverage making the task of incorporating them into the ARG of a set of modern samples challenging. \\n\\nReconstructing an accurate joint ARG between modern and ancient DNA samples has multiple potential applications that we aim to explore. It can be used for ancestry inference, by recovering the ancestry proportion a modern sample inherits from various ancient ancestral groups, enabling us to reconstruct historical events such as population migrations. We can further exploit ARG topology to detect natural selection, by locating regions of the genome that are unusually shared from certain individuals or ancient groups . Finding regions under positive or negative selection, particularly with known biological functionality, can be especially useful in healthcare-related applications and such regions have, for example, been leveraged to determine drug targets in pharmaceutical settings. Finally, the phenotypic impact of variants can be evaluated by testing whether ancestry from certain groups is more closely related to certain phenotypes.\\n\\nThe project's first goal is to build a relatedness inference algorithm that can infer tree topology and TMRCAs between modern and ancient samples and use it to reconstruct a joint ARG with data from the UK BioBank and other sources. Long-range chromosomal regions that are shared across pairs of samples are informative for this analysis but hard to detect in low coverage ancient DNA, so our algorithm will need to to implicitly or explicitly model haplotype sharing despite the lack of phasing information, or in the presence of noisy computational phasing. \\n\\nFor this algorithm, we leverage Deep Learning (DL), which has transformed many scientific fields in the past decade. Population genetics has traditionally focused on developing complex parametric models and has not yet significantly benefited from DL advances. Sequencing data has a spatial structure, so sequences from multiple samples can be stacked to form an image and analysed using computer vision approaches (e.g. Convolutional Neural Networks),adjusting for the fact that sample order is irrelevant (i.e. require exchangeable networks). As we already have access to an ARG for modern samples, our aim is to explore the use of attention- and graph-based methods to extract ARG information that will help infer ancestry between modern and ancient samples.\\n\\nOverall, we expect to make two main contributions. The first is algorithmic development that will allow the use of DL for reconstructing joint genealogical trees for modern and ancient DNA samples, tackling quality issues for the latter. The second is joint ARG inference using real UK Biobank and ancient data. We then aim to analyse this ARG to answer questions relating to natural selection and phenotypic impact of having ancestry from certain ancient groups.","4":"40354CB2-65B2-4C75-80C4-36E8AC7C190C"},{"1":"2420820","2":"Inferring ancestry and relatedness of human genomes using ancient DNA samples' and falls within the EPSRC Artificial Intelligence and Healthcare Techn","3":"At every genomic position, two individuals are connected through genealogical relationships that lead to a common ancestor. The chronological distance from the individuals to this ancestor is termed time to the most recent common ancestor (TMRCA). This can be generalized to a set of individuals by representing their genealogical relationships by a tree. Moving along the genome, the topology of the trees can change as the genome is broken up by recombination during meiosis. Hence, the evolutionary history of a set of samples can be compactly represented by a graph, called the ancestral recombination graph (ARG), comprised by the individual trees spanning different chunks of the genome. \\n\\nThere are multiple computationally intensive methods to reconstruct the ARG from high-quality sequencing data of modern DNA samples. Ancient samples are of degraded quality due to environmental conditions and contamination; usually they can only be sequenced at very low coverage making the task of incorporating them into the ARG of a set of modern samples challenging. \\n\\nReconstructing an accurate joint ARG between modern and ancient DNA samples has multiple potential applications that we aim to explore. It can be used for ancestry inference, by recovering the ancestry proportion a modern sample inherits from various ancient ancestral groups, enabling us to reconstruct historical events such as population migrations. We can further exploit ARG topology to detect natural selection, by locating regions of the genome that are unusually shared from certain individuals or ancient groups . Finding regions under positive or negative selection, particularly with known biological functionality, can be especially useful in healthcare-related applications and such regions have, for example, been leveraged to determine drug targets in pharmaceutical settings. Finally, the phenotypic impact of variants can be evaluated by testing whether ancestry from certain groups is more closely related to certain phenotypes.\\n\\nThe project's first goal is to build a relatedness inference algorithm that can infer tree topology and TMRCAs between modern and ancient samples and use it to reconstruct a joint ARG with data from the UK BioBank and other sources. Long-range chromosomal regions that are shared across pairs of samples are informative for this analysis but hard to detect in low coverage ancient DNA, so our algorithm will need to to implicitly or explicitly model haplotype sharing despite the lack of phasing information, or in the presence of noisy computational phasing. \\n\\nFor this algorithm, we leverage Deep Learning (DL), which has transformed many scientific fields in the past decade. Population genetics has traditionally focused on developing complex parametric models and has not yet significantly benefited from DL advances. Sequencing data has a spatial structure, so sequences from multiple samples can be stacked to form an image and analysed using computer vision approaches (e.g. Convolutional Neural Networks),adjusting for the fact that sample order is irrelevant (i.e. require exchangeable networks). As we already have access to an ARG for modern samples, our aim is to explore the use of attention- and graph-based methods to extract ARG information that will help infer ancestry between modern and ancient samples.\\n\\nOverall, we expect to make two main contributions. The first is algorithmic development that will allow the use of DL for reconstructing joint genealogical trees for modern and ancient DNA samples, tackling quality issues for the latter. The second is joint ARG inference using real UK Biobank and ancient data. We then aim to analyse this ARG to answer questions relating to natural selection and phenotypic impact of having ancestry from certain ancient groups.","4":"3840A8B5-F1B4-4885-AB48-D9CC62D43066"},{"1":"NE/M013545/1","2":"Sources of Nitrous Acid in the Atmospheric Boundary Layer","3":"Atmospheric chemical processing drives the removal of emitted pollutants, and leads to the formation of ozone and secondary aerosol, which are harmful to human and environmental health, and contribute to climate forcing. Reaction with the OH radical is the primary driver of these oxidation processes; OH abundance must be quantitatively understood in order to accurately predict such effects. In the free troposphere, ozone photolysis is the principal net OH source (neglecting NO-driven HOx cycling); however in the boundary layer a large body of evidence shows that nitrous acid (HONO) is an important, and sometimes the dominant, net OH precursor.\\n\\nWell-understood gas-phase HONO chemistry is not able to explain observed levels of HONO in the boundary layer: large additional sources, forming up to an order of magnitude more HONO, are required - however their identity remains elusive. Recent laboratory work (Su et al., Science 2011; Oswald et al., Science 2013) has identified soils as a globally significant source of HONO - driven, in part, by microbial action (analogous to the well known NO, N2O production), alongside surface NO2-to-HONO conversion mechanisms - but this microbial source has not been explored in the real environment. In urban areas, there is also increasing evidence, from field and chamber studies, that vehicles dominate HONO production - yet no data on HONO production from the UK vehicle fleet exist. \\n\\nPast studies have attempted to constrain HONO production through steady-state approaches, applied to co-located point measurements of OH, NO and HONO. Such analyses are however potentially hampered by the very different atmospheric lifetimes of these species, which dictates that they may not be in equilibrium in complex (spatially heterogeneous) environments. There is an urgent need for robust quantification of HONO sources, in order to quantitatively predict boundary layer HONO and OH abundance, and atmospheric chemical processing affecting air quality.\\n\\n\\nWithin SNAABL, we will directly measure HONO production from (1) natural ground surfaces (including soil production), and (2) road traffic emissions. Our approach will focus upon real-world environmental behaviour, and will avoid the uncertainties associated with analyses of ambient HONO concentrations. \\n\\n(1) Natural Ground Surfaces. We will measure surface HONO fluxes from contrasting agricultural and unmanaged environments, and relate these to NOx and N2O fluxes and physical, chemical atmospheric and soil parameters. Fertiliser manipulation experiments will assess the impact of nutrient addition at a unique field location permitting simultaneous measurement of perturbed- and control systems. We will also perform laboratory studies of natural surface HONO production, using soil cores from our field sites and other UK locations. Through manipulation and selective sterilisation, we will isolate and characterise the potential abiotic and microbial HONO production mechanism(s), including surface processes. \\n\\n(2) Traffic Emissions. We will directly determine HONO production from traffic, through measurement of HONO, NOx and CO2 in a road tunnel, an approach which provides a single, well characterised (video monitoring) source term, and removes the confounding factors of multiple sources, dispersion and photochemistry found in the ambient atmosphere. This approach will reflect the real-world fleet emissions, rather than potentially artificial results from dynamometer driving cycles. \\n\\nWe will use our data to parameterise the resulting HONO source terms, and assess their accuracy, and implications for boundary layer air quality, using photochemical box and regional chemistry-transport modelling. SNAABL will deliver quantitative understanding of HONO production from natural surfaces and vehicle traffic, and so substantially improve the accuracy of predictions of boundary layer atmospheric chemical processing.","4":"A3C9D4AD-4B51-4C5C-BD1B-94A6557F722F"},{"1":"NE/M010554/1","2":"Sources of Nitrous Acid in the Atmospheric Boundary Layer","3":"Atmospheric chemical processing drives the removal of emitted pollutants, and leads to the formation of ozone and secondary aerosol, which are harmful to human and environmental health, and contribute to climate forcing. Reaction with the OH radical is the primary driver of these oxidation processes; OH abundance must be quantitatively understood in order to accurately predict such effects. In the free troposphere, ozone photolysis is the principal net OH source (neglecting NO-driven HOx cycling); however in the boundary layer a large body of evidence shows that nitrous acid (HONO) is an important, and sometimes the dominant, net OH precursor.\\n\\nWell-understood gas-phase HONO chemistry is not able to explain observed levels of HONO in the boundary layer: large additional sources, forming up to an order of magnitude more HONO, are required - however their identity remains elusive. Recent laboratory work (Su et al., Science 2011; Oswald et al., Science 2013) has identified soils as a globally significant source of HONO - driven, in part, by microbial action (analogous to the well known NO, N2O production), alongside surface NO2-to-HONO conversion mechanisms - but this microbial source has not been explored in the real environment. In urban areas, there is also increasing evidence, from field and chamber studies, that vehicles dominate HONO production - yet no data on HONO production from the UK vehicle fleet exist. \\n\\nPast studies have attempted to constrain HONO production through steady-state approaches, applied to co-located point measurements of OH, NO and HONO. Such analyses are however potentially hampered by the very different atmospheric lifetimes of these species, which dictates that they may not be in equilibrium in complex (spatially heterogeneous) environments. There is an urgent need for robust quantification of HONO sources, in order to quantitatively predict boundary layer HONO and OH abundance, and atmospheric chemical processing affecting air quality.\\n\\n\\nWithin SNAABL, we will directly measure HONO production from (1) natural ground surfaces (including soil production), and (2) road traffic emissions. Our approach will focus upon real-world environmental behaviour, and will avoid the uncertainties associated with analyses of ambient HONO concentrations. \\n\\n(1) Natural Ground Surfaces. We will measure surface HONO fluxes from contrasting agricultural and unmanaged environments, and relate these to NOx and N2O fluxes and physical, chemical atmospheric and soil parameters. Fertiliser manipulation experiments will assess the impact of nutrient addition at a unique field location permitting simultaneous measurement of perturbed- and control systems. We will also perform laboratory studies of natural surface HONO production, using soil cores from our field sites and other UK locations. Through manipulation and selective sterilisation, we will isolate and characterise the potential abiotic and microbial HONO production mechanism(s), including surface processes. \\n\\n(2) Traffic Emissions. We will directly determine HONO production from traffic, through measurement of HONO, NOx and CO2 in a road tunnel, an approach which provides a single, well characterised (video monitoring) source term, and removes the confounding factors of multiple sources, dispersion and photochemistry found in the ambient atmosphere. This approach will reflect the real-world fleet emissions, rather than potentially artificial results from dynamometer driving cycles. \\n\\nWe will use our data to parameterise the resulting HONO source terms, and assess their accuracy, and implications for boundary layer air quality, using photochemical box and regional chemistry-transport modelling. SNAABL will deliver quantitative understanding of HONO production from natural surfaces and vehicle traffic, and so substantially improve the accuracy of predictions of boundary layer atmospheric chemical processing.","4":"E2EDA49B-21F6-4B9E-B31B-CFACBCAE61A9"},{"1":"310175","2":"48-GEM-Germany-Artificial Intelligence","3":"Awaiting Public Project Summary","4":"141D4162-C5AB-4245-B827-051A4EE5703D"},{"1":"640016","2":"Global Business Accelerator Programme —USA: Artificial Intelligence","3":"Awaiting Public Project Summary","4":"ED273FD6-039A-49A0-9B1E-5E8DA8593B9A"},{"1":"640015","2":"Global Business Accelerator Programme — Canada: Artificial Intelligence","3":"Awaiting Public Project Summary","4":"3B09D09F-8E0E-4CA2-A07B-74876AF2FD76"},{"1":"105629","2":"Food process efficiency - Optimize product quality using Artificial Intelligence","3":"Awaiting Public Project Summary","4":"9F4383C7-32A2-4961-9A17-F4EDFB4430F1"},{"1":"310156","2":"28-GEM-Canada-Artificial Intelligence and Health, Tools for Medicine Manufacture","3":"Awaiting Public Project Summary","4":"C9D62BDC-C670-4990-BC10-FE1C5DA319EF"},{"1":"106231","2":"The London Medical Imaging &amp; Artificial Intelligence Centre for Value Based Healthcare","3":"Awaiting Public Project Summary","4":"13BFBDF2-20EF-4F61-A468-832AF4C614E0"},{"1":"105620","2":"SALSA [Separation of Additive-Layer Supports by Automation &amp; Artificial Intelligence]","3":"Awaiting Public Project Summary","4":"D61A226E-92A7-414C-BFBA-A65490AA4F9C"},{"1":"105868","2":"Ci-iT: Change impact intelligence Tool","3":"Awaiting Public Project Summary","4":"880A1E6E-2D5C-435B-AA73-1D1933ABFA3B"},{"1":"133583","2":"Widening BU-CERT Threat Intelligence activities (WIDECERT)","3":"Awaiting Public Project Summary","4":"5D0547BD-1032-4D8F-8655-3F22F7CB3969"},{"1":"106302","2":"Artificial Behaviour Based Authentication for IoT","3":"Awaiting Public Project Summary","4":"A2F45CA4-89D8-40F9-A99B-64580711A202"},{"1":"133735","2":"(ACTIVE) Adaptive Cyber Threat Intelligence for Cyber Security InVEstment Optimisation","3":"Awaiting Public Project Summary","4":"13E7030B-3005-4945-9E30-2E9C780A2412"},{"1":"133663","2":"The Adaptive Cyber Threat Intelligence (ACTI) for Security Investment Optimisation","3":"Awaiting Public Project Summary","4":"064A1249-1966-49BA-8D1E-ABBE77FF2DD6"},{"1":"103461","2":"MIRIAM: Machine Intelligence for Radically Improved Additive Manufacturing","3":"Awaiting Public Project Summary","4":"2432A77E-AEA8-491E-8DB1-0D9F0ADCB08F"},{"1":"BB/E01075X/1","2":"Genomic analysis of regulatory networks for bacterial differentiation and multicellular behaviour","3":"Bacteria are single-cell organisms typically viewed as living and acting independently of each other. In fact, most bacteria exist in large communities; a major advantage of this lifestyle is the greatly improved protection of individual cells from environmental stresses such as loss of water and nutrients. Swarming is one of the few main types of community behaviour that we observe across a large number of bacterial species. During swarming, bacteria align themselves in a large group and migrate together over a surface. This is a medically important phenomenon: swarming enables bacteria to travel rapidly to locations in the host organism that are otherwise inaccessible and initiate infections. For example, Proteus mirabilis is a bacterial species that frequently causes life-threatening infections acquired in hospitals. These bacteria swarm over and colonise artificial medical implants to enter the patients' urinary tract and ascend to the kidneys. In addition to its importance in medicine, swarming is an excellent model for studying two fundamental processes in biology: (a) how cells change, or 'differentiate' into distinct cell types that perform specific functions; and (b) how single cells transform into multicellular populations that coordinate their behaviour. Increasing our knowledge of these processes will help us understand how complex, multicellular organisms exist. Cells initiate swarming by sensing contact with a surface and with each other. This triggers a metamorphosis in which cells lengthen 20-fold and build long molecular propellers called flagella that extend outward from the cell surface. These elongated cells (that number in the billions) then align to form large bacterial rafts and migrate away propelled by synchronised flagella rotation. To drive the transition, a complex cascade of molecular signals convert the initial stimuli to activate or repress a specific set of genes required for swarming. We know the identity of many of these genes, such as those responsible for flagella construction. However, it is also clear that several hundred more are involved which are currently unidentified. Additionally, we only have a basic understanding of how incoming molecular signals are transmitted to control the activity of these genes. We will address these problems by building on our and others' previous work in bacterial genetics, bioinformatics and genomics. By using existing approaches and developing new techniques, we will identify the full complement of genes involved in bacterial swarming and uncover the mechanisms controlling them. In doing so, we will reveal new principles underlying cellular differentiation and multicellularity, as well as discover ways to prevent bacterial movement to infection sites.","4":"D4F3EE12-1C8E-46F3-AC5E-F54FA3DB4255"},{"1":"BB/E011489/1","2":"Genomic analysis of regulatory networks for bacterial differentiation and multicellular behaviour","3":"Bacteria are single-cell organisms typically viewed as living and acting independently of each other. In fact, most bacteria exist in large communities; a major advantage of this lifestyle is the greatly improved protection of individual cells from environmental stresses such as loss of water and nutrients. Swarming is one of the few main types of community behaviour that we observe across a large number of bacterial species. During swarming, bacteria align themselves in a large group and migrate together over a surface. This is a medically important phenomenon: swarming enables bacteria to travel rapidly to locations in the host organism that are otherwise inaccessible and initiate infections. For example, Proteus mirabilis is a bacterial species that frequently causes life-threatening infections acquired in hospitals. These bacteria swarm over and colonise artificial medical implants to enter the patients' urinary tract and ascend to the kidneys. In addition to its importance in medicine, swarming is an excellent model for studying two fundamental processes in biology: (a) how cells change, or 'differentiate' into distinct cell types that perform specific functions; and (b) how single cells transform into multicellular populations that coordinate their behaviour. Increasing our knowledge of these processes will help us understand how complex, multicellular organisms exist. Cells initiate swarming by sensing contact with a surface and with each other. This triggers a metamorphosis in which cells lengthen 20-fold and build long molecular propellers called flagella that extend outward from the cell surface. These elongated cells (that number in the billions) then align to form large bacterial rafts and migrate away propelled by synchronised flagella rotation. To drive the transition, a complex cascade of molecular signals convert the initial stimuli to activate or repress a specific set of genes required for swarming. We know the identity of many of these genes, such as those responsible for flagella construction. However, it is also clear that several hundred more are involved which are currently unidentified. Additionally, we only have a basic understanding of how incoming molecular signals are transmitted to control the activity of these genes. We will address these problems by building on our and others' previous work in bacterial genetics, bioinformatics and genomics. By using existing approaches and developing new techniques, we will identify the full complement of genes involved in bacterial swarming and uncover the mechanisms controlling them. In doing so, we will reveal new principles underlying cellular differentiation and multicellularity, as well as discover ways to prevent bacterial movement to infection sites.","4":"1C278C1B-A160-4068-8540-61AC521D2419"},{"1":"BB/F013892/2","2":"Optimisation of perfusion bioreactor for bone tissue growth","3":"Bone tissue engineering is an emerging therapy for treating patients undergoing orthopaedic trauma or disease. The core of the method is the growth of bone tissue on a initial artificial porous scaffold which mimics real bone. The growth is achieved by flowing stem cells through the scaffold until it is replaced by bone tissue which closely resembles the patients own bone. However, much optimisation is needed before this therapy can be implemented. One of the important factors is the number of cells that are placed on scaffold at the start of the several week culture period necessary to create the tissue engineered construct. The correct number and location of starting cells placed onto a scaffold is critical in determining the functionality of the resulting construct. This project aims to optimise cell-seeding methods on the scaffolds, by developing an experimentally validated computer model. The validation, and subsequent investigation, will import real experimental geometries into the flow model; these will be achieved using digital data captured by micro tomography and other methods. The modelling component is a vital element of the proposed project; it overcomes (i) the problem of the inaccessibility of experimental data in complex flow geometries and (ii) the high cost of exploring the potential parameter space experimentally. Expertise from both Keele University (in tissue engineering and bioreactor design) and Sheffield Hallam University (in flow modelling techniques) will be utilised synergistically in order to address the project aims in this joint proposal. Cell type, attachment proteins, scaffold geometry/chemistry, media perfusion rates and mixing techniques will all be analysed in order to investigate the optimal method of cell seeding for bone tissue engineering. The optimised flow model, which will also make timely use of the most recent mathematical modelling information available (eg King, 2005), will then be practically tested in a sterile laboratory environment. Biochemical assessment will be undertaken to determine the efficacy of the predicted, optimised methodology. Utilising modelling techniques in this way, it is possible to significantly reduce time and costs that would otherwise be spent in the laboratory optimising these essential parameters for tissue engineering.","4":"BB963ADA-5FAA-46CD-89F3-F59F18BFDF94"},{"1":"BB/F013744/1","2":"Optimisation of perfusion bioreactor for bone tissue growth","3":"Bone tissue engineering is an emerging therapy for treating patients undergoing orthopaedic trauma or disease. The core of the method is the growth of bone tissue on a initial artificial porous scaffold which mimics real bone. The growth is achieved by flowing stem cells through the scaffold until it is replaced by bone tissue which closely resembles the patients own bone. However, much optimisation is needed before this therapy can be implemented. One of the important factors is the number of cells that are placed on scaffold at the start of the several week culture period necessary to create the tissue engineered construct. The correct number and location of starting cells placed onto a scaffold is critical in determining the functionality of the resulting construct. This project aims to optimise cell-seeding methods on the scaffolds, by developing an experimentally validated computer model. The validation, and subsequent investigation, will import real experimental geometries into the flow model; these will be achieved using digital data captured by micro tomography and other methods. The modelling component is a vital element of the proposed project; it overcomes (i) the problem of the inaccessibility of experimental data in complex flow geometries and (ii) the high cost of exploring the potential parameter space experimentally. Expertise from both Keele University (in tissue engineering and bioreactor design) and Sheffield Hallam University (in flow modelling techniques) will be utilised synergistically in order to address the project aims in this joint proposal. Cell type, attachment proteins, scaffold geometry/chemistry, media perfusion rates and mixing techniques will all be analysed in order to investigate the optimal method of cell seeding for bone tissue engineering. The optimised flow model, which will also make timely use of the most recent mathematical modelling information available (eg King, 2005), will then be practically tested in a sterile laboratory environment. Biochemical assessment will be undertaken to determine the efficacy of the predicted, optimised methodology. Utilising modelling techniques in this way, it is possible to significantly reduce time and costs that would otherwise be spent in the laboratory optimising these essential parameters for tissue engineering.","4":"920CD3E5-BB9B-4F58-AECD-2003355B4594"},{"1":"2634840","2":"Bayesian methods for learning and satisfaction of safety constraints","3":"Brief description of the context of the research including potential impact\\nGiven a goal in the real world, autonomous agents may come up with solutions that are undesirable to humans, ranging from mildly inconveniencing to life-threatening. This could be addressed by a set of constraints on their \\nbehaviour. However, especially if such constraints are supposed to represent complex human preferences, they will themselves need to be inferred from various sources (such as human behaviour or explicit feedback) and \\nthey will never be known with certainty. Probabilistic modeling can account for uncertainty both about the constraints themselves (what outcomes and behaviours should be avoided) and about the world dynamics (which outcomes a behaviour could lead to); however, the resulting Bayesian optimization problem runs into tractability issues. Various methods stemming from inverse reinforcement learning have recently been proposed for learning human preferences, including learning constraints. However, most proposed algorithms in this area deviate from the Bayesian framework in ways that do not preserve its desirable properties such as calibration, which may be essential especially for \\nsafety-critical constraints.\\nAims and Objectives\\nThis DPhil project will try to develop alternatives to these preference and constraint learning algorithms using principled Bayesian approaches, drawing on methods from areas such as Gaussian processes, Bayesian \\noptimization, and Bayesian inverse reinforcement learning. It will also examine methods for probabilistic planning and probabilistic verification to ensure that once a set of preferences and constraints is inferred, we can \\nguarantee their satisfaction with a sufficient degree of confidence. \\nNovelty of the research methodology\\nThe whole literature on inferring constraints from observations of an agent's behaviour is limited, but there is\\nalmost no work using Bayesian methods. This project will partly aim to fill this gap. Similarly, the work will \\naim to fill gaps in principled approaches for probabilistic guarantees on constraint satisfaction in forward \\nbehaviour planning. \\nAlignment to EPSRC's strategies and research areas\\nThe project naturally fits under EPSRC's research areas of artificial intelligence technologies and verification \\nand correctness, with possible overlaps also with control engineering whose techniques it might use to provide \\nsafety guarantees for autonomous AI systems. Algorithms developed as part of the project would also have\\nnatural applications in the research area of robotics.\\nCompanies or collaborators involved\\nNo external collaboration is currently arranged.","4":"4C291054-DCFB-457C-BE33-15F853D184DD"},{"1":"2416389","2":"Bayesian methods for learning and satisfaction of safety constraints","3":"Brief description of the context of the research including potential impact\\nGiven a goal in the real world, autonomous agents may come up with solutions that are undesirable to humans, ranging from mildly inconveniencing to life-threatening. This could be addressed by a set of constraints on their \\nbehaviour. However, especially if such constraints are supposed to represent complex human preferences, they will themselves need to be inferred from various sources (such as human behaviour or explicit feedback) and \\nthey will never be known with certainty. Probabilistic modeling can account for uncertainty both about the constraints themselves (what outcomes and behaviours should be avoided) and about the world dynamics (which outcomes a behaviour could lead to); however, the resulting Bayesian optimization problem runs into tractability issues. Various methods stemming from inverse reinforcement learning have recently been proposed for learning human preferences, including learning constraints. However, most proposed algorithms in this area deviate from the Bayesian framework in ways that do not preserve its desirable properties such as calibration, which may be essential especially for \\nsafety-critical constraints.\\nAims and Objectives\\nThis DPhil project will try to develop alternatives to these preference and constraint learning algorithms using principled Bayesian approaches, drawing on methods from areas such as Gaussian processes, Bayesian \\noptimization, and Bayesian inverse reinforcement learning. It will also examine methods for probabilistic planning and probabilistic verification to ensure that once a set of preferences and constraints is inferred, we can \\nguarantee their satisfaction with a sufficient degree of confidence. \\nNovelty of the research methodology\\nThe whole literature on inferring constraints from observations of an agent's behaviour is limited, but there is\\nalmost no work using Bayesian methods. This project will partly aim to fill this gap. Similarly, the work will \\naim to fill gaps in principled approaches for probabilistic guarantees on constraint satisfaction in forward \\nbehaviour planning. \\nAlignment to EPSRC's strategies and research areas\\nThe project naturally fits under EPSRC's research areas of artificial intelligence technologies and verification \\nand correctness, with possible overlaps also with control engineering whose techniques it might use to provide \\nsafety guarantees for autonomous AI systems. Algorithms developed as part of the project would also have\\nnatural applications in the research area of robotics.\\nCompanies or collaborators involved\\nNo external collaboration is currently arranged.","4":"79235F3C-B376-4A5D-8627-E4D595F00558"},{"1":"2426790","2":"The Use of PET/CT for the Tracking of Positron Labelled Cell Therapies to Investigate Cell Distribution and Therapeutic Efficacy in the Diseased Lung","3":"Brief description of the context of the research including potential impact \\n\\nCell therapies provide potential treatments for lung cancer and chronic respiratory diseases that that have limited treatment options and poor survival rates. However, the distribution of cells in patients is currently unknown and unpredictable due to the complexity of cell/host interactions. We have validated 89-Zirconium (89Zr)-oxine as a rapid cell label agent for PET/CT in mesenchymal stem cells (MSCs) at clinical doses for their application in 3 patients in an imaging arm of Phase II of the UCL TACTICAL (NCT03298763: fully funded by the MRC and the imaging arm by the JP Moulton Charity Trust). However, 89Zr is not yet widely used in clinical practice and accurate image quantitation of positron-based radiotracers within the lung is challenging. This first-in-man study will be used to establish 89Zr as an clinical cell labelling imaging agent and develop image analysis methods to map the bio-distribution and pharmacokinetics of a genetically-modified stem cell product, providing safety and efficacy data that can be used in the development and optimisation of cell therapies for patients. \\n\\nAims and Objectives \\n -The specific objectives are to: \\n\\nThis project aims to develop image analysis methods to clearly understand 89Zr-labelled cell localization and distribution to provide quantitative measures applicable to both healthy and diseased lung tissue using data acquired from both preclinical and clinical PET/CT systems. Our specific aims are to: \\n\\ni) To develop the tools needed to accurately track 89Zr radiolabelled cells in pre-clinical studies using PET/CT. \\n\\nii) To apply the methods to two different cell types and respiratory diseases to determine if quantitative changes in uptake can characterise the potential lung cell repair caused by cell treatments. \\n\\niii) To validate these techniques pre-clinically and to determine the potential of the using radiolabelled cells in humans with lung disease. \\n\\niv) To translate these techniques into humans and validate the ability to track radiolabelled MSC-TRAIL in humans. \\n\\nNovelty of Research Methodology. \\n\\n89Zr-oxine is a rapid cell labelling technology for PET imaging which we have validated for clinical translation in mesenchymal stem cells (cells) as part of the UCL TACTICAL clinical trial. However, 89Zr is not yet widely used in clinical practice and this will be the first study that has utilized an 89Zr-labelled cell product in human. It is therefore important to ensure that accurate image quantitation 89Zr-labelled cells are developed, validated, and correlated with imaging outcomes of disease progression. As positron-based radiotracers within the lung is challenging the imaging analysis methods obtained will also be of high valuable to the nuclear physics community as a whole. \\n\\nAlignment to EPSRC's strategies and research areas \\n\\nOur project is aligned to the &quot;Developing Future Therapies&quot; strategy (Healthcare technologies) by supporting the development of novel cell therapies by using imaging technologies to provide safety and efficacy data thereby reducing risk to patients. \\n\\nThe project also aligns with the following research areas: Medical Imaging (including medical image and vision computing), artificial intelligence technologies, biomaterials and tissue engineering. \\n\\nAny companies or collaborators involved \\n\\nGlaxoSmithKline (GSK), Royal Free Hospital (second supervisor Beverley Holman), Prof Sam Janes (leader of TACTICAL trail).","4":"73EA850B-62B0-4055-BCA3-1D24948890B8"},{"1":"2451625","2":"The Use of PET/CT for the Tracking of Positron Labelled Cell Therapies to Investigate Cell Distribution and Therapeutic Efficacy in the Diseased Lung","3":"Brief description of the context of the research including potential impact \\n\\nCell therapies provide potential treatments for lung cancer and chronic respiratory diseases that that have limited treatment options and poor survival rates. However, the distribution of cells in patients is currently unknown and unpredictable due to the complexity of cell/host interactions. We have validated 89-Zirconium (89Zr)-oxine as a rapid cell label agent for PET/CT in mesenchymal stem cells (MSCs) at clinical doses for their application in 3 patients in an imaging arm of Phase II of the UCL TACTICAL (NCT03298763: fully funded by the MRC and the imaging arm by the JP Moulton Charity Trust). However, 89Zr is not yet widely used in clinical practice and accurate image quantitation of positron-based radiotracers within the lung is challenging. This first-in-man study will be used to establish 89Zr as an clinical cell labelling imaging agent and develop image analysis methods to map the bio-distribution and pharmacokinetics of a genetically-modified stem cell product, providing safety and efficacy data that can be used in the development and optimisation of cell therapies for patients. \\n\\nAims and Objectives \\n -The specific objectives are to: \\n\\nThis project aims to develop image analysis methods to clearly understand 89Zr-labelled cell localization and distribution to provide quantitative measures applicable to both healthy and diseased lung tissue using data acquired from both preclinical and clinical PET/CT systems. Our specific aims are to: \\n\\ni) To develop the tools needed to accurately track 89Zr radiolabelled cells in pre-clinical studies using PET/CT. \\n\\nii) To apply the methods to two different cell types and respiratory diseases to determine if quantitative changes in uptake can characterise the potential lung cell repair caused by cell treatments. \\n\\niii) To validate these techniques pre-clinically and to determine the potential of the using radiolabelled cells in humans with lung disease. \\n\\niv) To translate these techniques into humans and validate the ability to track radiolabelled MSC-TRAIL in humans. \\n\\nNovelty of Research Methodology. \\n\\n89Zr-oxine is a rapid cell labelling technology for PET imaging which we have validated for clinical translation in mesenchymal stem cells (cells) as part of the UCL TACTICAL clinical trial. However, 89Zr is not yet widely used in clinical practice and this will be the first study that has utilized an 89Zr-labelled cell product in human. It is therefore important to ensure that accurate image quantitation 89Zr-labelled cells are developed, validated, and correlated with imaging outcomes of disease progression. As positron-based radiotracers within the lung is challenging the imaging analysis methods obtained will also be of high valuable to the nuclear physics community as a whole. \\n\\nAlignment to EPSRC's strategies and research areas \\n\\nOur project is aligned to the &quot;Developing Future Therapies&quot; strategy (Healthcare technologies) by supporting the development of novel cell therapies by using imaging technologies to provide safety and efficacy data thereby reducing risk to patients. \\n\\nThe project also aligns with the following research areas: Medical Imaging (including medical image and vision computing), artificial intelligence technologies, biomaterials and tissue engineering. \\n\\nAny companies or collaborators involved \\n\\nGlaxoSmithKline (GSK), Royal Free Hospital (second supervisor Beverley Holman), Prof Sam Janes (leader of TACTICAL trail).","4":"EBC03495-073B-4EBD-B8E6-4C8146A34BCF"},{"1":"74075","2":"CAPE - Cardiac Analysis for Pressure Establishment","3":"CAPE is a Machine Learning Project using Artificial Intelligence to probe the world of ECG signals.\\n\\nWe use the power of Google's Deepmind to probe, analyse and pattern match almost 100,000 publicly accessible ECG signals.\\n\\nThe aims of CAPE are to establish if there are hidden bio-markers within the ECG signals that can help us understand the cardiovascular system better. This could be understanding how the vascular system is operating in terms of arterial elasticity, blood viscosity etc.\\n\\nCAPE builds on the companies works to date of establishing multiple physiological parameters from wearable technologies such as smart-watches and fit bits.\\n\\nWe use the latest Artificial Intelligence thinking to look for patterns from one physiological parameter to another, which may not have been recorded. This enables us to create a system where the whole health of the individual may be recorded continuously and trended allowing for true performance scoring to take place.\\n\\nUsing data visualisation software from F1 Motorsport we link the data that CAPE is processing to a realisation system allowing us to visualise how one part of the physiological system is affecting the other.\\n\\nThe outputs of CAPE are simply an algorithm, but one that can see predict physiological performance based on the ECG system data being fed into it.\\n\\nThis is novel and is very applicable to the smart-watch market with Apple enabling the Apple Watch 4, in Europe, to record ECG in the last few months.\\n\\nWe all live in a busy world where getting to see a doctor and having readings on our physiology performed in the doctors surgery are getting harder to achieve owing to demand our stripping supply. We believe that the works undertaken as part of the CAPE project will allow for some of these performance physiological markers to be recorded on smart-watches and uploaded to the cloud to provide trending and analysis that can then be reviewed online by your GP or family doctor.\\n\\nWe are in effect through CAPE working to optimise the way in which we acquire and present physiological performance data to our clinicians. We still have a long way to go, but CAPE provides an exciting look into the world of physiological monitoring and using Machine Learning to see if there are aspects of the ECG signal - the hearts electrical system - that we currently do not understand or even realise that they are there.","4":"FD3E1117-4615-4CCD-B293-BFF02A68FB51"},{"1":"105762","2":"CAPE - Cardiac Analysis for Pressure Establishment","3":"CAPE is a Machine Learning Project using Artificial Intelligence to probe the world of ECG signals.\\n\\nWe use the power of Google's Deepmind to probe, analyse and pattern match almost 100,000 publicly accessible ECG signals.\\n\\nThe aims of CAPE are to establish if there are hidden bio-markers within the ECG signals that can help us understand the cardiovascular system better. This could be understanding how the vascular system is operating in terms of arterial elasticity, blood viscosity etc.\\n\\nCAPE builds on the companies works to date of establishing multiple physiological parameters from wearable technologies such as smart-watches and fit bits.\\n\\nWe use the latest Artificial Intelligence thinking to look for patterns from one physiological parameter to another, which may not have been recorded. This enables us to create a system where the whole health of the individual may be recorded continuously and trended allowing for true performance scoring to take place.\\n\\nUsing data visualisation software from F1 Motorsport we link the data that CAPE is processing to a realisation system allowing us to visualise how one part of the physiological system is affecting the other.\\n\\nThe outputs of CAPE are simply an algorithm, but one that can see predict physiological performance based on the ECG system data being fed into it.\\n\\nThis is novel and is very applicable to the smart-watch market with Apple enabling the Apple Watch 4, in Europe, to record ECG in the last few months.\\n\\nWe all live in a busy world where getting to see a doctor and having readings on our physiology performed in the doctors surgery are getting harder to achieve owing to demand our stripping supply. We believe that the works undertaken as part of the CAPE project will allow for some of these performance physiological markers to be recorded on smart-watches and uploaded to the cloud to provide trending and analysis that can then be reviewed online by your GP or family doctor.\\n\\nWe are in effect through CAPE working to optimise the way in which we acquire and present physiological performance data to our clinicians. We still have a long way to go, but CAPE provides an exciting look into the world of physiological monitoring and using Machine Learning to see if there are aspects of the ECG signal - the hearts electrical system - that we currently do not understand or even realise that they are there.","4":"3049768D-8451-4A56-B36B-76CC232173CC"},{"1":"NE/K003941/1","2":"Reactions of Stabilised Criegee Intermediates in the Atmosphere: Implications for Tropospheric Composition &amp; Climate","3":"Chemical reactions govern the rate of removal of many primary species emitted into the atmosphere, and control the production of secondary species. The dominant atmospheric oxidant is the OH radical; reaction with OH initiates the removal of many organic compounds, nitrogen oxides and other species such as sulphur dioxide (SO2). In the case of SO2, gas-phase oxidation by OH produces sulphuric acid, which increases aerosol mass, and may also act as a nucleating agent, forming new particles in the atmosphere - affecting climate by directly scattering solar radiation, and indirectly by affecting could droplet formation, making very substantial cooling contributions. Understanding oxidation rates is critical to accurate prediction of the impacts of these factors upon atmospheric composition and climate.\\n \\nThis project will determine the importance of an additional potential atmospheric oxidant: reactions with stabilised criegee intermediates (SCIs), formed from the ozonolysis of alkenes.\\n\\nOzone can act as a direct oxidising agent, reacting with alkenes (species with one or more double bonds). This class of compounds includes most biogenic reactive carbon emissions, which dominate the organic compounds released to the atmosphere. Gas-phase ozone-alkene reactions produce reactive intermediates, SCIs, which have lifetimes of a few seconds (or less - this is a critical uncertainty) in the atmosphere. It has been known for some time that SCIs can react with other species, notably including SO2; however the current generally accepted wisdom is that reaction with water vapour, or decomposition, dominates the removal of SCIs in the troposphere, and so they are not considered to be important oxidants.\\n\\nA number of recent pieces of evidence are changing this picture - model studies pointing to missing SO2 oxidation mechanisms; field and chamber studies pointing to enhanced SO2 oxidation in the presence of elevated levels of alkenes, and recent lab. studies which found that reactions of at least one SCI species with SO2 and NO2 are very fast, and with H2O very slow (at least under the specific experimental conditions considered). If this conclusion is generalised, simple calculations indicate that SCI reactions would be comparable to those of OH for the gas-phase oxidation of SO2 in the boundary layer. The associated sulphate aerosol increase would imply a significant change to radiative forcing calculations. Similarly, enhanced oxidation of NO2 would lead to increased nitrate production. Critically however, the recent results are not consistent with previous laboratory studies of the SCI reaction system, potentially as a consequence of differences in approach and conditions (reagent abundance, pressure, timescales etc.) which diverge substantially from those of relevance to the atmosphere.\\n\\nIn this project, we will apply a new approach to this critical and timely issue: application of an atmospheric simulation chamber to directly assess the importance of SCIs as oxidants. We will use the EUPHORE (European Photoreactor) chamber, which will allow us to replicate ambient conditions (using both artificial and real air samples), produce SCIs in a manner identical to their formation in the atmosphere (i.e. through alkene ozonolysis) and directly monitor their impacts upon SO2 and NO2. This approach will avoid the uncertainties of (large) extrapolation which affect interpretation of previous studies. \\n\\nOur experiments will confirm (or otherwise) the importance of SCI reactions through experiments which replicate the real atmosphere and may be analysed by direct inspection; in addition we will determine kinetic parameters for the reactions of a range of SCI species, which will be used to revise the mechanism for SCI formation in atmospheric chemical models. We will then apply to such models (the MCM and GEOS-Chem) to quantify the contribution of SCI reactions to atmospheric oxidation on both local and global scales.","4":"0196E3B6-59FB-47E2-933A-470823A41DD6"},{"1":"NE/K005448/1","2":"Reactions of Stabilised Criegee Intermediates in the Atmosphere: Implications for Tropospheric Composition &amp; Climate","3":"Chemical reactions govern the rate of removal of many primary species emitted into the atmosphere, and control the production of secondary species. The dominant atmospheric oxidant is the OH radical; reaction with OH initiates the removal of many organic compounds, nitrogen oxides and other species such as sulphur dioxide (SO2). In the case of SO2, gas-phase oxidation by OH produces sulphuric acid, which increases aerosol mass, and may also act as a nucleating agent, forming new particles in the atmosphere - affecting climate by directly scattering solar radiation, and indirectly by affecting could droplet formation, making very substantial cooling contributions. Understanding oxidation rates is critical to accurate prediction of the impacts of these factors upon atmospheric composition and climate.\\n \\nThis project will determine the importance of an additional potential atmospheric oxidant: reactions with stabilised criegee intermediates (SCIs), formed from the ozonolysis of alkenes.\\n\\nOzone can act as a direct oxidising agent, reacting with alkenes (species with one or more double bonds). This class of compounds includes most biogenic reactive carbon emissions, which dominate the organic compounds released to the atmosphere. Gas-phase ozone-alkene reactions produce reactive intermediates, SCIs, which have lifetimes of a few seconds (or less - this is a critical uncertainty) in the atmosphere. It has been known for some time that SCIs can react with other species, notably including SO2; however the current generally accepted wisdom is that reaction with water vapour, or decomposition, dominates the removal of SCIs in the troposphere, and so they are not considered to be important oxidants.\\n\\nA number of recent pieces of evidence are changing this picture - model studies pointing to missing SO2 oxidation mechanisms; field and chamber studies pointing to enhanced SO2 oxidation in the presence of elevated levels of alkenes, and recent lab. studies which found that reactions of at least one SCI species with SO2 and NO2 are very fast, and with H2O very slow (at least under the specific experimental conditions considered). If this conclusion is generalised, simple calculations indicate that SCI reactions would be comparable to those of OH for the gas-phase oxidation of SO2 in the boundary layer. The associated sulphate aerosol increase would imply a significant change to radiative forcing calculations. Similarly, enhanced oxidation of NO2 would lead to increased nitrate production. Critically however, the recent results are not consistent with previous laboratory studies of the SCI reaction system, potentially as a consequence of differences in approach and conditions (reagent abundance, pressure, timescales etc.) which diverge substantially from those of relevance to the atmosphere.\\n\\nIn this project, we will apply a new approach to this critical and timely issue: application of an atmospheric simulation chamber to directly assess the importance of SCIs as oxidants. We will use the EUPHORE (European Photoreactor) chamber, which will allow us to replicate ambient conditions (using both artificial and real air samples), produce SCIs in a manner identical to their formation in the atmosphere (i.e. through alkene ozonolysis) and directly monitor their impacts upon SO2 and NO2. This approach will avoid the uncertainties of (large) extrapolation which affect interpretation of previous studies. \\n\\nOur experiments will confirm (or otherwise) the importance of SCI reactions through experiments which replicate the real atmosphere and may be analysed by direct inspection; in addition we will determine kinetic parameters for the reactions of a range of SCI species, which will be used to revise the mechanism for SCI formation in atmospheric chemical models. We will then apply to such models (the MCM and GEOS-Chem) to quantify the contribution of SCI reactions to atmospheric oxidation on both local and global scales.","4":"3BBEDDEF-2E22-4DA6-881C-AABF7BB6C99B"},{"1":"NE/J023736/1","2":"Fragility of stream ecosystem functioning in response to drought: an experimental test","3":"Climate change and human activities are expected to change the quantity of water entering rivers and streams, with potentially dramatic impacts on animals and plants resident in these ecosystems. In many regions, climate change is expected to reduce rainfall and bring about drought conditions, and water abstraction and river diversions may also reduce flows in rivers and streams. To date, relatively little work has been done to determine the effect of hydrologic droughts on aquatic biodiversity, and less is known about impacts on important processes, such as decomposition and nutrient cycling, that affect water quality and productivity of aquatic life.\\n\\nOur study will use novel experiments to understand the ecological effects of hydrologic droughts in streams, with a view to predicting future change. We will use a series of artificial stream channels to directly manipulate flows, thereby simulating drought episodes, and measure the responses of flora and fauna, and a series of processes that reflect the ecological health of the ecosystem. We will establish a series of experimental drought treatments which differ in the extent of flow reduction, from unaltered reference conditions to extreme low flows that cause habitat loss. We will also examine how the physical nature of the stream bed affects the extent to which animals and plants can withstand periods of drought, and how quickly these communities recover from these events. With a project student, we will investigate how water abstraction, a leading anthropogenic cause of stream drought, affects biodiversity and functioning across a suite of lowland streams in south west England. Together, the results will give valuable insights into the ways in which the environment responds to change brought about through human activities and the likely effects of climate change.","4":"0BA0BBE2-241A-40F0-BBE9-4136EE49F9FF"},{"1":"NE/J02256X/1","2":"Fragility of stream ecosystem functioning in response to drought: an experimental test","3":"Climate change and human activities are expected to change the quantity of water entering rivers and streams, with potentially dramatic impacts on animals and plants resident in these ecosystems. In many regions, climate change is expected to reduce rainfall and bring about drought conditions, and water abstraction and river diversions may also reduce flows in rivers and streams. To date, relatively little work has been done to determine the effect of hydrologic droughts on aquatic biodiversity, and less is known about impacts on important processes, such as decomposition and nutrient cycling, that affect water quality and productivity of aquatic life.\\n\\nOur study will use novel experiments to understand the ecological effects of hydrologic droughts in streams, with a view to predicting future change. We will use a series of artificial stream channels to directly manipulate flows, thereby simulating drought episodes, and measure the responses of flora and fauna, and a series of processes that reflect the ecological health of the ecosystem. We will establish a series of experimental drought treatments which differ in the extent of flow reduction, from unaltered reference conditions to extreme low flows that cause habitat loss. We will also examine how the physical nature of the stream bed affects the extent to which animals and plants can withstand periods of drought, and how quickly these communities recover from these events. With a project student, we will investigate how water abstraction, a leading anthropogenic cause of stream drought, affects biodiversity and functioning across a suite of lowland streams in south west England. Together, the results will give valuable insights into the ways in which the environment responds to change brought about through human activities and the likely effects of climate change.","4":"94364497-6563-45A3-8D7B-ADACC0EAF3CC"},{"1":"NE/H013881/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS)","3":"Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"1CFD473D-00F9-4532-8AE0-C53794A93EA7"},{"1":"NE/H013911/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage","3":"Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"9324A805-CCF3-49EC-B69D-A89B970F2B7A"},{"1":"NE/H013989/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS).","3":"Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"A20A40F0-FD3F-43C1-99D3-96546D9F6461"},{"1":"NE/H013873/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage","3":"Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"88A0733C-96E8-4143-B05C-81302D22295A"},{"1":"NE/S003568/1","2":"Artificial Light Impacts on Coastal Ecosystems (ALICE)","3":"Coastlines are illuminated with artificial light at night (ALAN) from piers, promenades, ports harbours, and dockyards. Artificial sky glow created by lighting from coastal settlements can now be detected above 22% of the world's coasts nightly, and will dramatically increase as coastal human populations more than double by year 2060. Life history adaptations to the moon and sun are near ubiquitous in the upper 200m of the sea, such that cycle's and gradients of light intensity and colour are major structuring factors in marine ecosystems. The potential for ALAN to reshape the ecology of coastal habitats by interfering with natural light cycles and the biological processes they inform is increasingly recognised.\\n\\nMarine invertebrates are extremely sensitive to natural light throughout their life cycle. Examples include synchronised broadcast spawning in reef corals informed by moonlight cycles, zooplankton sensitivity to moonlight at &gt;100m depth, and phototaxis of larvae under light equivalent to moonless overcast nights. The reproductive, larval and adult phases of marine invertebrates are all affected by night-time lighting of equivalent illuminances to those found in ports and harbours. Further, direct impacts on organism behaviour can indirectly affect other species in coastal food web's, changing ecosystem structure. The potential for coastal ALAN to disrupt marine organisms, species interactions, population dynamics, and organism distributions is clear.\\nThe growing use of white Light Emitting Diodes (LEDs) (69% of global lighting by 2020) will exacerbate ALAN's impacts. LEDs emit more blue wavelength light that: i) penetrates deeper into seawater compared to older lighting technologies; and ii) many marine organism responses are most sensitive to. Tailoring LEDs to avoid blue wavelengths represents one mitigation option trialled on land that can be improved by investigating the spectral dependence of biological responses.\\nALICE will tackle fundamental gaps in our understanding of marine ecosystem responses to ALAN, by carrying out the following research: -\\n1. Laboratory experiments to determine the impacts of ALAN on coastal organisms: Parallel experiments will quantify the impacts of ALAN interference with natural light cycles on the life history responses of marine invertebrates. These relationships will be used to model the growth rate of marine invertebrate populations exposed to different intensities of cool white LED light assuming optimal conditions with no predators or competitors.\\n2. Laboratory experiments to determine the impact of ALAN on species interactions: The relationships between white LED light intensity, and species interactions (predation,competition and mutualism) will be simultaneously quantified during the above experiments, and used to model the impacts of ALAN on marine invertebrate populations accounting for their relationships with one another in nature.\\n3. Mapping and modelling the distribution of ALAN in coastal marine habitats: The intensity of colour composition of ALAN in coastal waters will be mapped across three contrastingly urbanised UK estuaries. These data, and associated optical modelling, will be used with satellite data to globally map ALAN intensity from the sea surface to a depth of 100m.\\n4. Modelling ALAN impacts on species distributions: The population models (1,2) and the ALAN distribution model (3), will allow a synthesis assessment of long term changes in species distributions that may result from ALAN impacts. \\n5. Quantifying the benefits of avoiding ALAN wavelengths: we will quantify the ecological benefits of: i) removing blue light form LEDs blue using optical filters; ii) replacing white, with longer wavelength Amber LEDs. In addition we will quantify the responses of marine invertebrate larvae to different colours of light, so that the design of ecologically friendly LED lighting can be better informed.","4":"114E5E1C-11E5-49AE-9D63-EB8A8F2283A9"},{"1":"NE/S003517/1","2":"Artificial Light Impacts on Coastal Ecosystems (ALICE)","3":"Coastlines are illuminated with artificial light at night (ALAN) from piers, promenades, ports harbours, and dockyards. Artificial sky glow created by lighting from coastal settlements can now be detected above 22% of the world's coasts nightly, and will dramatically increase as coastal human populations more than double by year 2060. Life history adaptations to the moon and sun are near ubiquitous in the upper 200m of the sea, such that cycle's and gradients of light intensity and colour are major structuring factors in marine ecosystems. The potential for ALAN to reshape the ecology of coastal habitats by interfering with natural light cycles and the biological processes they inform is increasingly recognised.\\n\\nMarine invertebrates are extremely sensitive to natural light throughout their life cycle. Examples include synchronised broadcast spawning in reef corals informed by moonlight cycles, zooplankton sensitivity to moonlight at &gt;100m depth, and phototaxis of larvae under light equivalent to moonless overcast nights. The reproductive, larval and adult phases of marine invertebrates are all affected by night-time lighting of equivalent illuminances to those found in ports and harbours. Further, direct impacts on organism behaviour can indirectly affect other species in coastal food web's, changing ecosystem structure. The potential for coastal ALAN to disrupt marine organisms, species interactions, population dynamics, and organism distributions is clear.\\nThe growing use of white Light Emitting Diodes (LEDs) (69% of global lighting by 2020) will exacerbate ALAN's impacts. LEDs emit more blue wavelength light that: i) penetrates deeper into seawater compared to older lighting technologies; and ii) many marine organism responses are most sensitive to. Tailoring LEDs to avoid blue wavelengths represents one mitigation option trialled on land that can be improved by investigating the spectral dependence of biological responses.\\nALICE will tackle fundamental gaps in our understanding of marine ecosystem responses to ALAN, by carrying out the following research: -\\n1. Laboratory experiments to determine the impacts of ALAN on coastal organisms: Parallel experiments will quantify the impacts of ALAN interference with natural light cycles on the life history responses of marine invertebrates. These relationships will be used to model the growth rate of marine invertebrate populations exposed to different intensities of cool white LED light assuming optimal conditions with no predators or competitors.\\n2. Laboratory experiments to determine the impact of ALAN on species interactions: The relationships between white LED light intensity, and species interactions (predation,competition and mutualism) will be simultaneously quantified during the above experiments, and used to model the impacts of ALAN on marine invertebrate populations accounting for their relationships with one another in nature.\\n3. Mapping and modelling the distribution of ALAN in coastal marine habitats: The intensity of colour composition of ALAN in coastal waters will be mapped across three contrastingly urbanised UK estuaries. These data, and associated optical modelling, will be used with satellite data to globally map ALAN intensity from the sea surface to a depth of 100m.\\n4. Modelling ALAN impacts on species distributions: The population models (1,2) and the ALAN distribution model (3), will allow a synthesis assessment of long term changes in species distributions that may result from ALAN impacts. \\n5. Quantifying the benefits of avoiding ALAN wavelengths: we will quantify the ecological benefits of: i) removing blue light form LEDs blue using optical filters; ii) replacing white, with longer wavelength Amber LEDs. In addition we will quantify the responses of marine invertebrate larvae to different colours of light, so that the design of ecologically friendly LED lighting can be better informed.","4":"38E659F8-54BC-49D2-BE76-00DE8A382436"},{"1":"NE/S003525/1","2":"Artificial Light Impacts on Coastal Ecosystems (ALICE)","3":"Coastlines are illuminated with artificial light at night (ALAN) from piers, promenades, ports harbours, and dockyards. Artificial sky glow created by lighting from coastal settlements can now be detected above 22% of the world's coasts nightly, and will dramatically increase as coastal human populations more than double by year 2060. Life history adaptations to the moon and sun are near ubiquitous in the upper 200m of the sea, such that cycle's and gradients of light intensity and colour are major structuring factors in marine ecosystems. The potential for ALAN to reshape the ecology of coastal habitats by interfering with natural light cycles and the biological processes they inform is increasingly recognised.\\n\\nMarine invertebrates are extremely sensitive to natural light throughout their life cycle. Examples include synchronised broadcast spawning in reef corals informed by moonlight cycles, zooplankton sensitivity to moonlight at &gt;100m depth, and phototaxis of larvae under light equivalent to moonless overcast nights. The reproductive, larval and adult phases of marine invertebrates are all affected by night-time lighting of equivalent illuminances to those found in ports and harbours. Further, direct impacts on organism behaviour can indirectly affect other species in coastal food web's, changing ecosystem structure. The potential for coastal ALAN to disrupt marine organisms, species interactions, population dynamics, and organism distributions is clear.\\nThe growing use of white Light Emitting Diodes (LEDs) (69% of global lighting by 2020) will exacerbate ALAN's impacts. LEDs emit more blue wavelength light that: i) penetrates deeper into seawater compared to older lighting technologies; and ii) many marine organism responses are most sensitive to. Tailoring LEDs to avoid blue wavelengths represents one mitigation option trialled on land that can be improved by investigating the spectral dependence of biological responses.\\nALICE will tackle fundamental gaps in our understanding of marine ecosystem responses to ALAN, by carrying out the following research: -\\n1. Laboratory experiments to determine the impacts of ALAN on coastal organisms: Parallel experiments will quantify the impacts of ALAN interference with natural light cycles on the life history responses of marine invertebrates. These relationships will be used to model the growth rate of marine invertebrate populations exposed to different intensities of cool white LED light assuming optimal conditions with no predators or competitors.\\n2. Laboratory experiments to determine the impact of ALAN on species interactions: The relationships between white LED light intensity, and species interactions (predation,competition and mutualism) will be simultaneously quantified during the above experiments, and used to model the impacts of ALAN on marine invertebrate populations accounting for their relationships with one another in nature.\\n3. Mapping and modelling the distribution of ALAN in coastal marine habitats: The intensity of colour composition of ALAN in coastal waters will be mapped across three contrastingly urbanised UK estuaries. These data, and associated optical modelling, will be used with satellite data to globally map ALAN intensity from the sea surface to a depth of 100m.\\n4. Modelling ALAN impacts on species distributions: The population models (1,2) and the ALAN distribution model (3), will allow a synthesis assessment of long term changes in species distributions that may result from ALAN impacts. \\n5. Quantifying the benefits of avoiding ALAN wavelengths: we will quantify the ecological benefits of: i) removing blue light form LEDs blue using optical filters; ii) replacing white, with longer wavelength Amber LEDs. In addition we will quantify the responses of marine invertebrate larvae to different colours of light, so that the design of ecologically friendly LED lighting can be better informed.","4":"88695C90-DD84-43EE-B022-194EC777ED03"},{"1":"10023522","2":"CADDIE - Resilience","3":"Colorectal cancer (CRC) is a worldwide disease with 1.3m new cases and 0.7m related deaths reported each year and estimated global economic impact approaching &pound;100 Billion per year. In the UK, CRC is the second most common cause of cancer related deaths, 41k new incidents and over 16k related deaths are reported each year. 230k people are living in the UK with CRC.\\n\\nColonoscopy is an important tool for the identification and removal of pre-cancerous and cancerous polyps. However, even with direct inspection of the mucosa, it is highly operator dependent and studies show significant miss rate for polyps and cancers. Furthermore, when abnormal lesions are identified in vivo they are often poorly characterised/diagnosed despite the development of advanced imaging modalities.\\n\\nComputer Aided Detection and Diagnosis for Intelligent Endoscopy (CADDIE) will disrupt gastroenterology by using artificial intelligence to analyse colonoscopy video images in real-time. CADDIE will automatically detect and analyse cancerous and pre-cancerous polyps with the goal of better earlier detection and diagnosis of cancer leading to better patient outcomes.","4":"B2F6704E-5812-423D-BF5C-4E82726BB2F8"},{"1":"26673","2":"CADDIE - Computer Aided Detection and Diagnosis for Intelligent Endoscopy","3":"Colorectal cancer (CRC) is a worldwide disease with 1.3m new cases and 0.7m related deaths reported each year and estimated global economic impact approaching &pound;100 Billion per year. In the UK, CRC is the second most common cause of cancer related deaths, 41k new incidents and over 16k related deaths are reported each year. 230k people are living in the UK with CRC.\\n\\nColonoscopy is an important tool for the identification and removal of pre-cancerous and cancerous polyps. However, even with direct inspection of the mucosa, it is highly operator dependent and studies show significant miss rate for polyps and cancers. Furthermore, when abnormal lesions are identified in vivo they are often poorly characterised/diagnosed despite the development of advanced imaging modalities.\\n\\nComputer Aided Detection and Diagnosis for Intelligent Endoscopy (CADDIE) will disrupt gastroenterology by using artificial intelligence to analyse colonoscopy video images in real-time. CADDIE will automatically detect and analyse cancerous and pre-cancerous polyps with the goal of better earlier detection and diagnosis of cancer leading to better patient outcomes.","4":"F2A47DC1-10CB-4424-AC85-8681575E8F4E"},{"1":"2629629","2":"Coopetition: Cooperation in Competition and Its Application to Artificial Intelligence","3":"Consider a decentralised machine learning problem in which two rival online marketing companies aim to share their data in order to train a recommendation algorithm to maximise its accuracy level, as none of them has sufficiently large amount of data to do so. The more data they decide to share, the better improvement in the performance of the algorithm they can get. However, as they are also rivals, sharing too many data with each other will also induce a cost (e.g., losing sensitive data and market to the rival). On the other hand, without cooperation, they would not be able to achieve high recommendation efficiency, and thus, would not be able to increase their profit. As such, each company has to find the best trade-off between hiding data and training the machine learning algorithm together.\\nAnother example comes from the domain of smart grids. Consider a set of households who together would like to form a coalition and trade for good energy deals in the market. In order to maximise their own utility, as well as the social welfare, these households are encouraged to share as much information about their personal preferences with each other as possible. However, this raises a number of privacy issues. Given this, the main objective here is to identify to what extent the homeowners should lower their privacy demands in order to achieve good social welfare (i.e., good collective deals). \\nThe abovementioned examples belong to a new and emerging class of optimisation problems called Coopetition (for cooperation in competition), rooted in Game Theory and Bi-Level Optimisation. With the rapid improvements in Data Science, Decentralised Machine Learning (predicted to be the future of Machine Learning), and large-scale ubiquitous intelligent systems (e.g., Internet of Things, smart grids, and smart cities), Coopetition is becoming emergent. However, as it is a very new research field, it still lacks rigorous mathematical models that can capture the complex interconnection between cooperation and competition. As such, during my PhD studies, I would like to investigate the problem of Coopetition and its application to artificial intelligence in more detail. In particular, I aim to achieve the following goals:\\n1. Theoretical foundations: Investigate the mathematical foundations of Coopetition, and propose a thorough theoretical analysis of different models (year 1).\\n2. Algorithmic Contribution: Develop scalable and efficient algorithms to calculate different solution concepts of Coopetition (year 2).\\n3. Application: Apply my theoretical and algorithmic findings to concrete domains of Artificial Intelligence and Machine Learning (year 3).","4":"69BCC1F5-E2F3-4FFB-9E99-91396CCD0DFC"},{"1":"2115277","2":"Coopetition: Cooperation in Competition and Its Application to Artificial Intelligence","3":"Consider a decentralised machine learning problem in which two rival online marketing companies aim to share their data in order to train a recommendation algorithm to maximise its accuracy level, as none of them has sufficiently large amount of data to do so. The more data they decide to share, the better improvement in the performance of the algorithm they can get. However, as they are also rivals, sharing too many data with each other will also induce a cost (e.g., losing sensitive data and market to the rival). On the other hand, without cooperation, they would not be able to achieve high recommendation efficiency, and thus, would not be able to increase their profit. As such, each company has to find the best trade-off between hiding data and training the machine learning algorithm together.\\nAnother example comes from the domain of smart grids. Consider a set of households who together would like to form a coalition and trade for good energy deals in the market. In order to maximise their own utility, as well as the social welfare, these households are encouraged to share as much information about their personal preferences with each other as possible. However, this raises a number of privacy issues. Given this, the main objective here is to identify to what extent the homeowners should lower their privacy demands in order to achieve good social welfare (i.e., good collective deals). \\nThe abovementioned examples belong to a new and emerging class of optimisation problems called Coopetition (for cooperation in competition), rooted in Game Theory and Bi-Level Optimisation. With the rapid improvements in Data Science, Decentralised Machine Learning (predicted to be the future of Machine Learning), and large-scale ubiquitous intelligent systems (e.g., Internet of Things, smart grids, and smart cities), Coopetition is becoming emergent. However, as it is a very new research field, it still lacks rigorous mathematical models that can capture the complex interconnection between cooperation and competition. As such, during my PhD studies, I would like to investigate the problem of Coopetition and its application to artificial intelligence in more detail. In particular, I aim to achieve the following goals:\\n1. Theoretical foundations: Investigate the mathematical foundations of Coopetition, and propose a thorough theoretical analysis of different models (year 1).\\n2. Algorithmic Contribution: Develop scalable and efficient algorithms to calculate different solution concepts of Coopetition (year 2).\\n3. Application: Apply my theoretical and algorithmic findings to concrete domains of Artificial Intelligence and Machine Learning (year 3).","4":"FC1F9ECE-BCA2-43A4-A285-3D24501B0D70"},{"1":"EP/E064906/1","2":"The Complexity of Counting in Constraint Satisfaction Problems","3":"Constraint Satisfaction, which originated in Artificial Intelligence, provides a general framework for modelling decision problems, and has many practical applications. Decisions are modelled by variables, which are subject to constraints, modelling logical and resource restrictions. The paradigm is sufficiently broad that many interesting problems can be modelled, from satisfiability problems to scheduling problems and graph-theory problems. Understanding the complexity of constraint satisfaction problems has become a major and active area within computational complexity. The overall goal is to classify CSPs according to complexity, giving a characterisation for which CSPs are tractable. We will focus especially on characterizing the complexity of counting in Constraint Satisfaction problems. Specifically, we will study the complexity of exactly counting CSP solutions, approximately counting CSP solutions, and sampling CSP solutions from appropriately-defined probability distributions. These important questions are closely related, and are strongly connected to questions in statistical physics.","4":"444490A6-AA18-426B-A3B8-92A5DFBAB060"},{"1":"EP/E062482/1","2":"The Complexity of Counting in Constraint Satisfaction Problems","3":"Constraint Satisfaction, which originated in Artificial Intelligence, provides a general framework for modelling decision problems, and has many practical applications. Decisions are modelled by variables, which are subject to constraints, modelling logical and resource restrictions. The paradigm is sufficiently broad that many interesting problems can be modelled, from satisfiability problems to scheduling problems and graph-theory problems. Understanding the complexity of constraint satisfaction problems has become a major and active area within computational complexity. The overall goal is to classify CSPs according to complexity, giving a characterisation for which CSPs are tractable. We will focus especially on characterizing the complexity of counting in Constraint Satisfaction problems. Specifically, we will study the complexity of exactly counting CSP solutions, approximately counting CSP solutions, and sampling CSP solutions from appropriately-defined probability distributions. These important questions are closely related, and are strongly connected to questions in statistical physics.","4":"C18C7455-35A8-496F-A281-FE20C1C5816D"},{"1":"EP/E009492/1","2":"Optimum detectors for artificial object recognition","3":"Current digital cameras have a linear response and are only really suitable for simple picture imaging under controlled light conditions. In fact compared to human vision these cameras have two critical and debilitating limitations, a limited dynamic range and variability in object colour. The first of these limitations causes saturation when the dynamic range of a scene is larger than that of the camera. The second limitation is subtler and means that unlike human vision it is difficult to use colour information from linear cameras in artificial object recognition systems. Both these limitations can be overcome using pixels with a relatively narrow spectral response and an output that is proportional to the logarithm of the detected photocurrent. We intend to overcome these limitations by developing a new camera technology based upon organic photodiodes and a logarithmic response.","4":"084CB566-3B13-4359-9256-FFF57E347596"},{"1":"EP/E01075X/1","2":"Optimum detectors for artificial object recognition","3":"Current digital cameras have a linear response and are only really suitable for simple picture imaging under controlled light conditions. In fact compared to human vision these cameras have two critical and debilitating limitations, a limited dynamic range and variability in object colour. The first of these limitations causes saturation when the dynamic range of a scene is larger than that of the camera. The second limitation is subtler and means that unlike human vision it is difficult to use colour information from linear cameras in artificial object recognition systems. Both these limitations can be overcome using pixels with a relatively narrow spectral response and an output that is proportional to the logarithm of the detected photocurrent. We intend to overcome these limitations by developing a new camera technology based upon organic photodiodes and a logarithmic response.","4":"DD68BD3B-635E-48F6-888F-96D800FD6DF4"},{"1":"44929","2":"Real-time AI enabled rail track inspection and analysis [RAPPID]","3":"Current inspection of rail track defects utilises Network Rail's four Ultrasonic Testing Units (UTUs) that traverse the UK network, 64,000 miles of track, in 750 shifts per year. With a limitation of 30 miles per hour for rail track inspection, UTUs cannot meet the high demand and increased capacity of customers.\\n\\nEvery day, 4.8 million people travel by train in Britain. Around 200,000 tonnes of freight and goods are transported by rail in that same time frame, supporting businesses and consumers, productivity, and economic growth whilst taking thousands of lorries off the road, and helping in the reduction of greenhouse gasses.\\n\\nA risk-free network of rail tracks across the UK is pivotal to Network Rail's long-term planning process strategy and its vision for running a safe, reliable, efficient and growing railway, in Control Period 6 and beyond. Undiscovered rail track defects lead to asset failure, unscheduled maintenance, timetable delays, accidents, and fatalities. Train delays cost passengers 3.6 million hours in 2016, whilst over &pound;72M was claimed by passengers from operators for service disruptions in 2016/17\\\\. With the growing demand on rail transport by passengers, there is need for commercial solutions that offers high-speed (i.e. above 60 miles per hour) high resolution, rail track inspection, and data analysis in real-time. A commercial solution with the capacity to enable UK network-wide coverage.\\n\\nThis RAPPID project seeks to address the challenges that the UK rail network faces regarding rapid high-speed high-resolution identification of rail track defects, data collation and analysis, enabling real-time predictive analysis, and predictive maintenance of rail tracks across the UK network and globally.\\n\\nThe RAPPID project is based on the novel use of Virtual Source Aperture non-destruction testing techniques in combination with artificial intelligence and deep-learning methodologies that enable real time data processing and analysis of rail track data derived via use of next generation phased-array ultrasonic testing hardware.","4":"DA541E4B-0EC2-4737-A7C8-AA739E829917"},{"1":"10024802","2":"Real-time AI enabled rail track inspection and analysis [RAPPID] - Resilience Fund Application","3":"Current inspection of rail track defects utilises Network Rail's four Ultrasonic Testing Units (UTUs) that traverse the UK network, 64,000 miles of track, in 750 shifts per year. With a limitation of 30 miles per hour for rail track inspection, UTUs cannot meet the high demand and increased capacity of customers.\\n\\nEvery day, 4.8 million people travel by train in Britain. Around 200,000 tonnes of freight and goods are transported by rail in that same time frame, supporting businesses and consumers, productivity, and economic growth whilst taking thousands of lorries off the road, and helping in the reduction of greenhouse gasses.\\n\\nA risk-free network of rail tracks across the UK is pivotal to Network Rail's long-term planning process strategy and its vision for running a safe, reliable, efficient and growing railway, in Control Period 6 and beyond. Undiscovered rail track defects lead to asset failure, unscheduled maintenance, timetable delays, accidents, and fatalities. Train delays cost passengers 3.6 million hours in 2016, whilst over &pound;72M was claimed by passengers from operators for service disruptions in 2016/17\\\\. With the growing demand on rail transport by passengers, there is need for commercial solutions that offers high-speed (i.e. above 60 miles per hour) high resolution, rail track inspection, and data analysis in real-time. A commercial solution with the capacity to enable UK network-wide coverage.\\n\\nThis RAPPID project seeks to address the challenges that the UK rail network faces regarding rapid high-speed high-resolution identification of rail track defects, data collation and analysis, enabling real-time predictive analysis, and predictive maintenance of rail tracks across the UK network and globally.\\n\\nThe RAPPID project is based on the novel use of Virtual Source Aperture non-destruction testing techniques in combination with artificial intelligence and deep-learning methodologies that enable real time data processing and analysis of rail track data derived via use of next generation phased-array ultrasonic testing hardware.","4":"1733A4E8-A8E7-4DC5-B590-0F5652488408"},{"1":"10023783","2":"London Medical Imaging &amp; Artificial Intelligence Centre for Value-Based Healthcare - Resilience Funding Application","3":"Despite rapidly increasing numbers of people living with dementia, there are substantial difficulties with diagnosis. Around a third of people living with dementia have not received a diagnosis. Even in people who are diagnosed, there is typically a delay of years between developing symptoms and receiving a diagnosis. These problems mean that people are not empowered by receiving information about what is causing their symptoms, and are unable to access treatments. The first treatment that could slow the progression of dementia has just been approved for use in the USA, making early and accurate diagnosis of dementia an urgent priority.\\n\\nOne important reason for delayed and inaccurate dementia diagnosis is that the human eye is not reliably able to tell from brain scans who has early dementia, and who is likely to have a deterioration in their memory in the future. We propose to use artificial intelligence to derive this information from routine brain scans.\\n\\nAINOSTICS' technology represents a breakthrough that would provide an automated, and personalised healthcare platform for assisting the clinical diagnosis of dementia using multi-modal imaging and non-imaging data that are already routinely acquired in healthcare and research settings; useful for both the treatment of patients, and importantly, in the development of novel therapeutics.\\n\\nAINOSTICS' technology can automatically and intelligently analyse scans to provide sensitive and accurate micro-structural information about key tissue and organ structures then compare this with information from healthy populations to detect the signatures of disease. We intend for AINOSTICS' software to become a routine part of clinical practice and drug development as the results of our intelligent analysis will provide clinicians, researchers, and imaging centres a convenient and cost-effective means to get reliable, quantitative and objective diagnostic and prognostic data.\\n\\nFor serious global diseases, AINOSTICS' technology has the potential to save time during patient assessments, accelerate clinical pathways, standardise the quality of care and improve patient outcomes in addition to making important contributions to the development of disease modifying therapeutics.","4":"ADEC50B3-E8A6-43AF-9A36-24B214AC4A9D"},{"1":"10015197","2":"QUANTIMA: Non-invasive imaging driven patient identification, stratification and outcome prognosis in dementia","3":"Despite rapidly increasing numbers of people living with dementia, there are substantial difficulties with diagnosis. Around a third of people living with dementia have not received a diagnosis. Even in people who are diagnosed, there is typically a delay of years between developing symptoms and receiving a diagnosis. These problems mean that people are not empowered by receiving information about what is causing their symptoms, and are unable to access treatments. The first treatment that could slow the progression of dementia has just been approved for use in the USA, making early and accurate diagnosis of dementia an urgent priority.\\n\\nOne important reason for delayed and inaccurate dementia diagnosis is that the human eye is not reliably able to tell from brain scans who has early dementia, and who is likely to have a deterioration in their memory in the future. We propose to use artificial intelligence to derive this information from routine brain scans.\\n\\nAINOSTICS' technology represents a breakthrough that would provide an automated, and personalised healthcare platform for assisting the clinical diagnosis of dementia using multi-modal imaging and non-imaging data that are already routinely acquired in healthcare and research settings; useful for both the treatment of patients, and importantly, in the development of novel therapeutics.\\n\\nAINOSTICS' technology can automatically and intelligently analyse scans to provide sensitive and accurate micro-structural information about key tissue and organ structures then compare this with information from healthy populations to detect the signatures of disease. We intend for AINOSTICS' software to become a routine part of clinical practice and drug development as the results of our intelligent analysis will provide clinicians, researchers, and imaging centres a convenient and cost-effective means to get reliable, quantitative and objective diagnostic and prognostic data.\\n\\nFor serious global diseases, AINOSTICS' technology has the potential to save time during patient assessments, accelerate clinical pathways, standardise the quality of care and improve patient outcomes in addition to making important contributions to the development of disease modifying therapeutics.","4":"D1F42962-37C3-49AD-9CDC-C76CA09D9F83"},{"1":"10020217","2":"Commercialisation of an imaging based dementia diagnostic test in the Chinese market - feasibility study","3":"Despite rapidly increasing numbers of people living with dementia, there are substantial difficulties with diagnosis. Around a third of people living with dementia have not received a diagnosis. Even in people who are diagnosed, there is typically a delay of years between developing symptoms and receiving a diagnosis. These problems mean that people are not empowered by receiving information about what is causing their symptoms, and are unable to access treatments. The first treatment that could slow the progression of dementia has just been approved for use in the USA, making early and accurate diagnosis of dementia an urgent priority.\\n\\nOne important reason for delayed and inaccurate dementia diagnosis is that the human eye is not reliably able to tell from brain scans who has early dementia, and who is likely to have a deterioration in their memory in the future. We propose to use artificial intelligence to derive this information from routine brain scans.\\n\\nAINOSTICS' technology represents a breakthrough that would provide an automated, and personalised healthcare platform for assisting the clinical diagnosis of dementia using multi-modal imaging and non-imaging data that are already routinely acquired in healthcare and research settings; useful for both the treatment of patients, and importantly, in the development of novel therapeutics.\\n\\nAINOSTICS' technology can automatically and intelligently analyse scans to provide sensitive and accurate micro-structural information about key tissue and organ structures then compare this with information from healthy populations to detect the signatures of disease. We intend for AINOSTICS' software to become a routine part of clinical practice and drug development as the results of our intelligent analysis will provide clinicians, researchers, and imaging centres a convenient and cost-effective means to get reliable, quantitative and objective diagnostic and prognostic data.\\n\\nFor serious global diseases, AINOSTICS' technology has the potential to save time during patient assessments, accelerate clinical pathways, standardise the quality of care and improve patient outcomes in addition to making important contributions to the development of disease modifying therapeutics.","4":"ACE5C4B2-5F74-43D0-989F-EF7CC8CA03DB"},{"1":"10028149","2":"First in class non-invasive companion diagnostic test for dementia","3":"Despite rapidly increasing numbers of people living with dementia, there are substantial difficulties with diagnosis. Around a third of people living with dementia have not received a diagnosis. Even in people who are diagnosed, there is typically a delay of years between developing symptoms and receiving a diagnosis. These problems mean that people are not empowered by receiving information about what is causing their symptoms, and are unable to access treatments. The first treatment that could slow the progression of dementia has just been approved for use in the USA, making early and accurate diagnosis of dementia an urgent priority.\\n\\nOne important reason for delayed and inaccurate dementia diagnosis is that the human eye is not reliably able to tell from brain scans who has early dementia, and who is likely to have a deterioration in their memory in the future. We propose to use artificial intelligence to derive this information from routine brain scans.\\n\\nAINOSTICS' technology represents a breakthrough that would provide an automated, and personalised healthcare platform for assisting the clinical diagnosis of dementia using multi-modal imaging and non-imaging data that are already routinely acquired in healthcare and research settings; useful for both the treatment of patients, and importantly, in the development of novel therapeutics.\\n\\nAINOSTICS' technology can automatically and intelligently analyse scans to provide sensitive and accurate micro-structural information about key tissue and organ structures then compare this with information from healthy populations to detect the signatures of disease. We intend for AINOSTICS' software to become a routine part of clinical practice and drug development as the results of our intelligent analysis will provide clinicians, researchers, and imaging centres a convenient and cost-effective means to get reliable, quantitative and objective diagnostic and prognostic data.\\n\\nFor serious global diseases, AINOSTICS' technology has the potential to save time during patient assessments, accelerate clinical pathways, standardise the quality of care and improve patient outcomes in addition to making important contributions to the development of disease modifying therapeutics.","4":"1E11BDD8-21D8-4888-8999-23735D17A9F4"},{"1":"MR/S502480/1","2":"Industrial Strategy PhD Studentships in Artificial Intelligence and Data Science","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"F11BAD22-DFA2-4D64-A0BC-C50BD42606C1"},{"1":"BB/S507581/1","2":"Artificial intelligence applications to precision mental health - artificial networks for predicting lifelong brain health","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"FDD2BAED-BE99-432C-96FD-9B956B22F2DC"},{"1":"BB/S507490/1","2":"Identifying microbes by combining spectral fingerprints with artificial intelligence and Bayesian machine learning","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"47D1387B-F744-4490-958A-DE3D3EA5AEF6"},{"1":"BB/S507441/1","2":"Artificial intelligence and deep learning in image based crop phenomics for predicting seed quality","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"E3130C6E-F563-43EF-9CE8-5BCB9B8F9E24"},{"1":"BB/S507350/1","2":"Application of artificial intelligence-driven design of function-directed ligands for selective retinoic acid receptor binding","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"5A468035-AE4D-4F02-B163-8F7054C1B4A8"},{"1":"BB/S507349/1","2":"Improving forecasting and management of fish stocks and forest pests using artificial intelligence and machine learning","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"D82C16F1-3A9A-40A6-91EF-F29EB91E9F14"},{"1":"BB/M016412/1","2":"Tissue Engineering an artificial Thymus","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"0EA3410A-0E99-4BC0-A7B6-000195829667"},{"1":"BB/R505985/1","2":"3D Bioprinting Engineering Artificial Respiratory Tract Tissue","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"72BBBB61-BFD9-43B6-A989-F47F2A8AEFA7"},{"1":"NC/W002043/1","2":"Efficacy of artificial imprinted antibodies in driving unwarranted immune responses","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"B3847877-4D95-496D-85B7-88C328AEC517"},{"1":"BB/K501177/1","2":"Predictive Ecotoxicology: use of an Artificial Neural Net to Predict Bioaccumulation for Invertebrates","3":"Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at https://www.ukri.org/apply-for-funding/how-we-fund-studentships/. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.","4":"CC17A3A0-3224-4021-9361-888CE632CD99"},{"1":"EP/E060722/1","2":"Evolutionary Algorithms for Dynamic Optimisation Problems: Design, Analysis and Applications","3":"Evolutionary algorithms (EAs) have been applied to solve many stationary problems. However, real-world problems are usually more complex and dynamic, where the objective function, decision variables, and environmental parameters may change over time. In this project, we will investigate novel EA approaches to address dynamic optimisation problems (DOPs), a challenging but very important research area. The proposed research has three main aspects: (1) designing and evaluating new EAs for DOPs in collaboration with researchers from Honda Research Institute Europe, (2) theoretically analysing EAs for DOPs, and (3) adapting developed EA approaches to solve dynamic telecommunication optimisation problems. In this project, we will first construct standardised, both discrete and continuous, dynamic test environments based on the concept of problem difficulty, scalability, cyclicity and noise of environments, and standardised performance measures for evaluating EAs for DOPs. Based on the standardised dynamic test and evaluation environment, we will then design and evaluate novel EAs and their hybridisation, e.g., Estimation of Distribution Algorithms (EDAs), Genetic Algorithms, Swarm Intelligence and Adaptive Evolutionary Algorithms, for DOPs based on our previous research. A guiding idea here is to improve EA's adaptability to different degrees of environmental change in the genotypic space, be it binary or not. Systematically and adaptively combining dualism-like schemes for significant changes, random immigration-like schemes for medium changes, and general mutation or variation schemes for small changes, is expected to greatly improve EA's performance in different dynamic environments. And memory schemes can be used when the environment involves cyclic changes. In order to better understand the fundamental issues, theoretical analysis of EAs for DOPs will be pursued in this project. We will apply drift analysis and martingale theory as the starting point to analyse the computational time complexity of EAs for DOPs and the dynamic behaviour of EAs for DOPs regarding such properties as tracking error, tracking velocity, and reliability of arriving at optima. Based on the above EA design, experimental evaluation, and formal analysis, we will then develop a generic framework of EAs for DOPs by extracting key techniques/properties of efficient EAs for DOPs and studying the relationship between them and the characteristics of DOPs being solved with respect to the environmental dynamics in the genotypic space. Another key aspect of this project is to apply and adapt developed EAs for general DOPs to solve core dynamic telecommunications problems, e.g., dynamic frequency assignment problems and dynamic call routing problems, in the real world. We will closely collaborate with researchers from British Telecommunications (BT) to extract domain-specific knowledge and model dynamic telecommunication problems using proper mathematical and graph representations. The obtained domain knowledge will be integrated into our EAs for increased efficiency and effectiveness. All algorithms and software developed in this project will be made available publicly to benefit as many users as possible, whether they are from academe or industry.","4":"E6585832-5044-4E62-936F-CB04F6393773"},{"1":"EP/E058884/1","2":"Evolutionary Algorithms for Dynamic Optimisation Problems: Design, Analysis and Applications","3":"Evolutionary algorithms (EAs) have been applied to solve many stationary problems. However, real-world problems are usually more complex and dynamic, where the objective function, decision variables, and environmental parameters may change over time. In this project, we will investigate novel EA approaches to address dynamic optimisation problems (DOPs), a challenging but very important research area. The proposed research has three main aspects: (1) designing and evaluating new EAs for DOPs in collaboration with researchers from Honda Research Institute Europe, (2) theoretically analysing EAs for DOPs, and (3) adapting developed EA approaches to solve dynamic telecommunication optimisation problems. In this project, we will first construct standardised, both discrete and continuous, dynamic test environments based on the concept of problem difficulty, scalability, cyclicity and noise of environments, and standardised performance measures for evaluating EAs for DOPs. Based on the standardised dynamic test and evaluation environment, we will then design and evaluate novel EAs and their hybridisation, e.g., Estimation of Distribution Algorithms (EDAs), Genetic Algorithms, Swarm Intelligence and Adaptive Evolutionary Algorithms, for DOPs based on our previous research. A guiding idea here is to improve EA's adaptability to different degrees of environmental change in the genotypic space, be it binary or not. Systematically and adaptively combining dualism-like schemes for significant changes, random immigration-like schemes for medium changes, and general mutation or variation schemes for small changes, is expected to greatly improve EA's performance in different dynamic environments. And memory schemes can be used when the environment involves cyclic changes. In order to better understand the fundamental issues, theoretical analysis of EAs for DOPs will be pursued in this project. We will apply drift analysis and martingale theory as the starting point to analyse the computational time complexity of EAs for DOPs and the dynamic behaviour of EAs for DOPs regarding such properties as tracking error, tracking velocity, and reliability of arriving at optima. Based on the above EA design, experimental evaluation, and formal analysis, we will then develop a generic framework of EAs for DOPs by extracting key techniques/properties of efficient EAs for DOPs and studying the relationship between them and the characteristics of DOPs being solved with respect to the environmental dynamics in the genotypic space. Another key aspect of this project is to apply and adapt developed EAs for general DOPs to solve core dynamic telecommunications problems, e.g., dynamic frequency assignment problems and dynamic call routing problems, in the real world. We will closely collaborate with researchers from British Telecommunications (BT) to extract domain-specific knowledge and model dynamic telecommunication problems using proper mathematical and graph representations. The obtained domain knowledge will be integrated into our EAs for increased efficiency and effectiveness. All algorithms and software developed in this project will be made available publicly to benefit as many users as possible, whether they are from academe or industry.","4":"DE7F5451-E58A-4FEA-B7C7-C4598D429F04"},{"1":"NE/V00378X/1","2":"PYRAMID: Platform for dYnamic, hyper-resolution, near-real time flood Risk AssessMent Integrating repurposed and novel Data sources","3":"Flooding has been identified by the government as the number one priority and risk to the UK. Flooding already causes millions of pounds worth of damage to people's homes, infrastructure and the economy every year, and is projected to become even more severe under climate change. Being able to plan for, respond to and manage flooding effectively is therefore essential.\\n\\nWe are lucky to have a tradition of flood management in the UK led by the Environment Agency. Operational flood models use meteorological data combined with elevation data to show us where flooding will occur. These models produce flood risk maps for planning and forecasting purposes and have helped us design flood defences for many areas.\\n\\nHowever, flooding is not only dependent on the topography of an area. There are many other factors at play that evolve over time: culverts can get blocked, flood gates are left open and flood walls can fall into disrepair. This can dramatically alter the extent and depth of a flood. Not only that, but our exposure to flood risk changes too. Far less disruption occurs from a flood overnight than during rush hour traffic. A prime example of this is the flooding of Boscastle in 2004. During the event, 116 cars parked in a carpark were washed downstream, blocking a bridge, causing water to back up and flood unexpected areas. If the rain had fallen in the evening, the cars would not have been in the carpark and the impact of the flood would have been smaller. Could we have predicted this? Can we reduce the impact of flooding for similar future events? We think that with the right data and tools, we can.\\n\\nWe will build a tool that will change how we respond to flood risks as they evolve. The tool will allow flood risk managers to deploy just-in-time maintenance and alleviation measures, such as clearing critical blocked culverts or setting up mobile flood defences. To achieve this, the tool will incorporate brand new types of data and cutting edge flood models into an easy-to-use online platform that allows users to visualise evolving flood risks. The platform (called PYRAMID) will be developed in conjunction with the Environment Agency, local authorities and community groups to ensure that it delivers relevant information for critical decision-making in near-real time. The platform will have toolkits to make it easy for communities to incorporate their data, providing essential local information.\\n\\nThe new data driving this modelling will be key. The data that we need are available but sit fragmented across a range of organisations in difficult-to-use formats. We will use artificial intelligence to extract this useful information from hidden datasets, such as old reports, flood asset registers and various types of satellite imagery. In addition, we want to incorporate brand new information from novel sensors that are being deployed as part of Newcastle University's Urban Observatory. These sensors monitor things like soil moisture and rainfall at very high resolutions, as well as other factors like traffic and congestion. We can also monitor the condition of specific factors affecting flood risk, such as whether particular culverts are blocked or whether certain flood walls are in poor condition. These factors can be monitored by looking at a combination of satellite remote sensing and sensors deployed on lorries and other vehicles. We will also harness data collected communities and citizens.\\n\\nAll of this information will be put into our flood models. We have a hyper-resolution hydrodynamic flood model that can accurately simulate the movement of debris in flood flows at a centimetre scale. This model will work in conjunction with a broader catchment model, which will provide information on the hydrological conditions in the wider area. The platform will be trialled in Newcastle to take advantage of existing government investments in the Urban Observatory and a legacy of flood research conducted here.","4":"6B52A2A1-68EA-4A56-8AA3-B5BDE2F84CB3"},{"1":"NE/V003321/1","2":"PYRAMID: Platform for dYnamic, hyper-resolution, near-real time flood Risk AssessMent Integrating repurposed and novel Data sources","3":"Flooding has been identified by the government as the number one priority and risk to the UK. Flooding already causes millions of pounds worth of damage to people's homes, infrastructure and the economy every year, and is projected to become even more severe under climate change. Being able to plan for, respond to and manage flooding effectively is therefore essential.\\n\\nWe are lucky to have a tradition of flood management in the UK led by the Environment Agency. Operational flood models use meteorological data combined with elevation data to show us where flooding will occur. These models produce flood risk maps for planning and forecasting purposes and have helped us design flood defences for many areas.\\n\\nHowever, flooding is not only dependent on the topography of an area. There are many other factors at play that evolve over time: culverts can get blocked, flood gates are left open and flood walls can fall into disrepair. This can dramatically alter the extent and depth of a flood. Not only that, but our exposure to flood risk changes too. Far less disruption occurs from a flood overnight than during rush hour traffic. A prime example of this is the flooding of Boscastle in 2004. During the event, 116 cars parked in a carpark were washed downstream, blocking a bridge, causing water to back up and flood unexpected areas. If the rain had fallen in the evening, the cars would not have been in the carpark and the impact of the flood would have been smaller. Could we have predicted this? Can we reduce the impact of flooding for similar future events? We think that with the right data and tools, we can.\\n\\nWe will build a tool that will change how we respond to flood risks as they evolve. The tool will allow flood risk managers to deploy just-in-time maintenance and alleviation measures, such as clearing critical blocked culverts or setting up mobile flood defences. To achieve this, the tool will incorporate brand new types of data and cutting edge flood models into an easy-to-use online platform that allows users to visualise evolving flood risks. The platform (called PYRAMID) will be developed in conjunction with the Environment Agency, local authorities and community groups to ensure that it delivers relevant information for critical decision-making in near-real time. The platform will have toolkits to make it easy for communities to incorporate their data, providing essential local information.\\n\\nThe new data driving this modelling will be key. The data that we need are available but sit fragmented across a range of organisations in difficult-to-use formats. We will use artificial intelligence to extract this useful information from hidden datasets, such as old reports, flood asset registers and various types of satellite imagery. In addition, we want to incorporate brand new information from novel sensors that are being deployed as part of Newcastle University's Urban Observatory. These sensors monitor things like soil moisture and rainfall at very high resolutions, as well as other factors like traffic and congestion. We can also monitor the condition of specific factors affecting flood risk, such as whether particular culverts are blocked or whether certain flood walls are in poor condition. These factors can be monitored by looking at a combination of satellite remote sensing and sensors deployed on lorries and other vehicles. We will also harness data collected communities and citizens.\\n\\nAll of this information will be put into our flood models. We have a hyper-resolution hydrodynamic flood model that can accurately simulate the movement of debris in flood flows at a centimetre scale. This model will work in conjunction with a broader catchment model, which will provide information on the hydrological conditions in the wider area. The platform will be trialled in Newcastle to take advantage of existing government investments in the Urban Observatory and a legacy of flood research conducted here.","4":"C0A3C97B-472A-4FDA-92FE-E09FC9A7C1E2"},{"1":"10038028","2":"Sustainable manufacture systems towards novel bio-based materials (GREEN-LOOP)","3":"GREEN-LOOP addresses novel bio-based materials solutions, tackling the problem from a circular-business thinking perspective, which will enable overcoming the barrier for new manufacture tools, energy efficiency improvements and sustainable value chains. The main goal of GREEN-LOOP is to design and optimise 3 innovative bio-based materials and components for industrial sectors: construction, packaging, food, beverage, appliances and tooling. 1) multifunctional panels rubber panels with fire resistance and vibrational applications, 2) bioplastic bottle closures for oil and fruit juice, 3) wood composites bearings for plastic injection machines. The value chain of each product is optimised from raw material source to End Of Life of products, ensuring the circular economy. All manufacturing lines are retrofitted to adapt to the new enhanced bio compoundings using Artificial Intelligence. Ultrasound improvement is included in the lignin production and rubber manufacture, and Microwaves improve preheating of bioplastic on injection moulding and curing of wood composites. The whole process is monitored and presented in a virtual platform that includes KPI evaluation, business optimization in real-time, training (webinars), and social engagement.no public description","4":"266801E3-2E1C-40C5-A959-D0E89C376FA4"},{"1":"10038017","2":"GREEN-LOOP","3":"GREEN-LOOP addresses novel bio-based materials solutions, tackling the problem from a circular-business thinking perspective, which will enable overcoming the barrier for new manufacture tools, energy efficiency improvements and sustainable value chains. The main goal of GREEN-LOOP is to design and optimise 3 innovative bio-based materials and components for industrial sectors: construction, packaging, food, beverage, appliances and tooling. 1) multifunctional panels rubber panels with fire resistance and vibrational applications, 2) bioplastic bottle closures for oil and fruit juice, 3) wood composites bearings for plastic injection machines. The value chain of each product is optimised from raw material source to End Of Life of products, ensuring the circular economy. All manufacturing lines are retrofitted to adapt to the new enhanced bio compoundings using Artificial Intelligence. Ultrasound improvement is included in the lignin production and rubber manufacture, and Microwaves improve preheating of bioplastic on injection moulding and curing of wood composites. The whole process is monitored and presented in a virtual platform that includes KPI evaluation, business optimization in real-time, training (webinars), and social engagement.no public description","4":"053D83E3-4293-442D-85C2-3AB2A5392B28"},{"1":"NE/R012806/1","2":"Changing Arctic Carbon cycle in the cOastal Ocean Near-shore (CACOON)","3":"Global climate change has led to substantial increases in air temperatures across the Earth, particularly in Arctic regions. This has led to changes in patterns of rainfall and snow cover, as well as the structure and stability of terrestrial systems. Unlike the tropics - where the majority of land-based carbon is usually stored in trees on land, the Arctic plays host to vast quantities of carbon locked up underground in frozen soils and ice, known as permafrost. This permafrost has been locked up for tens of thousands of years, and still often contains the remains of woolly mammoth, and exotic viruses [1]. \\n \\n The Arctic Ocean (AO) receives huge quantities of material from the Arctic mainland, much being delivered by giant Arctic rivers that drain vast swathes of the Eurasian and American Arctic. These rivers are now delivering greater quantities of water from land to the ocean, fuelled by climate-driven increases in rainfall and permafrost thaw. This will cause a shift in the amount, age and type of materials being delivered from land to the ocean. So, should why is this important?\\n \\n The AO plays a crucial role in the storage and cycling of carbon, through the uptake of CO2 by marine plants, and the subsequent export of a fraction of this to the deep ocean - locking away carbon from the atmosphere. The ocean also plays host to bacteria (and other processes), which can release carbon from the ocean to the atmosphere. The balance of these processes is critical in determining how much carbon the AO will store, or release, in the future. Currently, we think the AO is a small overall 'store' of CO2 over the year, but this could change in the future, with hazardous consequences for global temperatures. \\n \\n We will examine these processes, focusing upon coastal regions where freshwaters meet the ocean. Studies to date, have focused upon rivers only, or the ocean itself, but few have investigated where they mix. We propose to carry out three different strands of research that will fill these gaps in our knowledge. We will study the East Siberian Shelf Sea (ESAS) region, and two very large Arctic river systems (the Kolyma and Lena Rivers) that drain into the AO over this shelf. We'll focus on this remote Russian Arctic area as it is currently experiencing extremely rapid climate warming, riverine runoff rates are increasing fast here, and despite the shelf covering a very large area little is known about how this region will change. \\n\\n Firstly, we'll conduct field campaigns collecting waters across the two study sites, sampling waters, soils and sediments during winter, summer and spring. This will involve sampling by boat in summer, and by skidoo - with drilling over ice during the Siberian winter. Secondly, we'll bring samples back from the field to conduct detailed experiments to determine how key environmental processes, such as sunlight and bacteria, use and alter terrestrial materials as they move from the rivers into the AO. This includes shining artificial sunlight at waters to see how materials change, or allowing microbes to 'feed' on what's in the water to see what they use and how quickly. Lastly, we'll combine our findings to develop modelling tools allowing us to model, or 'simulate', how fluxes of water, and materials travel from land-to-ocean over the ESAS. This model will contain separate compartments, representing different fractions of the materials sourced from land, for example different nutrients or carbon types. Also, it will simulate the major (small to microscopic) biological groups within the ecosystem, for example bacteria, and different phytoplankton groups. This will allow us to examine how the AO, and its biological processes will respond to future changes in freshwater supply and increased permafrost, and ultimately identify how these processes may alter the role of the AO in global climate.\\n\\n[1] http://www.bbc.com/earth/story/20170504-there-are-diseases-hidden-in-ice-and-they-are-waking-up","4":"5EB79356-0DAA-4C24-9399-65D365F433B6"},{"1":"NE/R012814/1","2":"Changing Arctic Carbon cycle in the cOastal Ocean Near-shore (CACOON)","3":"Global climate change has led to substantial increases in air temperatures across the Earth, particularly in Arctic regions. This has led to changes in patterns of rainfall and snow cover, as well as the structure and stability of terrestrial systems. Unlike the tropics - where the majority of land-based carbon is usually stored in trees on land, the Arctic plays host to vast quantities of carbon locked up underground in frozen soils and ice, known as permafrost. This permafrost has been locked up for tens of thousands of years, and still often contains the remains of woolly mammoth, and exotic viruses [1]. \\n \\n The Arctic Ocean (AO) receives huge quantities of material from the Arctic mainland, much being delivered by giant Arctic rivers that drain vast swathes of the Eurasian and American Arctic. These rivers are now delivering greater quantities of water from land to the ocean, fuelled by climate-driven increases in rainfall and permafrost thaw. This will cause a shift in the amount, age and type of materials being delivered from land to the ocean. So, should why is this important?\\n \\n The AO plays a crucial role in the storage and cycling of carbon, through the uptake of CO2 by marine plants, and the subsequent export of a fraction of this to the deep ocean - locking away carbon from the atmosphere. The ocean also plays host to bacteria (and other processes), which can release carbon from the ocean to the atmosphere. The balance of these processes is critical in determining how much carbon the AO will store, or release, in the future. Currently, we think the AO is a small overall 'store' of CO2 over the year, but this could change in the future, with hazardous consequences for global temperatures. \\n \\n We will examine these processes, focusing upon coastal regions where freshwaters meet the ocean. Studies to date, have focused upon rivers only, or the ocean itself, but few have investigated where they mix. We propose to carry out three different strands of research that will fill these gaps in our knowledge. We will study the East Siberian Shelf Sea (ESAS) region, and two very large Arctic river systems (the Kolyma and Lena Rivers) that drain into the AO over this shelf. We'll focus on this remote Russian Arctic area as it is currently experiencing extremely rapid climate warming, riverine runoff rates are increasing fast here, and despite the shelf covering a very large area little is known about how this region will change. \\n\\n Firstly, we'll conduct field campaigns collecting waters across the two study sites, sampling waters, soils and sediments during winter, summer and spring. This will involve sampling by boat in summer, and by skidoo - with drilling over ice during the Siberian winter. Secondly, we'll bring samples back from the field to conduct detailed experiments to determine how key environmental processes, such as sunlight and bacteria, use and alter terrestrial materials as they move from the rivers into the AO. This includes shining artificial sunlight at waters to see how materials change, or allowing microbes to 'feed' on what's in the water to see what they use and how quickly. Lastly, we'll combine our findings to develop modelling tools allowing us to model, or 'simulate', how fluxes of water, and materials travel from land-to-ocean over the ESAS. This model will contain separate compartments, representing different fractions of the materials sourced from land, for example different nutrients or carbon types. Also, it will simulate the major (small to microscopic) biological groups within the ecosystem, for example bacteria, and different phytoplankton groups. This will allow us to examine how the AO, and its biological processes will respond to future changes in freshwater supply and increased permafrost, and ultimately identify how these processes may alter the role of the AO in global climate.\\n\\n[1] http://www.bbc.com/earth/story/20170504-there-are-diseases-hidden-in-ice-and-they-are-waking-up","4":"7509D3B0-8058-4ACA-9F0D-7F35E567B8F5"},{"1":"2277801","2":"One Health Aquaculture: Healthy Food, Healthy Animals, Healthy Ecosystems","3":"Globally, aquaculture produces more than 70 million tonnes of fish per annum, with an estimated value of $160billion and has increased 5-fold over the last 30 years and is predicted to be 40% of global fisheries (by tonnage) by 2025 (http://www.fao.org/3/a-i5555e.pdf). While, the human health benefits of a diet rich in oily fish and seafood have been well publicised, a recent Seafood Intelligence transparency report (http://www.seafoodintell.com/wp-content/uploads/2014/09/Top-16-Organisations-Transparency-Synoptic-Table-Oct-13-2016.jpg) highlights a range of environmental, social, and societal areas in which knowledge is lacking in terms of impacts. These areas concern energy and GHG emissions, biodiversity and ecosystem function waste, and water consumption. \\n\\nThe UK is the second largest aquaculture producer in the EU, on account of its large Atlantic salmon industry, and is also a major exporter of salmon to the rest of the EU, and in the UK there is also growing consumer concern over animal welfare, pest &amp; disease management. \\n\\nThis PhD will focus on quantifying the impact of Scottish Aquaculture production systems in terms of the environmental, ecological and animal health impacts of farming practices. Specifically, it will:\\n1. Review and synthesise knowledge about the above impacts and how they are affected by farming practice for salmonids and shellfish\\n2. Conduct case study assessments on Scottish producers\\n3. Co-design and develop, with and for Scottish farmers and supply chain actors, a decision support system to assess impacts\\n4. Identify best practices with respect to environmental and animal health and welfare.\\n\\n\\nThe UK-based aquaculture supply chain needs to be able to understand, report, and improve on the sustainability of aquaculture farming and related upstream processes in relation to the above issues. This will enable the identification of currently unsustainable practices at &quot;farm&quot; level and will also allow the transparent reporting to consumers and to provide policy with evidence on good and bad practices to inform policy and regulation in the UK and overseas. Robust understanding of the environmental impacts will mean that social licence for aquaculture will be easier to maintain in potentially threatened areas such as Scottish sea lochs.","4":"F924CBD8-9A0B-4478-BD9F-FE62C598D24C"},{"1":"2279198","2":"One Health Aquaculture: Healthy Food, Healthy Animals, Healthy Ecosystems","3":"Globally, aquaculture produces more than 70 million tonnes of fish per annum, with an estimated value of $160billion and has increased 5-fold over the last 30 years and is predicted to be 40% of global fisheries (by tonnage) by 2025 (http://www.fao.org/3/a-i5555e.pdf). While, the human health benefits of a diet rich in oily fish and seafood have been well publicised, a recent Seafood Intelligence transparency report (http://www.seafoodintell.com/wp-content/uploads/2014/09/Top-16-Organisations-Transparency-Synoptic-Table-Oct-13-2016.jpg) highlights a range of environmental, social, and societal areas in which knowledge is lacking in terms of impacts. These areas concern energy and GHG emissions, biodiversity and ecosystem function waste, and water consumption. \\n\\nThe UK is the second largest aquaculture producer in the EU, on account of its large Atlantic salmon industry, and is also a major exporter of salmon to the rest of the EU, and in the UK there is also growing consumer concern over animal welfare, pest &amp; disease management. \\n\\nThis PhD will focus on quantifying the impact of Scottish Aquaculture production systems in terms of the environmental, ecological and animal health impacts of farming practices. Specifically, it will:\\n1. Review and synthesise knowledge about the above impacts and how they are affected by farming practice for salmonids and shellfish\\n2. Conduct case study assessments on Scottish producers\\n3. Co-design and develop, with and for Scottish farmers and supply chain actors, a decision support system to assess impacts\\n4. Identify best practices with respect to environmental and animal health and welfare.\\n\\n\\nThe UK-based aquaculture supply chain needs to be able to understand, report, and improve on the sustainability of aquaculture farming and related upstream processes in relation to the above issues. This will enable the identification of currently unsustainable practices at &quot;farm&quot; level and will also allow the transparent reporting to consumers and to provide policy with evidence on good and bad practices to inform policy and regulation in the UK and overseas. Robust understanding of the environmental impacts will mean that social licence for aquaculture will be easier to maintain in potentially threatened areas such as Scottish sea lochs.","4":"63A0BA09-7720-49D3-9FCA-8DC25D630791"},{"1":"10037991","2":"Hybrid Human Artificial Collective Intelligence in Open-Ended Decision Making","3":"HACID develops a novel hybrid collective intelligence for decision support to professionals facing complex open-ended problems, promoting engagement, fairness and trust. A decision support system (HACID-DSS) is proposed that is based on structured domain knowledge, semi-automatically assembled in a domain knowledge graph (DKG) from available data sources, such as scientific and gray literature. Given a specific case within the addressed domain, a pool of experts is consulted to (i) extract supporting evidence and enrich it, generating a case knowledge graph (CKG) as a subset of the DKG, and (ii) provide one or more solutions to the problem. Exploiting the CKG, the HACID-DSS gathers the expert advice in a collective solution that aggregates the individual opinions and expands them with machine-generated suggestions. In this way, HACID harnesses the wisdom of the crowd in open-ended problems, relying on a traceable process based on supporting evidence for better explainability. A set of evaluation methods is proposed to deal with domains where ground truth is not available, demonstrating the suitability of the proposed approach in a wide range of application domains. Demonstrations are provided in two compelling case studies contributing to the UN Sustainable Development Goals: crowd-sourcing medical diagnostics and climate services for urban adaptation.","4":"5F0C9A63-168D-4978-8FAD-B40EADA16764"},{"1":"10057873","2":"HACID","3":"HACID develops a novel hybrid collective intelligence for decision support to professionals facing complex open-ended problems, promoting engagement, fairness and trust. A decision support system (HACID-DSS) is proposed that is based on structured domain knowledge, semi-automatically assembled in a domain knowledge graph (DKG) from available data sources, such as scientific and gray literature. Given a specific case within the addressed domain, a pool of experts is consulted to (i) extract supporting evidence and enrich it, generating a case knowledge graph (CKG) as a subset of the DKG, and (ii) provide one or more solutions to the problem. Exploiting the CKG, the HACID-DSS gathers the expert advice in a collective solution that aggregates the individual opinions and expands them with machine-generated suggestions. In this way, HACID harnesses the wisdom of the crowd in open-ended problems, relying on a traceable process based on supporting evidence for better explainability. A set of evaluation methods is proposed to deal with domains where ground truth is not available, demonstrating the suitability of the proposed approach in a wide range of application domains. Demonstrations are provided in two compelling case studies contributing to the UN Sustainable Development Goals: crowd-sourcing medical diagnostics and climate services for urban adaptation.","4":"5F760912-3B85-47A0-934B-B477AEFF9F9F"},{"1":"NE/C520447/1","2":"Hominoid energetics: could load carriage have driven the early adoption of bipedal locomotion in human evolution?","3":"Habitual walking on two legs (bipedalism) is one of the unique features that distinguishes humans and their immediate fossil ancestors from the chimpanzees, gorillas and all other non-human primates. The evidence from fossil hominid leg bones and preserved trails of footprints shows that this change to bipedal walking happened very soon after the human evolutionary lineage diverged from the African apes, which suggests that bipedalism may have been an important catalyst for some of the other traits that define the human condition. The importance of bipedal locomotion is highlighted by the large number of theories that have been proposed to explain why walking on two legs is preferable to walking on all fours. Many of these theories argue that by using only your legs for walking you are able to free up your arms for some other purpose, and it is often suggested that this other purpose involves manipulating or carrying something - whether it is food, infants, tools or weapons. This is supported by the fact that when chimpanzees are observed walking upright t is often when they are carrying items of food. However carrying unlike walking is an activity that leaves no direct trace in the fossil record. One of the few ways of testing such theories relating to the advantages and disadvantages of behaviours in long extinct animals is to create computer simulations. These simulations allow us to estimate the actual numerical values of the benefits of behavioural change in terms of energy (and hence food) saved. If such a change has a disproportionately large net benefit then we have some evidence to support our hypothesis, however if the effect is small then this would suggest that we need to look elsewhere. The caveat here is that we must be very careful that our simulation of a particular behaviour is good enough otherwise we would be unwise to trust its predictions. The goal of this project is to produce a computer simulation of carrying behaviour in early human and human-like fossils and to use it to estimate the ease with which one of these animals could carry particular objects any given distance. To achieve this we are proposing to modify our existing computerised walking simulator (which is accurate to within 5-15%) so that it can simulate walking whilst carrying a variety of objects. We then propose to adjust the model so that its predictions match the carrying capabilities of human and non-human subjects that we intend to measure experimentally. Finally we aim to scale the model to represent the body shape of the fossil forms which will allow us to estimate the energy cost of carrying in these fossils. We will then be able to see whether our values affect the conclusions of previous work that has used alternative methods to estimate carrying cost. Our walking simulation is at the cutting edge of computer science. It uses an advanced physics engine similar to those used in the latest video games to rapidly calculate the movements of the skeleton depending on the forces generated by the muscles. The muscle forces themselves are chosen using an artificial intelligence technique (the so called genetic algorithm) that lets us tailor these forces to maximise some underlying quantity - in our case the energetic efficiency of carrying. This combination means that our simulations do not simply copy the movements seen in modern humans but are able to generate their own unique sets of movements from scratch which provides a much better estimate of the actual capabilities of fossil animals. For validation we will use the full range of modern biomechanical analysis techniques on our human subjects and for our non-human subjects we intend to use a combination of RADAR and thermal imaging to measure heart and breath rate. These values are known to be related to actual metabolic cost and because they can be measured remotely they are ideal for use with endangered animals such as great apes.","4":"E51FC8E8-4365-4126-A752-A9DBD85801D8"},{"1":"NE/C520463/1","2":"Hominoid energetics: could load carriage have driven the early adoption of bipedal locomotion in human evolution?","3":"Habitual walking on two legs (bipedalism) is one of the unique features that distinguishes humans and their immediate fossil ancestors from the chimpanzees, gorillas and all other non-human primates. The evidence from fossil hominid leg bones and preserved trails of footprints shows that this change to bipedal walking happened very soon after the human evolutionary lineage diverged from the African apes, which suggests that bipedalism may have been an important catalyst for some of the other traits that define the human condition. The importance of bipedal locomotion is highlighted by the large number of theories that have been proposed to explain why walking on two legs is preferable to walking on all fours. Many of these theories argue that by using only your legs for walking you are able to free up your arms for some other purpose, and it is often suggested that this other purpose involves manipulating or carrying something - whether it is food, infants, tools or weapons. This is supported by the fact that when chimpanzees are observed walking upright t is often when they are carrying items of food. However carrying unlike walking is an activity that leaves no direct trace in the fossil record. One of the few ways of testing such theories relating to the advantages and disadvantages of behaviours in long extinct animals is to create computer simulations. These simulations allow us to estimate the actual numerical values of the benefits of behavioural change in terms of energy (and hence food) saved. If such a change has a disproportionately large net benefit then we have some evidence to support our hypothesis, however if the effect is small then this would suggest that we need to look elsewhere. The caveat here is that we must be very careful that our simulation of a particular behaviour is good enough otherwise we would be unwise to trust its predictions. The goal of this project is to produce a computer simulation of carrying behaviour in early human and human-like fossils and to use it to estimate the ease with which one of these animals could carry particular objects any given distance. To achieve this we are proposing to modify our existing computerised walking simulator (which is accurate to within 5-15%) so that it can simulate walking whilst carrying a variety of objects. We then propose to adjust the model so that its predictions match the carrying capabilities of human and non-human subjects that we intend to measure experimentally. Finally we aim to scale the model to represent the body shape of the fossil forms which will allow us to estimate the energy cost of carrying in these fossils. We will then be able to see whether our values affect the conclusions of previous work that has used alternative methods to estimate carrying cost. Our walking simulation is at the cutting edge of computer science. It uses an advanced physics engine similar to those used in the latest video games to rapidly calculate the movements of the skeleton depending on the forces generated by the muscles. The muscle forces themselves are chosen using an artificial intelligence technique (the so called genetic algorithm) that lets us tailor these forces to maximise some underlying quantity - in our case the energetic efficiency of carrying. This combination means that our simulations do not simply copy the movements seen in modern humans but are able to generate their own unique sets of movements from scratch which provides a much better estimate of the actual capabilities of fossil animals. For validation we will use the full range of modern biomechanical analysis techniques on our human subjects and for our non-human subjects we intend to use a combination of RADAR and thermal imaging to measure heart and breath rate. These values are known to be related to actual metabolic cost and because they can be measured remotely they are ideal for use with endangered animals such as great apes.","4":"EA64788B-3FF7-4EEA-A2C3-9E9D23BE40F5"},{"1":"MR/R006393/1","2":"Development of a Gal-free Calcification Resistant Porcine Pericardial Heart Valve: Establishment of Biological Tissue Equivalence.","3":"Heart valves control the normal flow of blood through the lungs and body. These valves may be damaged because of birth defects, old age, or infection. This damage may require heart valves to be replaced with artificial valves to improve the quality of life of heart valve disease patients or to save their lives. There are about 300,000 heart valve replacements worldwide each year. Replacement valves are either mechanical, made of carbon and metal, or biological, made of non-living tissue generally obtained from pigs or cows. Patients and doctors tend to prefer biological heart valves (BHVs) because they generally do not require blood thinners, which are needed with mechanical valves. In younger patients (&lt;60 years) and in children, BHVs wear out more rapidly, sometimes within 5 years. BHVs fail because they build up bone-like deposits of calcium, which weaken the valve, leading to tears, or obstructed blood flow as the calcium deposits block the opening of the valve. Scientists and commercial valve companies have long sought to produce BHVs, which do not calcify, because these could be used in younger patients without the need for blood thinners. So far, calcification-blocking treatments have been able to reduce valve calcification when tested in animals, but have not been able stop calcification in patients or reliably allow the use of BHVs in younger adults. We identified a type of rejection that makes calcification worse in BHV material. This rejection is unique to humans as the human immune system reacts with a substance, called Gal present on BHVs. To block this rejection reaction we have genetically altered pigs that can be used to make BHVs called Gal knockout pigs. The Gal knockout pigs are healthy and normal and their Gal-free tissue has reduced calcification. Before using Gal knockout tissue in patients, we need to be certain the genetic change in Gal knockout pigs has not had a damaging effect on the tissues used to make these BHVs. We have compared the tissues in current standard and Gal knockout BHVs and used simple tests, such as measuring how hard it is to stretch and tear the tissue, to see if the mechanical properties of Gal knockout tissue remain strong and unchanged. We have also made BHVs using both current standard and Gal knockout tissue, and tested them in a laboratory machine that mimics their function in the heart. Both types of BHVs performed similarly in these tests. This current project is to compare how well the Gal knockout tissue works as a BHV in the standard industry animal model. This test is the only way of determining if the valve works well inside of the body and can function to control the normal flow of blood in the heart. Successfully performing this test, which is required by International standards, is a major step forward to making a new BHV, which reduces calcification and be usable in younger patients. Such a new BHV would greatly increase the quality of life for patients. If successful, we hope to advance a new Gal knockout heart valve for a clinical test in man.","4":"FFC6AEF4-4E64-4E80-9056-ADC4D3C77ED6"},{"1":"MC_PC_21007","2":"Development of a Gal-free Calcification Resistant Porcine Pericardial Heart Valve","3":"Heart valves control the normal flow of blood through the lungs and body. These valves may be damaged because of birth defects, old age, or infection. This damage may require heart valves to be replaced with artificial valves to improve the quality of life of heart valve disease patients or to save their lives. There are about 300,000 heart valve replacements worldwide each year. Replacement valves are either mechanical, made of carbon and metal, or biological, made of non-living tissue generally obtained from pigs or cows. Patients and doctors tend to prefer biological heart valves (BHVs) because they generally do not require blood thinners, which are needed with mechanical valves. In younger patients (&lt;60 years) and in children, BHVs wear out more rapidly, sometimes within 5 years. BHVs fail because they build up bone-like deposits of calcium, which weaken the valve, leading to tears, or obstructed blood flow as the calcium deposits block the opening of the valve. Scientists and commercial valve companies have long sought to produce BHVs, which do not calcify, because these could be used in younger patients without the need for blood thinners. So far, calcification-blocking treatments have been able to reduce valve calcification when tested in animals, but have not been able stop calcification in patients or reliably allow the use of BHVs in younger adults. We identified a type of rejection that makes calcification worse in BHV material. This rejection is unique to humans as the human immune system reacts with a substance, called Gal present on BHVs. To block this rejection reaction we have genetically altered pigs that can be used to make BHVs called Gal knockout pigs. The Gal knockout pigs are healthy and normal and their Gal-free tissue has reduced calcification. Before using Gal knockout tissue in patients, we need to be certain the genetic change in Gal knockout pigs has not had a damaging effect on the tissues used to make these BHVs. We have compared the tissues in current standard and Gal knockout BHVs and used simple tests, such as measuring how hard it is to stretch and tear the tissue, to see if the mechanical properties of Gal knockout tissue remain strong and unchanged. We have also made BHVs using both current standard and Gal knockout tissue, and tested them in a laboratory machine that mimics their function in the heart. Both types of BHVs performed similarly in these tests. This current project is to compare how well the Gal knockout tissue works as a BHV in the standard industry animal model. This test is the only way of determining if the valve works well inside of the body and can function to control the normal flow of blood in the heart. Successfully performing this test, which is required by International standards, is a major step forward to making a new BHV, which reduces calcification and be usable in younger patients. Such a new BHV would greatly increase the quality of life for patients. If successful, we hope to advance a new Gal knockout heart valve for a clinical test in man.","4":"AAFE78A6-C210-4D60-87A8-656E489CE5A5"},{"1":"BB/E013430/1","2":"Understanding the constraints on sex ratio adaptation using artificial neural networks","3":"How perfect should behaviour be? There is little doubt that, given sufficient time, and genetic variation, natural selection can produce organisms displaying startlingly precise adaptations to their surroundings. At the same time, a variety of processes can constrain the ability of populations to reach adaptive peaks. Determining the relative importance of these processes is a major challenge still facing evolutionary biology. We will work on a model system that will allow us to say how obtaining and processing information can limit adaptation. We will work on parasitic wasps, which can choose the sex of their offspring by deciding whether or not to fertilize their eggs. The choice of offspring sex in this group has been the subject of much previous work, and the wasps generally show a good but imperfect fit to predicted behaviour. We will ask if we can better understand the behaviour shown in this group of organisms by incorporating the limitations of sensory and nervous systems into our predictions using a modeling approach known as artificial neural networks. The new predictions will then be tested against existing datasets and against new data, obtained by manipulating the information available to a parasitic wasp in the laboratory.","4":"378BF0CE-20B5-4C7D-B1F1-0B62D1EF2617"},{"1":"BB/E014372/2","2":"Understanding the constraints on sex ratio adaptation using artificial neural networks","3":"How perfect should behaviour be? There is little doubt that, given sufficient time, and genetic variation, natural selection can produce organisms displaying startlingly precise adaptations to their surroundings. At the same time, a variety of processes can constrain the ability of populations to reach adaptive peaks. Determining the relative importance of these processes is a major challenge still facing evolutionary biology. We will work on a model system that will allow us to say how obtaining and processing information can limit adaptation. We will work on parasitic wasps, which can choose the sex of their offspring by deciding whether or not to fertilize their eggs. The choice of offspring sex in this group has been the subject of much previous work, and the wasps generally show a good but imperfect fit to predicted behaviour. We will ask if we can better understand the behaviour shown in this group of organisms by incorporating the limitations of sensory and nervous systems into our predictions using a modeling approach known as artificial neural networks. The new predictions will then be tested against existing datasets and against new data, obtained by manipulating the information available to a parasitic wasp in the laboratory.","4":"B186507A-DC65-4FC1-858C-902A2B1B63AA"},{"1":"2451635","2":"Continual and Meta-Learning in Drug Discovery","3":"Identifying and validating novel therapeutic targets, screening and optimising potential drug candidates and investigating their safety and efficacy profiles in pre-clinical and clinical trials is a notoriously difficult process, with attrition rates estimated to be as high as 95%. A single therapeutic may often take more than a decade to develop and requires hundreds of millions to billions of US dollars in R&amp;D spending before it can be brought to market. Additionally, addressing unresolved clinical needs is becoming progressively more demanding, with estimates indicating that inflation-adjusted R&amp;D spending per approved drug is doubling every 9 years. Coinciding with the advent and maturation of many high-throughput screening and multi-omics techniques, there has been a renewed interest in employing machine learning, and specifically deep learning, algorithms to address this decline in productivity. While numerous highly publicised studies have illustrated the undeniable potential of applying deep learning techniques to biomedical research questions, many difficulties remain to be addressed before they can be pervasively employed throughout the drug discovery pipeline. Perhaps the most pertinent obstacle is obtaining adequate data sets that can be used to train deep learning algorithms. Setting aside more fundamental concerns about the reliability, reproducibility, errors and biases of biomedical data in the scientific literature, the predominant practical issues that researchers are faced with are data scarcity and data heterogeneity. A promising approach that could address these shortcomings is optimisation-based meta-learning. Instead of only using the limited data that exists for a given task, a model is trained on a collection of related tasks with the objective of finding an initialisation that can be easily adapted to a new setting with as few additional data points as possible. The potential applications in drug discovery are numerous. Be it for predicting the binding affinities of drug candidates to different macromolecules of interest or identifying essential genes in different cell lines and under different conditions, the concept of leveraging shared structures across distinct tasks to attain an increase in robustness and predictive power promises to increase the breadth of settings to which machine learning techniques can be successfully applied. The goal of the project is to closely examine the numerous meta-learning techniques that are being continually developed and adapt them to the field of drug discovery. Very few publications have employed meta-learning algorithms in the context of biomedical science and many diverse and impactful applications remain to be explored. The project falls within the EPSRC Artificial Intelligence Technologies, Biological Informatics, and Computational and Theoretical Chemistry research areas. The project is co-supervised by scientists from F. Hoffmann-La Roche AG.","4":"8BDF4B39-6A41-41BA-9B25-F3A8ECA52595"},{"1":"2649056","2":"Continual and Meta-Learning in Drug Discovery","3":"Identifying and validating novel therapeutic targets, screening and optimising potential drug candidates and investigating their safety and efficacy profiles in pre-clinical and clinical trials is a notoriously difficult process, with attrition rates estimated to be as high as 95%. A single therapeutic may often take more than a decade to develop and requires hundreds of millions to billions of US dollars in R&amp;D spending before it can be brought to market. Additionally, addressing unresolved clinical needs is becoming progressively more demanding, with estimates indicating that inflation-adjusted R&amp;D spending per approved drug is doubling every 9 years. Coinciding with the advent and maturation of many high-throughput screening and multi-omics techniques, there has been a renewed interest in employing machine learning, and specifically deep learning, algorithms to address this decline in productivity. While numerous highly publicised studies have illustrated the undeniable potential of applying deep learning techniques to biomedical research questions, many difficulties remain to be addressed before they can be pervasively employed throughout the drug discovery pipeline. Perhaps the most pertinent obstacle is obtaining adequate data sets that can be used to train deep learning algorithms. Setting aside more fundamental concerns about the reliability, reproducibility, errors and biases of biomedical data in the scientific literature, the predominant practical issues that researchers are faced with are data scarcity and data heterogeneity. A promising approach that could address these shortcomings is optimisation-based meta-learning. Instead of only using the limited data that exists for a given task, a model is trained on a collection of related tasks with the objective of finding an initialisation that can be easily adapted to a new setting with as few additional data points as possible. The potential applications in drug discovery are numerous. Be it for predicting the binding affinities of drug candidates to different macromolecules of interest or identifying essential genes in different cell lines and under different conditions, the concept of leveraging shared structures across distinct tasks to attain an increase in robustness and predictive power promises to increase the breadth of settings to which machine learning techniques can be successfully applied. The goal of the project is to closely examine the numerous meta-learning techniques that are being continually developed and adapt them to the field of drug discovery. Very few publications have employed meta-learning algorithms in the context of biomedical science and many diverse and impactful applications remain to be explored. The project falls within the EPSRC Artificial Intelligence Technologies, Biological Informatics, and Computational and Theoretical Chemistry research areas. The project is co-supervised by scientists from F. Hoffmann-La Roche AG.","4":"FB090E07-793D-4E8D-83F5-7007E6699E2C"},{"1":"1945083","2":"Agent-Based Modelling and Dynamic Data Assimilation for Modelling Urban Dynamics","3":"Identifying the factors that encourage and discourage attendance in urban spaces is vital from both academic and practitioner perspectives. For policy makers, an understanding of what attracts people to city centres, and what discourages them, is vital for urban planning and emergency management. From an academic perspective, understanding the drivers of footfall is essential in order to answer questions of mobility, inclusivity, and accessibility of opportunities.\\n\\nSimply quantifying footfall, let alone dissecting the underlying drivers, is extremely challenging. Although there are a series of diverse 'big' datasets that are emerging which can contribute to footfall estimates, none in isolation provide a comprehensive picture. In addition, there are no standard methods that are appropriate for assimilating dynamic, diverse, noisy, and biased data sources to create a complete picture.\\n\\nTo address these gaps in our understanding of urban dynamics, this project will embark on an ambitious programme of methodological development and empirical data analysis. It will adapt relevant methods from fields such as computer science (e.g. machine learning and artificial intelligence), atmospheric modelling (e.g. data assimilation and ensemble modelling), and geography (e.g. GIS and spatial analysis) to create a robust model of footfall. Indicative data source that will underpin the analysis include: footfall data collected by Leeds City Council CCTV cameras; Census workday population estimates; dynamic weather data; geo-located Twitter data; times of public events and holidays; business opening hours; Wi-Fi sensor footfall data; and others as they become available. There is great potential to leverage these data as both an explanatory tool for understanding what has been driving city-centre attendance and as a predictive tool for forecasting future footfall under different scenarios. Leeds City Council are actively involved in designing the research questions, identifying data sources, and will jointly supervise the research. \\n\\nThis project will feed into work across the CDRC remit, providing baseline and temporally nuanced urban populations, along with an explanation of their drivers. It will feed into real-world urban management systems at Leeds City Council, as well as broader cities and future cities literature. Finally, it will act as a foundation for future work in emergency planning and diurnal population movements, as well as longer-term predictions of city accessibility.","4":"48AE6884-D5B3-4AB7-A54A-91D874D55E40"},{"1":"1944290","2":"Predictive Data Analytics for Urban Dynamics","3":"Identifying the factors that encourage and discourage attendance in urban spaces is vital from both academic and practitioner perspectives. For policy makers, an understanding of what attracts people to city centres, and what discourages them, is vital for urban planning and emergency management. From an academic perspective, understanding the drivers of footfall is essential in order to answer questions of mobility, inclusivity, and accessibility of opportunities.\\n\\nSimply quantifying footfall, let alone dissecting the underlying drivers, is extremely challenging. Although there are a series of diverse 'big' datasets that are emerging which can contribute to footfall estimates, none in isolation provide a comprehensive picture. In addition, there are no standard methods that are appropriate for assimilating dynamic, diverse, noisy, and biased data sources to create a complete picture.\\n\\nTo address these gaps in our understanding of urban dynamics, this project will embark on an ambitious programme of methodological development and empirical data analysis. It will adapt relevant methods from fields such as computer science (e.g. machine learning and artificial intelligence), atmospheric modelling (e.g. data assimilation and ensemble modelling), and geography (e.g. GIS and spatial analysis) to create a robust model of footfall. Indicative data source that will underpin the analysis include: footfall data collected by Leeds City Council CCTV cameras; Census workday population estimates; dynamic weather data; geo-located Twitter data; times of public events and holidays; business opening hours; Wi-Fi sensor footfall data; and others as they become available. There is great potential to leverage these data as both an explanatory tool for understanding what has been driving city-centre attendance and as a predictive tool for forecasting future footfall under different scenarios. Leeds City Council are actively involved in designing the research questions, identifying data sources, and will jointly supervise the research. \\n\\nThis project will feed into work across the CDRC remit, providing baseline and temporally nuanced urban populations, along with an explanation of their drivers. It will feed into real-world urban management systems at Leeds City Council, as well as broader cities and future cities literature. Finally, it will act as a foundation for future work in emergency planning and diurnal population movements, as well as longer-term predictions of city accessibility.","4":"7D06A2ED-00E5-4B29-8C43-BE369626B83E"},{"1":"NE/M002896/1","2":"Genomic prediction in a wild mammal","3":"Imagine a world where a scientist could sample an animal or plant and, by DNA profiling, predict what it would look like, how long it would live, how many offspring it would have, and whether or not it would out-compete other members of its population. Although the idea seems fanciful, it has become a possibility, even for wild populations within complex ecological systems. The aim of this proposal is to develop, test and apply so called 'genomic prediction' methods for use in evolutionary ecology.\\n\\nIn the last decade remarkable advances in genomics methods, most notably next-generation sequencing, have revolutionised all areas of biological research. It is now possible to generate DNA profiles at hundreds of thousands of variable sites across the genome, in any organism. Many of these sites (known as single nucleotide polymorphisms, or SNPs) will reside within, or very close to, genes that cause phenotypic variation. Traditionally, the search for these genes, or quantitative trait loci (QTL), has involved testing each SNP individually and then identifying those which are statistically significant. However, this approach is problematic, in that it is biased towards finding genes of large effect, which for many phenotypes simply do not exist. If, as is more common, there are many genes of small effect then QTL will remain undetected. In animal and plant breeding, the problem has been solved by considering the phenotypic effect of all SNPs simultaneously. First a 'training population' of genotyped samples with known phenotype are used to estimate effect sizes of each SNP. Then a second sample of 'test' individuals is genotyped, and the genotypes are used to predict phenotype; i.e. perform genomic prediction. This approach underpins successful modern artificial selection programmes and is set to be used in personalised medicine. However, genomic prediction has never been applied to wild populations, despite its potential to revolutionise evolutionary ecological genetics.\\n\\nWe will test and apply genomic prediction in the feral population of Soay sheep on the island of Hirta (St Kilda, Scotland); one of the most intensively studied vertebrate populations in the world. Since 1985, over 95% of animals born in the Village Bay study area have been monitored over their entire lifetimes, such that detailed life histories (e.g. date of birth, date of death, sex, twin status, morphological measurements, immunological assays, parasite loads and lifetime fitness) are described for over 7000 sheep. Many traits have been measured numerous times across development. Furthermore, the sheep genome has been sequenced and most of the Soay study population has been typed at 38K SNPs discovered by the International Sheep Genomics Consortium. Additional features that make Soay sheep the ideal system for testing genomic prediction are: (i) different traits have well described and very different genetic architectures. eg. coat colour and horn type have a simple genetic basis while skeletal measurements are far more polygenic (but still highly heritable) and (ii) linkage disequilibrium extends for long distances in the genome, so that the SNPs on the chip 'tag' most of the genome. Using a 'training population' of all animals born until 2010 we will estimate the effects of individual SNPs, and then use these estimates to predict the phenotype of animals born after 2010. We will compare the predictions to observed values; the first time genomic prediction has been tested or applied in a wild population. We will also use genomic predictions to establish which traits have made an evolutionary response to natural selection.\\n\\nWe predict that genomic prediction will be achievable in our study population and that it will outperform traditional pedigree-based approaches to studying micro-evolution in nature.","4":"540CD214-8219-441C-8277-80D53386A73D"},{"1":"NE/M003035/1","2":"Genomic prediction in a wild mammal","3":"Imagine a world where a scientist could sample an animal or plant and, by DNA profiling, predict what it would look like, how long it would live, how many offspring it would have, and whether or not it would out-compete other members of its population. Although the idea seems fanciful, it has become a possibility, even for wild populations within complex ecological systems. The aim of this proposal is to develop, test and apply so called 'genomic prediction' methods for use in evolutionary ecology.\\n\\nIn the last decade remarkable advances in genomics methods, most notably next-generation sequencing, have revolutionised all areas of biological research. It is now possible to generate DNA profiles at hundreds of thousands of variable sites across the genome, in any organism. Many of these sites (known as single nucleotide polymorphisms, or SNPs) will reside within, or very close to, genes that cause phenotypic variation. Traditionally, the search for these genes, or quantitative trait loci (QTL), has involved testing each SNP individually and then identifying those which are statistically significant. However, this approach is problematic, in that it is biased towards finding genes of large effect, which for many phenotypes simply do not exist. If, as is more common, there are many genes of small effect then QTL will remain undetected. In animal and plant breeding, the problem has been solved by considering the phenotypic effect of all SNPs simultaneously. First a 'training population' of genotyped samples with known phenotype are used to estimate effect sizes of each SNP. Then a second sample of 'test' individuals is genotyped, and the genotypes are used to predict phenotype; i.e. perform genomic prediction. This approach underpins successful modern artificial selection programmes and is set to be used in personalised medicine. However, genomic prediction has never been applied to wild populations, despite its potential to revolutionise evolutionary ecological genetics.\\n\\nWe will test and apply genomic prediction in the feral population of Soay sheep on the island of Hirta (St Kilda, Scotland); one of the most intensively studied vertebrate populations in the world. Since 1985, over 95% of animals born in the Village Bay study area have been monitored over their entire lifetimes, such that detailed life histories (e.g. date of birth, date of death, sex, twin status, morphological measurements, immunological assays, parasite loads and lifetime fitness) are described for over 7000 sheep. Many traits have been measured numerous times across development. Furthermore, the sheep genome has been sequenced and most of the Soay study population has been typed at 38K SNPs discovered by the International Sheep Genomics Consortium. Additional features that make Soay sheep the ideal system for testing genomic prediction are: (i) different traits have well described and very different genetic architectures. eg. coat colour and horn type have a simple genetic basis while skeletal measurements are far more polygenic (but still highly heritable) and (ii) linkage disequilibrium extends for long distances in the genome, so that the SNPs on the chip 'tag' most of the genome. Using a 'training population' of all animals born until 2010 we will estimate the effects of individual SNPs, and then use these estimates to predict the phenotype of animals born after 2010. We will compare the predictions to observed values; the first time genomic prediction has been tested or applied in a wild population. We will also use genomic predictions to establish which traits have made an evolutionary response to natural selection.\\n\\nWe predict that genomic prediction will be achievable in our study population and that it will outperform traditional pedigree-based approaches to studying micro-evolution in nature.","4":"2170E822-47E9-4F62-A4DE-521CDE3ECD95"},{"1":"2699078","2":"Micro-Electronics for autonomous neural implants","3":"Implantable neural interfaces can be used to connect human brains to artificial electronic circuits allowing e.g. control of computers by thoughts or treatment of various injuries and illnesses. An example of such is the possibility of treating spinal cord injuries by bypassing the damaged neural connection and allowing control of artificial limbs. This is achieved by inserting electrodes into neural tissue connected to instrumentation circuits recording electronic potentials generated by active neurons and decoding their meaning. While past solutions typically relied on recording of extracellular action potentials (EAPs), also known as neural spikes, generated by single neurons, the aim of this project is to focus on acquisition and processing of local field potentials (LFPs). This is as EAP-recording implants are typically hindered by limited lifetime due to the host body's foreign body response leading to scar tissue growth acting as a spatial and frequential low-pass filter, hence limiting the fidelity of recorded high-frequency EAPs. The low-frequency nature of LFPs allows for a significant reduction of the effect scar-tissue growth has on the recording.\\n\\nSome of the involved challenges include selection of electrode material. This has to ensure minimal added thermal noise within the frequency band of LFPs when in contact with cerebrospinal fluid while at the same time being chemically inert and medically harmless. Preliminary results have shown Niobium (Nb) as a promising material suitable for such a recording as it is known to be biologically inert and is commonly used e.g. in dental implants. Its polarizability leads to generally smaller noise power densities in LFP frequency bands than commonly used platinum and tungsten making it a suitable candidate material for neural recordings. This is to be investigated by direct noise measurements in electrolytes and subsequently verified by direct in-vivo measurements.\\n\\nAnother challenge lies in the development of acquisition electronics which is greatly constrained by limits imposed on their power consumption governed by safety limits of heat dissipation in neural tissue on the order of 80 mW/cm^2. One of the techniques allowing reduction of used energy is clock-less, also known as continuous-time (CT), acquisition of signals. This approach leads to activity-dependent circuits that only use energy when activity is detected at their input. One of the aims of this project is to investigate the suitability and possible advantages of such circuits for acquisition of LFPs. As properties of such acquisition and sampling processes remain to a large extent unknown, mathematical simulations are used for their investigation. This is complemented by design of integrated circuits and practical measurements verifying hypotheses arising from theoretical models. This has already led to formulation of a minimal requirement to be satisfied in order to prevent aliasing similar to the Nyquist theorem which applies to ordinary sampling. In addition a novel method allowing reduction of flicker noise in CT sampling has been proposed and theoretically validated. An integrated circuit verifying this theory is to be designed, manufactured and tested.\\n\\nThis work is part of the ENGINI project with an aim to design the next generation of neural implants that achieve superior chronicity by being completely wireless, minimal in size, targeting LFP recordings and allowing formation of distributed implant networks.\\n\\nThe research aligns with the following EPSRC Research Areas: Assistive technology, rehabilitation and musculoskeletal biomechanics; Microelectronic device technology","4":"EB4CADB1-34E7-45A3-8BDD-2CB76F0AE0BE"},{"1":"1859713","2":"Micro-Electronics for autonomous neural implants","3":"Implantable neural interfaces can be used to connect human brains to artificial electronic circuits allowing e.g. control of computers by thoughts or treatment of various injuries and illnesses. An example of such is the possibility of treating spinal cord injuries by bypassing the damaged neural connection and allowing control of artificial limbs. This is achieved by inserting electrodes into neural tissue connected to instrumentation circuits recording electronic potentials generated by active neurons and decoding their meaning. While past solutions typically relied on recording of extracellular action potentials (EAPs), also known as neural spikes, generated by single neurons, the aim of this project is to focus on acquisition and processing of local field potentials (LFPs). This is as EAP-recording implants are typically hindered by limited lifetime due to the host body's foreign body response leading to scar tissue growth acting as a spatial and frequential low-pass filter, hence limiting the fidelity of recorded high-frequency EAPs. The low-frequency nature of LFPs allows for a significant reduction of the effect scar-tissue growth has on the recording.\\n\\nSome of the involved challenges include selection of electrode material. This has to ensure minimal added thermal noise within the frequency band of LFPs when in contact with cerebrospinal fluid while at the same time being chemically inert and medically harmless. Preliminary results have shown Niobium (Nb) as a promising material suitable for such a recording as it is known to be biologically inert and is commonly used e.g. in dental implants. Its polarizability leads to generally smaller noise power densities in LFP frequency bands than commonly used platinum and tungsten making it a suitable candidate material for neural recordings. This is to be investigated by direct noise measurements in electrolytes and subsequently verified by direct in-vivo measurements.\\n\\nAnother challenge lies in the development of acquisition electronics which is greatly constrained by limits imposed on their power consumption governed by safety limits of heat dissipation in neural tissue on the order of 80 mW/cm^2. One of the techniques allowing reduction of used energy is clock-less, also known as continuous-time (CT), acquisition of signals. This approach leads to activity-dependent circuits that only use energy when activity is detected at their input. One of the aims of this project is to investigate the suitability and possible advantages of such circuits for acquisition of LFPs. As properties of such acquisition and sampling processes remain to a large extent unknown, mathematical simulations are used for their investigation. This is complemented by design of integrated circuits and practical measurements verifying hypotheses arising from theoretical models. This has already led to formulation of a minimal requirement to be satisfied in order to prevent aliasing similar to the Nyquist theorem which applies to ordinary sampling. In addition a novel method allowing reduction of flicker noise in CT sampling has been proposed and theoretically validated. An integrated circuit verifying this theory is to be designed, manufactured and tested.\\n\\nThis work is part of the ENGINI project with an aim to design the next generation of neural implants that achieve superior chronicity by being completely wireless, minimal in size, targeting LFP recordings and allowing formation of distributed implant networks.\\n\\nThe research aligns with the following EPSRC Research Areas: Assistive technology, rehabilitation and musculoskeletal biomechanics; Microelectronic device technology","4":"DFD9A14E-DDB6-45D8-BB3A-C0782F195C3E"},{"1":"2164505","2":"AI-Generated Objects for Cyber Defensive Purposes as Intellectual Property","3":"In my DPhil project, I intend to bring novel, updated and illuminating insights into the topic of computational creativity in the context of current state of the art in artificial intelligence algorithms deployed in network defence. My Thesis will feature a legal assessment of whether objects generated by artificial intelligence-driven systems qualify for copyright and patent protection under the current legal regimes in Europe and in the United Kingdom. It should also contain recommendations towards adaptations of intellectual property laws for the emerging realities of computational creativity. If these objects generated by an artificial intelligence-driven network defence system were protectable by intellectual property rights, it could eventually lead to rapid sharing and spread of cyber-security knowledge, enhanced trust in information communication technologies and stability for economies and democracy. \\n\\nThis research project is linked the EPSRC Cyber Security Theme and Law","4":"EA84D26D-1289-4197-9AB0-4ACC08ED7E88"},{"1":"2873242","2":"AI-Generated Objects for Cyber Defensive Purposes as Intellectual Property","3":"In my DPhil project, I intend to bring novel, updated and illuminating insights into the topic of computational creativity in the context of current state of the art in artificial intelligence algorithms deployed in network defence. My Thesis will feature a legal assessment of whether objects generated by artificial intelligence-driven systems qualify for copyright and patent protection under the current legal regimes in Europe and in the United Kingdom. It should also contain recommendations towards adaptations of intellectual property laws for the emerging realities of computational creativity. If these objects generated by an artificial intelligence-driven network defence system were protectable by intellectual property rights, it could eventually lead to rapid sharing and spread of cyber-security knowledge, enhanced trust in information communication technologies and stability for economies and democracy. \\n\\nThis research project is linked the EPSRC Cyber Security Theme and Law","4":"043A1657-8E1C-4526-8A99-5C8A8B54006A"},{"1":"EP/E061915/1","2":"Defying the rules: How self-regulatory social systems work","3":"In nature, systems self-regulate and self-stabilise through non-centralised bottom-level rules. A system that is able to adapt to (internal and external) feedback and context is self-regulatory. The dynamics of such a system modifies the environment in which it is evolving. This therefore represents a feedback loop which is an emergent structure, since it is the result of the dynamics, which in turn is determined by the interactions at the bottom-level. The feedback loop can be pictured as an engine that allows the system to develop and eventually reach sustainability through self-regulation. The seed, or the spark for the engine, is given by the rules at the bottom-level in context. These as referred to as generic rules.The aim of this project is to identify the generic rules, feedback and context that allow systems to develop and reach sustainability through self-regulation. This will be achieved by contrasting the social behaviour in three different systems, a biological social system where we know we have emergent behaviour, an artificial social system where we have full control, and human social systems where observable data is available.The biological system that we will investigate is ant colonies. This is tractable from the microscopic individual-level to the macroscopic social level. The inferred bottom-level rules will be explored and verified in totally controlled experiments with robots. This will allow us to develop a conceptual and theoretical framework for self-regulatory social systems.The theoretical framework will serve as a foundation to design self-sustainable bottom-up programmes for community regeneration and control methods for automated manufacturing.This is very timely since in the case of rural and urban regeneration, the limitations on traditional mechanistic top-down approaches to socio-economic development have been widely documented. Sponsoring multinational agencies, like the World Bank and United Nations, have recognised the importance of shifting the paradigm of development to more holistic approaches, empowering and supporting communities to lead and self-regulate their main development agendas. However, the effort made in practice has not yet achieved that goal.In the case of the manufacturing industry, the efficiency is limited due to the low performance of existing methods for controlling large collaborative multi-robot systems. Traditional, centralised, top-down algorithms are generally preferable in small systems as they can identify globally optimal solutions. In large systems, however, it is not possible to identify optimal solutions due to the complexity of the problem. As a result, behaviour-based, distributed, bottom-up algorithms which employ some form of individual learning from experience, have been studied extensively and they have been shown to have superior performance.The establishment of generic rules, giving rise to self-regulatory systems, will initiate new research worldwide. The developed conceptual and theoretical framework will provide a new approach for understanding the behaviour of dynamical systems. The potential applications within such a framework are limitless, and span over a variety of fields, for example, nanotechnology, evolutionary theory, synthetic biology, and social systems.","4":"5C5056AB-274A-4F89-A923-69C00E24EBE2"},{"1":"EP/E061796/1","2":"Defying the rules: How self-regulatory social systems work","3":"In nature, systems self-regulate and self-stabilise through non-centralised bottom-level rules. A system that is able to adapt to (internal and external) feedback and context is self-regulatory. The dynamics of such a system modifies the environment in which it is evolving. This therefore represents a feedback loop which is an emergent structure, since it is the result of the dynamics, which in turn is determined by the interactions at the bottom-level. The feedback loop can be pictured as an engine that allows the system to develop and eventually reach sustainability through self-regulation. The seed, or the spark for the engine, is given by the rules at the bottom-level in context. These as referred to as generic rules.The aim of this project is to identify the generic rules, feedback and context that allow systems to develop and reach sustainability through self-regulation. This will be achieved by contrasting the social behaviour in three different systems, a biological social system where we know we have emergent behaviour, an artificial social system where we have full control, and human social systems where observable data is available.The biological system that we will investigate is ant colonies. This is tractable from the microscopic individual-level to the macroscopic social level. The inferred bottom-level rules will be explored and verified in totally controlled experiments with robots. This will allow us to develop a conceptual and theoretical framework for self-regulatory social systems.The theoretical framework will serve as a foundation to design self-sustainable bottom-up programmes for community regeneration and control methods for automated manufacturing.This is very timely since in the case of rural and urban regeneration, the limitations on traditional mechanistic top-down approaches to socio-economic development have been widely documented. Sponsoring multinational agencies, like the World Bank and United Nations, have recognised the importance of shifting the paradigm of development to more holistic approaches, empowering and supporting communities to lead and self-regulate their main development agendas. However, the effort made in practice has not yet achieved that goal.In the case of the manufacturing industry, the efficiency is limited due to the low performance of existing methods for controlling large collaborative multi-robot systems. Traditional, centralised, top-down algorithms are generally preferable in small systems as they can identify globally optimal solutions. In large systems, however, it is not possible to identify optimal solutions due to the complexity of the problem. As a result, behaviour-based, distributed, bottom-up algorithms which employ some form of individual learning from experience, have been studied extensively and they have been shown to have superior performance.The establishment of generic rules, giving rise to self-regulatory systems, will initiate new research worldwide. The developed conceptual and theoretical framework will provide a new approach for understanding the behaviour of dynamical systems. The potential applications within such a framework are limitless, and span over a variety of fields, for example, nanotechnology, evolutionary theory, synthetic biology, and social systems.","4":"1B831F15-CB12-4E30-8D1A-36192B361F8C"},{"1":"EP/E061761/1","2":"Defying the rules: How self-regulatory social systems work","3":"In nature, systems self-regulate and self-stabilise through non-centralised bottom-level rules. A system that is able to adapt to (internal and external) feedback and context is self-regulatory. The dynamics of such a system modifies the environment in which it is evolving. This therefore represents a feedback loop which is an emergent structure, since it is the result of the dynamics, which in turn is determined by the interactions at the bottom-level. The feedback loop can be pictured as an engine that allows the system to develop and eventually reach sustainability through self-regulation. The seed, or the spark for the engine, is given by the rules at the bottom-level in context. These as referred to as generic rules.The aim of this project is to identify the generic rules, feedback and context that allow systems to develop and reach sustainability through self-regulation. This will be achieved by contrasting the social behaviour in three different systems, a biological social system where we know we have emergent behaviour, an artificial social system where we have full control, and human social systems where observable data is available.The biological system that we will investigate is ant colonies. This is tractable from the microscopic individual-level to the macroscopic social level. The inferred bottom-level rules will be explored and verified in totally controlled experiments with robots. This will allow us to develop a conceptual and theoretical framework for self-regulatory social systems.The theoretical framework will serve as a foundation to design self-sustainable bottom-up programmes for community regeneration and control methods for automated manufacturing.This is very timely since in the case of rural and urban regeneration, the limitations on traditional mechanistic top-down approaches to socio-economic development have been widely documented. Sponsoring multinational agencies, like the World Bank and United Nations, have recognised the importance of shifting the paradigm of development to more holistic approaches, empowering and supporting communities to lead and self-regulate their main development agendas. However, the effort made in practice has not yet achieved that goal.In the case of the manufacturing industry, the efficiency is limited due to the low performance of existing methods for controlling large collaborative multi-robot systems. Traditional, centralised, top-down algorithms are generally preferable in small systems as they can identify globally optimal solutions. In large systems, however, it is not possible to identify optimal solutions due to the complexity of the problem. As a result, behaviour-based, distributed, bottom-up algorithms which employ some form of individual learning from experience, have been studied extensively and they have been shown to have superior performance.The establishment of generic rules, giving rise to self-regulatory systems, will initiate new research worldwide. The developed conceptual and theoretical framework will provide a new approach for understanding the behaviour of dynamical systems. The potential applications within such a framework are limitless, and span over a variety of fields, for example, nanotechnology, evolutionary theory, synthetic biology, and social systems.","4":"552CCA5D-2681-4C01-9548-42FB6839CB0A"},{"1":"EP/E061982/1","2":"Defying the rules: How self-regulatory social systems work","3":"In nature, systems self-regulate and self-stabilise through non-centralised bottom-level rules. A system that is able to adapt to (internal and external) feedback and context is self-regulatory. The dynamics of such a system modifies the environment in which it is evolving. This therefore represents a feedback loop which is an emergent structure, since it is the result of the dynamics, which in turn is determined by the interactions at the bottom-level. The feedback loop can be pictured as an engine that allows the system to develop and eventually reach sustainability through self-regulation. The seed, or the spark for the engine, is given by the rules at the bottom-level in context. These as referred to as generic rules.The aim of this project is to identify the generic rules, feedback and context that allow systems to develop and reach sustainability through self-regulation. This will be achieved by contrasting the social behaviour in three different systems, a biological social system where we know we have emergent behaviour, an artificial social system where we have full control, and human social systems where observable data is available.The biological system that we will investigate is ant colonies. This is tractable from the microscopic individual-level to the macroscopic social level. The inferred bottom-level rules will be explored and verified in totally controlled experiments with robots. This will allow us to develop a conceptual and theoretical framework for self-regulatory social systems.The theoretical framework will serve as a foundation to design self-sustainable bottom-up programmes for community regeneration and control methods for automated manufacturing.This is very timely since in the case of rural and urban regeneration, the limitations on traditional mechanistic top-down approaches to socio-economic development have been widely documented. Sponsoring multinational agencies, like the World Bank and United Nations, have recognised the importance of shifting the paradigm of development to more holistic approaches, empowering and supporting communities to lead and self-regulate their main development agendas. However, the effort made in practice has not yet achieved that goal.In the case of the manufacturing industry, the efficiency is limited due to the low performance of existing methods for controlling large collaborative multi-robot systems. Traditional, centralised, top-down algorithms are generally preferable in small systems as they can identify globally optimal solutions. In large systems, however, it is not possible to identify optimal solutions due to the complexity of the problem. As a result, behaviour-based, distributed, bottom-up algorithms which employ some form of individual learning from experience, have been studied extensively and they have been shown to have superior performance.The establishment of generic rules, giving rise to self-regulatory systems, will initiate new research worldwide. The developed conceptual and theoretical framework will provide a new approach for understanding the behaviour of dynamical systems. The potential applications within such a framework are limitless, and span over a variety of fields, for example, nanotechnology, evolutionary theory, synthetic biology, and social systems.","4":"64967F8E-6BFC-4CD2-BB0F-31A60A6835AA"},{"1":"BB/H008500/1","2":"Integrating developmental pathways and chromatin structure during lineage specifcation","3":"In order for the fertilized egg to develop into a live-borne animal, genes that control the regenerative capacity, identity and fate of cells must be switched on and off at the right time and place. Changes in the way that the DNA sequence is packaged up with proteins, to form a structure called chromatin, are important in this regulation of gene expression. However, to date this has mainly been studied in artificial cell culture systems and little is known about the changes in chromatin structure that happen at specific genes in a situation that is more relevant to the development of the embryo. We propose to use a newly developed cell culture system that enables mouse embryonic stem cells to be directed to undergo development towards cell types that usually go on to form muscle and bone (mesoderm) or gut, lung, liver and pancreas (endoderm). With this system we can produce large quantities of cells that closely resemble their equivalents in an embryo and challenge these cells with specific chemical signals that are known to be important for embryonic development. This system will be used to study how chromatin structure is changed both globally and at a particular set of genes, the Hox genes, which are key regulators of development. Our global analysis will help us to understand the way in which cells are progressively restricted to the mesoderm and endoderm lineages, while at the Hox cluster in particular we will be able to ask specific questions about how this happens. This work will help to better understand how stem cells can be used to target organs derived from these cell types in regenerative medicine.","4":"64AEDC2C-D47F-480E-B1B0-3254CFED3333"},{"1":"BB/H005978/1","2":"Integrating developmental pathways and chromatin structure during lineage specification","3":"In order for the fertilized egg to develop into a live-borne animal, genes that control the regenerative capacity, identity and fate of cells must be switched on and off at the right time and place. Changes in the way that the DNA sequence is packaged up with proteins, to form a structure called chromatin, are important in this regulation of gene expression. However, to date this has mainly been studied in artificial cell culture systems and little is known about the changes in chromatin structure that happen at specific genes in a situation that is more relevant to the development of the embryo. We propose to use a newly developed cell culture system that enables mouse embryonic stem cells to be directed to undergo development towards cell types that usually go on to form muscle and bone (mesoderm) or gut, lung, liver and pancreas (endoderm). With this system we can produce large quantities of cells that closely resemble their equivalents in an embryo and challenge these cells with specific chemical signals that are known to be important for embryonic development. This system will be used to study how chromatin structure is changed both globally and at a particular set of genes, the Hox genes, which are key regulators of development. Our global analysis will help us to understand the way in which cells are progressively restricted to the mesoderm and endoderm lineages, while at the Hox cluster in particular we will be able to ask specific questions about how this happens. This work will help to better understand how stem cells can be used to target organs derived from these cell types in regenerative medicine.","4":"1930DB26-72AC-4060-87F5-EF6EE0B24AE4"},{"1":"10042622","2":"European Lighthouse on Secure and Safe AI","3":"In order to reinforce European leadership in safe and secure AI technology, we are proposing a virtual center of excellence on safe and secure AI that will address major challenges hampering the deployment of AI technology. These grand challenges are fundamental in nature. Addressing them in a sustainable manner requires a lighthouse rooted in scientific excellence and rigorous methods. We will develop a strategic research agenda which is supported by research programmes that focus on “technical robustness and safety”, “privacy preserving techniques and infrastructures” and “human agency and oversight”. Furthermore, we focus our efforts to detect, prevent and mitigate threats and enable recovery from harm by 3 grand challenges: “Robustness guarantees and certification”, “Private and robust collaborative learning at scale” and “Human-in-the-loop decision making: Integrated governance to ensure meaningful oversight” that cut across 6 use cases: health, autonomous driving, robotics, cybersecurity, multi-media, and document intelligence. Throughout our project, we seek to integrate robust technical approaches with legal and ethical principles supported by meaningful and effective governance architectures to nurture and sustain the development and deployment of AI technology that serves and promotes foundational European values. Our initiative builds on and expands the internationally recognized, highly successful and fully operational network of excellence ELLIS (European Laboratory for Learning and Intelligent Systems). We build ELSA on its 3 pillars: research programmes, a set of research units, and a PhD/postdoc programme, thereby connecting a network of over 100 organizations and more than 337 ELLIS fellows and scholars (113 ERC grants) committed to shared standards of excellence. We will not only establish a virtual center of excellence, but all our activities will be also inclusive and open to input, interactions and collaboration of AI researchers and industrial partners in order to drive the entire field forward.","4":"CD453216-5DCA-4C43-963D-7D37C112D84A"},{"1":"10044744","2":"European Lighthouse on Secure and Safe AI (ELSA)","3":"In order to reinforce European leadership in safe and secure AI technology, we are proposing a virtual center of excellence on safe and secure AI that will address major challenges hampering the deployment of AI technology. These grand challenges are fundamental in nature. Addressing them in a sustainable manner requires a lighthouse rooted in scientific excellence and rigorous methods. We will develop a strategic research agenda which is supported by research programmes that focus on “technical robustness and safety”, “privacy preserving techniques and infrastructures” and “human agency and oversight”. Furthermore, we focus our efforts to detect, prevent and mitigate threats and enable recovery from harm by 3 grand challenges: “Robustness guarantees and certification”, “Private and robust collaborative learning at scale” and “Human-in-the-loop decision making: Integrated governance to ensure meaningful oversight” that cut across 6 use cases: health, autonomous driving, robotics, cybersecurity, multi-media, and document intelligence. Throughout our project, we seek to integrate robust technical approaches with legal and ethical principles supported by meaningful and effective governance architectures to nurture and sustain the development and deployment of AI technology that serves and promotes foundational European values. Our initiative builds on and expands the internationally recognized, highly successful and fully operational network of excellence ELLIS (European Laboratory for Learning and Intelligent Systems). We build ELSA on its 3 pillars: research programmes, a set of research units, and a PhD/postdoc programme, thereby connecting a network of over 100 organizations and more than 337 ELLIS fellows and scholars (113 ERC grants) committed to shared standards of excellence. We will not only establish a virtual center of excellence, but all our activities will be also inclusive and open to input, interactions and collaboration of AI researchers and industrial partners in order to drive the entire field forward.","4":"F32E316B-29F9-438A-9AD7-5BC745F9DC8F"},{"1":"NE/I022337/1","2":"Mitigating climate change impacts on India agriculture through improved Irrigation water Management","3":"In this project, we propose to investigate the effects of climate change and variability on irrigation water security in India and to evaluate the effectiveness of better irrigation and water management strategies in mitigating any water shortage situation and so improve the productivity of the available water. To assess the impacts of climate change on the runoff and hence on groundwater recharge and inflows into rivers, canals and on-farm reservoir systems at the catchment scale, we will use the Soil Water Assessment Tool (SWAT) hydrological model, appropriately calibrated and validated, and a range of GCMs projections and emissions scenarios.\\n\\nTo better understand the impacts on irrigation water requirements at the farm scale, we propose to use an existing physically-based root water uptake model that will be further calibrated and tested using data that will be collected at experimental farms in four diverse climatic zones spread over the four Indian states of Haryana, Himachal Pradesh, Uttarapradesh and Uttarakhand. For future conditions, the calibrated model will be coupled with the SWAT model. \\n\\nGiven the assessed irrigation water requirements for the future, the adequacy of existing on-farm irrigation infrastructures- direct river abstractions, reservoirs and tube wells- will be assessed in terms of reliability, vulnerability, resilience and sustainability. We propose to develop better operational practices for the infrastructures- e.g. enhanced rule curves for reservoirs, conjunctive use of reservoirs, rivers, groundwater etc- that will temper the effect of water shortages or desynchronise water availability from rainfall on crop production. The project will involve collaboration between internationally-leading scientists at 3 Indian institutions (IIT-Roorkee; NIT-Kurukshetra; NIT, Hamirpur) and 2 UK Universities: Heriot-Watt University, Edinburgh (H-WU), Cranfield University (CU). It will build on an existing and thriving collaboration between the H-WU team and the team at the IIT-Roorkee. We will work with stakeholders in India (farmers, regional and regional to national policymakers) and leading UK irrigation practitioners. The project will last for 48 months and comprise four work packages as follows: \\n\\nWP 1- Impacts and uncertainty, of climate change on water resources availability for agriculture in India: This WP aims to understand climate change impacts on water availability and to identify thresholds in catchment/aquifer-scale runoff, groundwater recharge and responses (led by CU). \\nWP 2- Impacts of shifting temporal and spatial rainfall patterns on crop-soil moisture regimes and potential irrigation demand, and associated field experimentation: The effects of the changing temperature and rainfall patterns on farm-scale soil moisture and hence future irrigation water requirements with be assessed with IIT-R physically-based root water uptake model, which will be further calibrated and validated against observed crop yields and soil moisture data from dedicated field experiments at 4 sites across 4 Indian states (Led by IIT-R).\\nWP 3- Understanding the potential of robust irrigation water management practices and systems to reduce the impacts of future water scarcity: The effectiveness of on-farm infrastructures will be assessed for their reliability, vulnerability resilience and sustainability to deliver irrigation water needs for existing and climate-change conditions. Better operational practices- irrigation water scheduling; rule and operating policies for on-farm reservoirs; conjunctive use and artificial recharge- that are robust to the uncertainties in the climate change predictions will be developed and evaluated (Led by H-WU)\\n WP 4- Stakeholder engagement and dissemination: A broad range of contextually-appropriate engagement and dissemination activities will be co-ordinated by IIT-Roorkee throughout the project duration, to maximise stakeholder buy-in (Joint UK/Indian led).","4":"554C8C66-69DA-4385-BF1F-C4B14ECB45F0"},{"1":"NE/I022329/1","2":"Mitigating climate change impacts on India agriculture through improved Irrigation water Management","3":"In this project, we propose to investigate the effects of climate change and variability on irrigation water security in India and to evaluate the effectiveness of better irrigation and water management strategies in mitigating any water shortage situation and so improve the productivity of the available water. To assess the impacts of climate change on the runoff and hence on groundwater recharge and inflows into rivers, canals and on-farm reservoir systems at the catchment scale, we will use the Soil Water Assessment Tool (SWAT) hydrological model, appropriately calibrated and validated, and a range of GCMs projections and emissions scenarios.\\n\\nTo better understand the impacts on irrigation water requirements at the farm scale, we propose to use an existing physically-based root water uptake model that will be further calibrated and tested using data that will be collected at experimental farms in four diverse climatic zones spread over the four Indian states of Haryana, Himachal Pradesh, Uttarapradesh and Uttarakhand. For future conditions, the calibrated model will be coupled with the SWAT model. \\n\\nGiven the assessed irrigation water requirements for the future, the adequacy of existing on-farm irrigation infrastructures- direct river abstractions, reservoirs and tube wells- will be assessed in terms of reliability, vulnerability, resilience and sustainability. We propose to develop better operational practices for the infrastructures- e.g. enhanced rule curves for reservoirs, conjunctive use of reservoirs, rivers, groundwater etc- that will temper the effect of water shortages or desynchronise water availability from rainfall on crop production. The project will involve collaboration between internationally-leading scientists at 3 Indian institutions (IIT-Roorkee; NIT-Kurukshetra; NIT, Hamirpur) and 2 UK Universities: Heriot-Watt University, Edinburgh (H-WU), Cranfield University (CU). It will build on an existing and thriving collaboration between the H-WU team and the team at the IIT-Roorkee. We will work with stakeholders in India (farmers, regional and regional to national policymakers) and leading UK irrigation practitioners. The project will last for 48 months and comprise four work packages as follows: \\n\\nWP 1- Impacts and uncertainty, of climate change on water resources availability for agriculture in India: This WP aims to understand climate change impacts on water availability and to identify thresholds in catchment/aquifer-scale runoff, groundwater recharge and responses (led by CU). \\nWP 2- Impacts of shifting temporal and spatial rainfall patterns on crop-soil moisture regimes and potential irrigation demand, and associated field experimentation: The effects of the changing temperature and rainfall patterns on farm-scale soil moisture and hence future irrigation water requirements with be assessed with IIT-R physically-based root water uptake model, which will be further calibrated and validated against observed crop yields and soil moisture data from dedicated field experiments at 4 sites across 4 Indian states (Led by IIT-R).\\nWP 3- Understanding the potential of robust irrigation water management practices and systems to reduce the impacts of future water scarcity: The effectiveness of on-farm infrastructures will be assessed for their reliability, vulnerability resilience and sustainability to deliver irrigation water needs for existing and climate-change conditions. Better operational practices- irrigation water scheduling; rule and operating policies for on-farm reservoirs; conjunctive use and artificial recharge- that are robust to the uncertainties in the climate change predictions will be developed and evaluated (Led by H-WU)\\n WP 4- Stakeholder engagement and dissemination: A broad range of contextually-appropriate engagement and dissemination activities will be co-ordinated by IIT-Roorkee throughout the project duration, to maximise stakeholder buy-in (Joint UK/Indian led).","4":"B88229BD-BF44-4294-AA2A-BB328BFFDB0A"},{"1":"2420649","2":"Causal machine learning for multiple treatments and multiple outcomes in dynamic treatment regimes","3":"It has recently been shown that patients need more personalised treatments that can evolve over time depending on their response. This is formalised through a dynamic treatment regime, a sequence of decision rules over multiple time points where the next treatment is chosen based on patient history - former states, former treatments, current state. In addition, in a context where multimorbidity and polypharmacy are rising, a single medication might impact not one but several conditions, while multiple medications might simultaneously be required to address multiple conditions. However, this fact is overlooked in current clinical trials, which most often address one intervention over one disease. For example, patients with both cardiovascular disease and high blood blessure might be excluded from studies on cardiovascular disease. \\n\\nThereby, the general aim of this project is to construct reliable causal inference for multiple, parallel treatments and outcomes within each time point of a dynamic treatment regime. Particularly, this causal inference is aimed to be of use for modern longitudinal observational data, such electronic health records, that provide an alternative to sequential randomised trials.\\n\\nA key aspect of research pertaining to these aims is developing reliable dimensionality reduction techniques suited for longitudinal studies. Indeed, the dimension of the data grows linearly with the time point, as each state and treatment decision is added into the history used to decide the next treatment, and as a consequence, the number of states and treatments grows exponentially. We will explore modern machine learning techniques such as representation learning and deep generative modelling to provide such low-dimensional spaces.\\n\\nIn addition, these spaces will be usable in settings such as matching. In a static, single-intervention setting, matching is a methodology for forming similar (or &quot;balanced&quot;) treatment and control groups. Despite its ubiquitous use in the medical and social sciences, relatively little work has been done to extend it in a longitudinal setting, let alone in dynamic treatment regimes. We will aim at providing a generalisation of matching to a multi-treatment and time-varying setting thanks to a more suitable notion of balance and thanks to adequate dimensionally-reduced spaces. As a result, we will in part circumvent the challenge of regression in the dynamic treatment regime, where the non-regularity of the outcome function remains an open research problem.\\n\\nThis project falls within the EPSRC &quot;Artificial intelligence technologies&quot;, &quot;Statistics and applied probability&quot; and &quot;Healthcare technologies&quot; research area. It is co-funded by Novo Nordisk and is supervised by Professor Chris Holmes.","4":"23E1FBE7-D94F-466C-8626-3C30EB66DA18"},{"1":"2635641","2":"Causal machine learning for multiple treatments and multiple outcomes in dynamic treatment regimes","3":"It has recently been shown that patients need more personalised treatments that can evolve over time depending on their response. This is formalised through a dynamic treatment regime, a sequence of decision rules over multiple time points where the next treatment is chosen based on patient history - former states, former treatments, current state. In addition, in a context where multimorbidity and polypharmacy are rising, a single medication might impact not one but several conditions, while multiple medications might simultaneously be required to address multiple conditions. However, this fact is overlooked in current clinical trials, which most often address one intervention over one disease. For example, patients with both cardiovascular disease and high blood blessure might be excluded from studies on cardiovascular disease. \\n\\nThereby, the general aim of this project is to construct reliable causal inference for multiple, parallel treatments and outcomes within each time point of a dynamic treatment regime. Particularly, this causal inference is aimed to be of use for modern longitudinal observational data, such electronic health records, that provide an alternative to sequential randomised trials.\\n\\nA key aspect of research pertaining to these aims is developing reliable dimensionality reduction techniques suited for longitudinal studies. Indeed, the dimension of the data grows linearly with the time point, as each state and treatment decision is added into the history used to decide the next treatment, and as a consequence, the number of states and treatments grows exponentially. We will explore modern machine learning techniques such as representation learning and deep generative modelling to provide such low-dimensional spaces.\\n\\nIn addition, these spaces will be usable in settings such as matching. In a static, single-intervention setting, matching is a methodology for forming similar (or &quot;balanced&quot;) treatment and control groups. Despite its ubiquitous use in the medical and social sciences, relatively little work has been done to extend it in a longitudinal setting, let alone in dynamic treatment regimes. We will aim at providing a generalisation of matching to a multi-treatment and time-varying setting thanks to a more suitable notion of balance and thanks to adequate dimensionally-reduced spaces. As a result, we will in part circumvent the challenge of regression in the dynamic treatment regime, where the non-regularity of the outcome function remains an open research problem.\\n\\nThis project falls within the EPSRC &quot;Artificial intelligence technologies&quot;, &quot;Statistics and applied probability&quot; and &quot;Healthcare technologies&quot; research area. It is co-funded by Novo Nordisk and is supervised by Professor Chris Holmes.","4":"C4FAF548-50EB-41F8-8980-DDBA4AC15E75"},{"1":"BB/E024831/1","2":"Using translational genomics to underpin germplasm improvement for complex traits in crop legumes","3":"Legumes are a group of important plant species that, together with bacteria that live in nodules on the root, can convert nitrogen in the atmosphere to a form that can be used by plants. They include peas and beans as well as crop plants that are used for animal feed. Some legume species have been developed as 'models' that allow us to investigate genome structure, DNA sequence and the control of gene expression in a way that would be more difficult in crops. Model species typically have a small genome size, short generation times and an inbreeding system of reproduction. The barrel medic (Medicago truncatula ) has been developed as a model legume and, for example, is expected to have all its genes sequenced by the end of 2007. Information and resources from model species can be used to understand more about the genetics and genomics of crop plants in a way that will facilitate improved ways of breeding new varieties for the changing needs of agriculture. In this work we will use knowledge of Medicago truncatula to gain understanding of a closely related species, red clover. Red clover (Trifolium pratense L.) is an important crop for feeding animals (sheep, beef and dairy cattle) in the UK and many temperate parts of the world. In this work we will compare the genomes of the model and crop to lay the foundation for new approaches to breeding in the crop. We will do this in several different ways: (i) The sequences of long stretches of DNA will be compared. To do this we will use DNA that has been inserted into bacterial artificial chromosomes (BACs) in a way that allows it to be held together and suitable for sequencing. The extent of similarity in sequence between red clover and M. truncatula will tell us how closely related the two species are and the extent to which we can use information from the model e.g. to clone genes in the crop. (ii) The position of differences in DNA sequence (polymorphisms) will be mapped in the genome of red clover in such as way as to relate these differences to the physical genome as represented by the BACs (iii) A number of bio-informatic approaches will be used to extract information from DNA sequencing, physical and genetic mapping and to place the information found in the wider context of legume genetics. The bioinformatic component of the work will also facilitate the application of the knowledge gained and resources generated to the development of new varieties of red clover and other important crop species.","4":"92A8D6CA-6B2F-4863-B411-885A39CBDE20"},{"1":"BB/E024823/1","2":"Using translational genomics to underpin germplasm improvement for complex traits in crop legumes","3":"Legumes are a group of important plant species that, together with bacteria that live in nodules on the root, can convert nitrogen in the atmosphere to a form that can be used by plants. They include peas and beans as well as crop plants that are used for animal feed. Some legume species have been developed as 'models' that allow us to investigate genome structure, DNA sequence and the control of gene expression in a way that would be more difficult in crops. Model species typically have a small genome size, short generation times and an inbreeding system of reproduction. The barrel medic (Medicago truncatula ) has been developed as a model legume and, for example, is expected to have all its genes sequenced by the end of 2007. Information and resources from model species can be used to understand more about the genetics and genomics of crop plants in a way that will facilitate improved ways of breeding new varieties for the changing needs of agriculture. In this work we will use knowledge of Medicago truncatula to gain understanding of a closely related species, red clover. Red clover (Trifolium pratense L.) is an important crop for feeding animals (sheep, beef and dairy cattle) in the UK and many temperate parts of the world. In this work we will compare the genomes of the model and crop to lay the foundation for new approaches to breeding in the crop. We will do this in several different ways: (i) The sequences of long stretches of DNA will be compared. To do this we will use DNA that has been inserted into bacterial artificial chromosomes (BACs) in a way that allows it to be held together and suitable for sequencing. The extent of similarity in sequence between red clover and M. truncatula will tell us how closely related the two species are and the extent to which we can use information from the model e.g. to clone genes in the crop. (ii) The position of differences in DNA sequence (polymorphisms) will be mapped in the genome of red clover in such as way as to relate these differences to the physical genome as represented by the BACs (iii) A number of bio-informatic approaches will be used to extract information from DNA sequencing, physical and genetic mapping and to place the information found in the wider context of legume genetics. The bioinformatic component of the work will also facilitate the application of the knowledge gained and resources generated to the development of new varieties of red clover and other important crop species.","4":"22DD9CE2-E3C8-47AE-8EF5-222DD9372F1B"},{"1":"EP/E033601/1","2":"Advanced Design and Control of Active and Passive Metamaterials : from Microwaves to Optics","3":"Light usually passes through transparent materials in simple ways that science pupils learn at school. However, physicists have recently been examining the possibility of taking a normal transparent material, and inserting tiny metallic inclusions in various shapes and arrangements. As the light passes over these structures tiny currents are set up that generate electric and magnetic fields that modify the way the light travels through the material. The effects can be dramatic leading to slowing light to a few metres per second, or to bending light in very unusual ways / so-called negative refraction. These effects are potentially very useful in all kinds of ways that are only beginning to be understood. Lenses that break traditional diffraction limits are one possibility. An 'invisibility cloak' another. At the more mundane level, the wavelengths that we will be focussing upon (mm) are expected to enhance the performance of cellular telephone networks, designed to handle large area, mobile applications personal communication services/networks, global positioning systems, broadcast satellite television, satellite phone services and automotive electronics. Waves in this part of the spectrum have a fabulous information capacity. The highly directive nature of mm-wave beams, the predicted small size and lightweight of the hardware are also great advantages. The main desire is to unify more than one function in an application (e.g. antenna/filter/generator or amplifier). The progress towards higher frequencies, coupled to miniaturization, increases the bandwidth and information capacity. The metallic inclusions are much smaller than the wavelength of light so that as far as the light is concerned the material with inclusions behaves as a uniform effective medium, or a meta-material. The research is therefore a concerted theoretical platform activity aimed at creating the best design and understanding of metamaterials obtained so far. We will be focussing on design solutions that will address some of the known problems associated with metamaterials as well as others not known at this stage. Loss is a major issue that we will address through the design of metallic inclusions that are actively controlled by applying external currents. Salford will address loss control and elimination by using active diode additions to traditional ring and omega particle structures. Chiral, or handed, inclusions will provide smart artificial molecules that are expected to have completely new properties. These expectations will require new and deeper insights into the equations that describe how electromagnetic fields interact with matter, and so will be extended to embrace the ideas of nonlinearity (i.e. output is not proportional to input) and nolocality (memory). Surrey will adopt a specific modelling approach that will bridge the gap between methods used at the microscopic nanomaterials level and techniques previously used for macroscopic multilayered structures. They will examine ordered/disordered arrays of inclusions as conceived by Salford. Surrey will pursue new ideas of using metamaterials to slow down light that rely on geometry rather than resonance effects. Imperial will address fundamental aspects related to plasmonics. Issues of definition also need to be addressed so that our code will be written with the utmost robustness and reliability.","4":"2A662187-F0F5-4E85-98A9-5A2C6DB7C9B1"},{"1":"EP/E031463/1","2":"Advanced Design and Control of Active and Passive Metamaterials : from Microwaves to Optics","3":"Light usually passes through transparent materials in simple ways that science pupils learn at school. However, physicists have recently been examining the possibility of taking a normal transparent material, and inserting tiny metallic inclusions in various shapes and arrangements. As the light passes over these structures tiny currents are set up that generate electric and magnetic fields that modify the way the light travels through the material. The effects can be dramatic leading to slowing light to a few metres per second, or to bending light in very unusual ways / so-called negative refraction. These effects are potentially very useful in all kinds of ways that are only beginning to be understood. Lenses that break traditional diffraction limits are one possibility. An 'invisibility cloak' another. At the more mundane level, the wavelengths that we will be focussing upon (mm) are expected to enhance the performance of cellular telephone networks, designed to handle large area, mobile applications personal communication services/networks, global positioning systems, broadcast satellite television, satellite phone services and automotive electronics. Waves in this part of the spectrum have a fabulous information capacity. The highly directive nature of mm-wave beams, the predicted small size and lightweight of the hardware are also great advantages. The main desire is to unify more than one function in an application (e.g. antenna/filter/generator or amplifier). The progress towards higher frequencies, coupled to miniaturization, increases the bandwidth and information capacity. The metallic inclusions are much smaller than the wavelength of light so that as far as the light is concerned the material with inclusions behaves as a uniform effective medium, or a meta-material. The research is therefore a concerted theoretical platform activity aimed at creating the best design and understanding of metamaterials obtained so far. We will be focussing on design solutions that will address some of the known problems associated with metamaterials as well as others not known at this stage. Loss is a major issue that we will address through the design of metallic inclusions that are actively controlled by applying external currents. Salford will address loss control and elimination by using active diode additions to traditional ring and omega particle structures. Chiral, or handed, inclusions will provide smart artificial molecules that are expected to have completely new properties. These expectations will require new and deeper insights into the equations that describe how electromagnetic fields interact with matter, and so will be extended to embrace the ideas of nonlinearity (i.e. output is not proportional to input) and nolocality (memory). Surrey will adopt a specific modelling approach that will bridge the gap between methods used at the microscopic nanomaterials level and techniques previously used for macroscopic multilayered structures. They will examine ordered/disordered arrays of inclusions as conceived by Salford. Surrey will pursue new ideas of using metamaterials to slow down light that rely on geometry rather than resonance effects. Imperial will address fundamental aspects related to plasmonics. Issues of definition also need to be addressed so that our code will be written with the utmost robustness and reliability.","4":"AFDE9F38-2397-47EE-A376-64818B3F28B7"},{"1":"10032974","2":"ulTRafast hOlograPHic FTIR microscopY (TROPHY)","3":"Many human pathologies such as cancer are due to complex biochemical alterations that start at a sub-cellular level and lead to progressive changes that result in a heterogeneous tumor composition. The polyclonality of tumor cells hampers the diagnosis and the therapy giving rise to tumor clones that lead to therapy resistance and promote metastases. An accurate diagnosis of tumor biopsies to identify these particular cell clones is crucial to provide targeted therapy tailored to the tumor characteristics, to improve the patient outcomes and increase survival rates. For this vision to come true, we introduce ulTRafast hOlograPHic FT-IR microscopY (TROPHY) as a paradigm shift in vibrational microscopy, blending elements of photo-thermal infrared (PT-IR), Fourier transform (FT)-IR, and Digital Holography Microscopy (DHM). TROPHY brings these techniques to the unprecedented ultrafast timescale, where the refractive index change induced by coherent IR vibrations is probed at its peak value before thermal relaxation. TROPHY borrows from PT-IR the combination of IR vibrational excitation with visible probing for high spatial resolution, from FT-IR the use of time-domain interferometry to obtain a high spectral resolution from broadband excitation, from DHM highly sensitive and quantitative detection of the refractive index (phase) change. Combined with artificial intelligence algorithms, this technology will enable quantitative concentration imaging of molecular biomarkers with high spatial resolution, high chemical selectivity and high speed, with a transformative impact on medical research and clinics. In oncology, it will be applied to intraoperative diagnosis of tumor biopsies, providing tumor grading, staging and subtyping, and supporting complete tumor resection. It will also allow to determine the best therapeutic approach tailored to the patient and identify resistant tumor clones under targeted therapy, paving the way for precision medicine in cancer.","4":"AB553A05-3E4A-4C20-9E28-96E77CD6395D"},{"1":"10032224","2":"TROPHY: ulTRafast hOlograPHic FTIR microscopY","3":"Many human pathologies such as cancer are due to complex biochemical alterations that start at a sub-cellular level and lead to progressive changes that result in a heterogeneous tumor composition. The polyclonality of tumor cells hampers the diagnosis and the therapy giving rise to tumor clones that lead to therapy resistance and promote metastases. An accurate diagnosis of tumor biopsies to identify these particular cell clones is crucial to provide targeted therapy tailored to the tumor characteristics, to improve the patient outcomes and increase survival rates. For this vision to come true, we introduce ulTRafast hOlograPHic FT-IR microscopY (TROPHY) as a paradigm shift in vibrational microscopy, blending elements of photo-thermal infrared (PT-IR), Fourier transform (FT)-IR, and Digital Holography Microscopy (DHM). TROPHY brings these techniques to the unprecedented ultrafast timescale, where the refractive index change induced by coherent IR vibrations is probed at its peak value before thermal relaxation. TROPHY borrows from PT-IR the combination of IR vibrational excitation with visible probing for high spatial resolution, from FT-IR the use of time-domain interferometry to obtain a high spectral resolution from broadband excitation, from DHM highly sensitive and quantitative detection of the refractive index (phase) change. Combined with artificial intelligence algorithms, this technology will enable quantitative concentration imaging of molecular biomarkers with high spatial resolution, high chemical selectivity and high speed, with a transformative impact on medical research and clinics. In oncology, it will be applied to intraoperative diagnosis of tumor biopsies, providing tumor grading, staging and subtyping, and supporting complete tumor resection. It will also allow to determine the best therapeutic approach tailored to the patient and identify resistant tumor clones under targeted therapy, paving the way for precision medicine in cancer.","4":"FD5A4ACA-59D8-483E-A6F1-4575CF9D1431"},{"1":"NE/K011626/1","2":"Multiscale Impacts of Cyanobacterial Crusts on Landscape Stability","3":"Most soils are a mixture of inorganic (mineral) and organic (e.g. plant) material. In deserts, often there is not enough rain for large plants to grow, but organisms such as algae and lichens can survive. Our research focuses on cyanobacteria which live on or near the soil surface and produce sugars as they grow, called polysaccharides, which can stick small particles (e.g. sand grains), together. This binds the soil forming a 'cyanobacterial soil crust' that is helpful in the landscape because it makes it harder for soil erosion to take place. Cyanobacterial soil crusts occur naturally, but can also be made artificially as part of a land management plan. In deserts often soil erosion is caused by wind, however sometimes it rains causing water erosion, and the amount and intensity of rainfall affects crusts. Light rain causes cyanobacterial growth and helps to thicken and strengthen the crusts, but heavy rain can break up crusts, making them less able to protect soil from erosion. We know little about the relationship between different rainfall intensities and the ability of crusts to protect soil from wind and water erosion but it would be useful to do so because we could better plan activities such as where cattle graze (hooves break up weak crusts) and when to leave fields bare.\\n\\nThis project is exciting because it studies the impact of rainfall, runoff (surface flow) and wind erosion on cyanobacterial crusts at different scales. To begin, we control conditions by growing crusts on an artificial soil bed under a rainfall simulator that lets us choose how much rainfall occurs, and how long it lasts. We also choose whether the soil bed is flat or sloping, and can control runoff rate. After the simulated rainfall, we will use a wind tunnel on top of the soil bed to simulate wind erosion - again we choose the wind speed and can measure how much soil is blown away. By doing this, we can test the response of cyanobacterial soil crust to different rainfall events (does the crust get thicker? is it broken up and washed away?) and we can measure how good the crust is at preventing wind erosion. Our approach is unusual because it looks at how one set of processes (rainfall and runoff) affects a second process (wind erosion). From this we will develop a model to explain and predict the impact of rainfall/runoff on soil crust growth and susceptibility to wind erosion. The model will then be tested in the field using natural soils and cyanobacterial crusts. To guarantee a range of rainfall and wind events, the field tests will be partially controlled using a portable field rainfall simulator and wind tunnel.\\n\\nFinally, the controlled experiments are conducted at a small scale but we will broaden the spatial and temporal scope of the project to the regional scale which is more applicable to understanding landscape stability. We will do this using remote sensing because aspects of cyanobacterial crust growth and development can be detected using satellite data. We will use these data to examine cyanobacterial crust response to rainfall and runoff (both also detectable from space) at the regional scale and monitor the time-lag between these hydrological inputs and dust storms to further test the model at larger spatial and temporal scales.\\n\\nCyanobacteria occur in many environments, e.g. the protective crusts they form are important in temperate climates where they protect soil between crops from water erosion, and they form on coastal dunes, where they reduce sand transport by wind. Having developed and calibrated a model for predicting the impact of water on the protective role of crusts in drylands, we can test the model on other soils and under alternative rainfall patterns (e.g. temperate or tropical). Scientists have predicted that the amount and intensity of rainfall in many areas will change in the future; it will also be possible to use our model to try and predict how this will affect cyanobacterial crusts.","4":"BB06C32C-F6F0-4407-A89C-C15E94A4B7FD"},{"1":"NE/K011464/1","2":"Multiscale Impacts of Cyanobacterial Crusts on Landscape Stability","3":"Most soils are a mixture of inorganic (mineral) and organic (e.g. plant) material. In deserts, often there is not enough rain for large plants to grow, but organisms such as algae and lichens can survive. Our research focuses on cyanobacteria which live on or near the soil surface and produce sugars as they grow, called polysaccharides, which can stick small particles (e.g. sand grains), together. This binds the soil forming a 'cyanobacterial soil crust' that is helpful in the landscape because it makes it harder for soil erosion to take place. Cyanobacterial soil crusts occur naturally, but can also be made artificially as part of a land management plan. In deserts often soil erosion is caused by wind, however sometimes it rains causing water erosion, and the amount and intensity of rainfall affects crusts. Light rain causes cyanobacterial growth and helps to thicken and strengthen the crusts, but heavy rain can break up crusts, making them less able to protect soil from erosion. We know little about the relationship between different rainfall intensities and the ability of crusts to protect soil from wind and water erosion but it would be useful to do so because we could better plan activities such as where cattle graze (hooves break up weak crusts) and when to leave fields bare.\\n\\nThis project is exciting because it studies the impact of rainfall, runoff (surface flow) and wind erosion on cyanobacterial crusts at different scales. To begin, we control conditions by growing crusts on an artificial soil bed under a rainfall simulator that lets us choose how much rainfall occurs, and how long it lasts. We also choose whether the soil bed is flat or sloping, and can control runoff rate. After the simulated rainfall, we will use a wind tunnel on top of the soil bed to simulate wind erosion - again we choose the wind speed and can measure how much soil is blown away. By doing this, we can test the response of cyanobacterial soil crust to different rainfall events (does the crust get thicker? is it broken up and washed away?) and we can measure how good the crust is at preventing wind erosion. Our approach is unusual because it looks at how one set of processes (rainfall and runoff) affects a second process (wind erosion). From this we will develop a model to explain and predict the impact of rainfall/runoff on soil crust growth and susceptibility to wind erosion. The model will then be tested in the field using natural soils and cyanobacterial crusts. To guarantee a range of rainfall and wind events, the field tests will be partially controlled using a portable field rainfall simulator and wind tunnel.\\n\\nFinally, the controlled experiments are conducted at a small scale but we will broaden the spatial and temporal scope of the project to the regional scale which is more applicable to understanding landscape stability. We will do this using remote sensing because aspects of cyanobacterial crust growth and development can be detected using satellite data. We will use these data to examine cyanobacterial crust response to rainfall and runoff (both also detectable from space) at the regional scale and monitor the time-lag between these hydrological inputs and dust storms to further test the model at larger spatial and temporal scales.\\n\\nCyanobacteria occur in many environments, e.g. the protective crusts they form are important in temperate climates where they protect soil between crops from water erosion, and they form on coastal dunes, where they reduce sand transport by wind. Having developed and calibrated a model for predicting the impact of water on the protective role of crusts in drylands, we can test the model on other soils and under alternative rainfall patterns (e.g. temperate or tropical). Scientists have predicted that the amount and intensity of rainfall in many areas will change in the future; it will also be possible to use our model to try and predict how this will affect cyanobacterial crusts.","4":"0DA35B06-5E8F-447C-9E11-BE9B2275F625"},{"1":"EP/E035248/1","2":"Computational Logic of Euclidean Spaces","3":"Much of the spatial information we encounter in everyday situations isqualitative, rather than quantitative, in character. Thus, forinstance, we may know which of two objects is the closer withoutmeasuring their distances; we may perceive an object to be convexwithout being able to describe its precise shape; or we may identifytwo areas on a map as sharing a boundary without knowing the equationthat describes it. This observation has prompted the development,within Artificial Intelligence, of various formalisms for reasoningwith qualitative spatial information.Although substantial progress has been made in analysing themathematical foundations and computational characteristics of suchformalisms, most of that progress has centred on systems for reasoningabout highly abstract problems concerning (typically) arbitraryregions in very general classes of topological spaces. But of course,the geometrical entities of interest for practical problems are notarbitrary subsets of general topological spaces, but rathermathematically very well-behaved regions of 2 and 3-dimensionalEuclidean space; moreover, the geometrical properties and relationsthese problems are concerned with are typically not merely topological, butrather affine or even metric in character. Together, these factorsseverly limit the practical usefulness of current qualitative spatialreasoning formalisms. Overcoming this limitation represents anexciting mathematical and computational challenge.We propose to meet this challenge by drawing on developments inmathematical logic, geometrical topology, and algebraic geometry thatthe spatial reasoning literature in AI has so far failed fully toexploit. Specifically, we shall investigate the computationalproperties of spatial and spatio-temporal logics for reasoning aboutmathematically well-behaved regions of 2- and 3-dimensional Euclideanspace. We shall develop and implement algorithms for reasoning with these logics. This investigation will illuminate the important relationships betweenhitherto separate research traditions, provide new techniques foraddressing challenging problems in the mathematical geometry, andyield logics of direct relevance to practical spatial reasoningproblems.","4":"82AB0846-EDD3-46F1-8BE2-33C802272F22"},{"1":"EP/E034942/1","2":"Computational Logic of Euclidean Spaces","3":"Much of the spatial information we encounter in everyday situations isqualitative, rather than quantitative, in character. Thus, forinstance, we may know which of two objects is the closer withoutmeasuring their distances; we may perceive an object to be convexwithout being able to describe its precise shape; or we may identifytwo areas on a map as sharing a boundary without knowing the equationthat describes it. This observation has prompted the development,within Artificial Intelligence, of various formalisms for reasoningwith qualitative spatial information.Although substantial progress has been made in analysing themathematical foundations and computational characteristics of suchformalisms, most of that progress has centred on systems for reasoningabout highly abstract problems concerning (typically) arbitraryregions in very general classes of topological spaces. But of course,the geometrical entities of interest for practical problems are notarbitrary subsets of general topological spaces, but rathermathematically very well-behaved regions of 2 and 3-dimensionalEuclidean space; moreover, the geometrical properties and relationsthese problems are concerned with are typically not merely topological, butrather affine or even metric in character. Together, these factorsseverly limit the practical usefulness of current qualitative spatialreasoning formalisms. Overcoming this limitation represents anexciting mathematical and computational challenge.We propose to meet this challenge by drawing on developments inmathematical logic, geometrical topology, and algebraic geometry thatthe spatial reasoning literature in AI has so far failed fully toexploit. Specifically, we shall investigate the computationalproperties of spatial and spatio-temporal logics for reasoning aboutmathematically well-behaved regions of 2- and 3-dimensional Euclideanspace. We shall develop and implement algorithms for reasoning with these logics. This investigation will illuminate the important relationships betweenhitherto separate research traditions, provide new techniques foraddressing challenging problems in the mathematical geometry, andyield logics of direct relevance to practical spatial reasoningproblems.","4":"596517D4-61B1-48E2-B30A-836F7F658E4D"},{"1":"2326972","2":"Containerised NFV orchestration and management","3":"Network Function Virtualization (NFV) has been gaining traction as a way to manage computer networks, and to enable fast and flexible service deployment on top of physical connectivity. Lightweight virtualization approaches such as, e.g., software containers, are also being increasingly explored as a platform for efficient orchestration and management of virtualised Network Functions at scale.\\n\\nCurrent networks operations and management (O&amp;M) solutions assume software containers are deployed within data centres with single geographic location, high speed communications and high performance infrastructure. However, many network benefits can be obtained if container network functions can be distributed around the network edge.\\n\\nThis PhD research will address the challenge of how to devise network-wide platforms that can manage and interconnect Network Functions for many customers at scale and over the Internet. We have been using platforms like Openstack as well as our own Glasgow Network Function (GNF) orchestrator, and we will further investigate how such platforms can be designed to work at scale (e.g., billions of small IoT devices) and over heterogeneous environments where network resource utilisation becomes as significant as CPU and memory resource use.\\n\\nThere is a rich bibliography regarding the mathematical interpretation of orchestrating services and managing resources. An adaptation of this work to the needs of modern networking systems can lead to significant results. The evaluation of this adaptation will be done through simulation and also through deployment on actual testbed systems.\\n\\nIn NFV, the software is decoupled from hardware allowing the migration of services according to resource availability or specific resource constraints. This enables an efficient management of resources and reduces the Operational Expenditure. This research will extend the work of Open MANO and working on the evolution of ICT networks and distributed systems towards the most energy efficient way of operation. The means to achieve that is by making use of applied probabilities and statistics. By developing an intelligent way of utilizing network resources and systems, we aim to contribute towards the Connected Nation prosperity outcome.\\n\\nNetwork functions can be used for accurate monitoring of network parameters through analysing real-time traffic or hardware characteristics like utilization, temperature, etc. By using statistical models, malicious traffic or hardware changes can be detected, and a potential attack can be promptly identified. This direction of research helps establish a safe cyber society and is well aligned with EPSRC's Resilient Nation prosperity outcome.\\n\\nIn addition, virtualised computing infrastructure operating at the Edge of the network are being investigated for the development of smart-home and assisted leaving technologies such as health and safety monitoring, environmental and personal condition monitoring and alerting, etc. Therefore, the infrastructure developed within this project will also be contributing towards the Healthy Nation prosperity outcome.\\n\\nOverall, the goal of this project is to advance networking communications by making intelligence an integral part of NFV management and orchestration. The research will be conducted in close collaboration with BT who sponsor this PhD through an iCASE award, and want to explore lightweight virtulisation technologies for the efficient management of customer premises equipment.","4":"40F9C94C-CEF0-44FF-A061-A99B2D06CC75"},{"1":"2777852","2":"Containerised NFV orchestration and management","3":"Network Function Virtualization (NFV) has been gaining traction as a way to manage computer networks, and to enable fast and flexible service deployment on top of physical connectivity. Lightweight virtualization approaches such as, e.g., software containers, are also being increasingly explored as a platform for efficient orchestration and management of virtualised Network Functions at scale.\\n\\nCurrent networks operations and management (O&amp;M) solutions assume software containers are deployed within data centres with single geographic location, high speed communications and high performance infrastructure. However, many network benefits can be obtained if container network functions can be distributed around the network edge.\\n\\nThis PhD research will address the challenge of how to devise network-wide platforms that can manage and interconnect Network Functions for many customers at scale and over the Internet. We have been using platforms like Openstack as well as our own Glasgow Network Function (GNF) orchestrator, and we will further investigate how such platforms can be designed to work at scale (e.g., billions of small IoT devices) and over heterogeneous environments where network resource utilisation becomes as significant as CPU and memory resource use.\\n\\nThere is a rich bibliography regarding the mathematical interpretation of orchestrating services and managing resources. An adaptation of this work to the needs of modern networking systems can lead to significant results. The evaluation of this adaptation will be done through simulation and also through deployment on actual testbed systems.\\n\\nIn NFV, the software is decoupled from hardware allowing the migration of services according to resource availability or specific resource constraints. This enables an efficient management of resources and reduces the Operational Expenditure. This research will extend the work of Open MANO and working on the evolution of ICT networks and distributed systems towards the most energy efficient way of operation. The means to achieve that is by making use of applied probabilities and statistics. By developing an intelligent way of utilizing network resources and systems, we aim to contribute towards the Connected Nation prosperity outcome.\\n\\nNetwork functions can be used for accurate monitoring of network parameters through analysing real-time traffic or hardware characteristics like utilization, temperature, etc. By using statistical models, malicious traffic or hardware changes can be detected, and a potential attack can be promptly identified. This direction of research helps establish a safe cyber society and is well aligned with EPSRC's Resilient Nation prosperity outcome.\\n\\nIn addition, virtualised computing infrastructure operating at the Edge of the network are being investigated for the development of smart-home and assisted leaving technologies such as health and safety monitoring, environmental and personal condition monitoring and alerting, etc. Therefore, the infrastructure developed within this project will also be contributing towards the Healthy Nation prosperity outcome.\\n\\nOverall, the goal of this project is to advance networking communications by making intelligence an integral part of NFV management and orchestration. The research will be conducted in close collaboration with BT who sponsor this PhD through an iCASE award, and want to explore lightweight virtulisation technologies for the efficient management of customer premises equipment.","4":"3FD95E8C-CCA2-47E5-83B8-0A5DAA383AEB"},{"1":"EP/F038518/2","2":"Application of intelligent imaging sensors to image guided and intensity modulated radiotherapy","3":"Novel active pixel sensors have been developed under the Multidisciplinary Integrated Intelligent Imaging (Mi3) Project (funded by a Joint Research Council Basic Technology Award), which Sheffield leads and Institute of Cancer Research (ICR) are partners. This will lead to the realisation of radiation sensors that have increased functionality and integrated intelligence for use in a range of imaging applications within the field of medical imaging. This proposed research aims to exploit these latest developments in imaging and detector technology for application to two challenges in image guided radiotherapy (IGRT) and intensity modulated radiotherapy (IMRT), which will help the optimisation of conformality of dose delivery in cancer treatment with radiotherapy. The first challenge is to improve dose delivery accuracy to moving tumours in sites such as the lung. On of the most promising techniques for dealing with motion during treatment employs IGRT with X-ray fluoroscopy. Using two dedicated fluoroscopy units, tumour motion has been imaged continuously with the aid of X-ray radio-opaque markers. However, serious problems with this system exist: there is a significant latency period between the time at which the tumour is imaged and the identification of the tumour position and the cost of using such systems in terms of unwanted dose is non-trivial. If these systems are to be realised lower radiation intensity must be used which inevitably leads to poorer image quality and lower tracking accuracy. Lower tracking accuracy will lead to a greater number of errors or impractical treatment times. The second challenge is to optimise treatment verification, which due to the complex nature of IMRT delivery, is a time-consuming and therefore resource-heavy process. Groups, including the ICR Group, have pioneered the use of electronic portal imaging devices for IMRT verification. Imaging solutions for the verification of the position of the radiation beam during dynamic beam delivery with respect to the treatment plan have been presented, however, fully automated verification has yet to be implemented in day-to-day clinical practice.It is against this background that the project will address the question: given the image quality limitations of current portal imaging devices and the high dose rates imparted by fluoroscopic systems, is it feasible to increase tracking accuracy of intra-fractional tumour motion and the efficiency of IMRT verification using intelligent pixels sensors? Intelligent sensors will be built that have novel functionality that will potentially be useful for speeding up IMRT verification and optimising the tracking process. For example, these sensors will have region of interest read out and self-triggered detector read out that have advantages for increased speed of acquisition and data reduction. These functions can be used to make better use of the radiation available allowing us to acquire the minimum amount of data required to make decisions regarding tumour marker position or field leaf position for IMRT verification. A prototype imaging system will be designed, constructed and tested in order to investigate data acquisition methods that can be implemented with active pixel sensors to fully optimise image acquisition and hence make portal/fluoroscopic imaging a viable solution for IGRT and efficient verification. Optimisation of tracking methods through the use of intelligent sensors will be carried out firstly through the development of tracking algorithms that can be implemented within the image sensor, through simulation of this hard ware processes and through testing using the novel sensors and field programmable gate arrays. The final stage of this project will include the development of a concept demonstrator for intelligent sensor based radiotherapy imaging.","4":"29328345-8F4A-46EE-8083-8A949C436C31"},{"1":"EP/F035985/1","2":"Application of intelligent imaging sensors to image guided and intensity modulated radiotherapy","3":"Novel active pixel sensors have been developed under the Multidisciplinary Integrated Intelligent Imaging (Mi3) Project (funded by a Joint Research Council Basic Technology Award), which Sheffield leads and Institute of Cancer Research (ICR) are partners. This will lead to the realisation of radiation sensors that have increased functionality and integrated intelligence for use in a range of imaging applications within the field of medical imaging. This proposed research aims to exploit these latest developments in imaging and detector technology for application to two challenges in image guided radiotherapy (IGRT) and intensity modulated radiotherapy (IMRT), which will help the optimisation of conformality of dose delivery in cancer treatment with radiotherapy. The first challenge is to improve dose delivery accuracy to moving tumours in sites such as the lung. On of the most promising techniques for dealing with motion during treatment employs IGRT with X-ray fluoroscopy. Using two dedicated fluoroscopy units, tumour motion has been imaged continuously with the aid of X-ray radio-opaque markers. However, serious problems with this system exist: there is a significant latency period between the time at which the tumour is imaged and the identification of the tumour position and the cost of using such systems in terms of unwanted dose is non-trivial. If these systems are to be realised lower radiation intensity must be used which inevitably leads to poorer image quality and lower tracking accuracy. Lower tracking accuracy will lead to a greater number of errors or impractical treatment times. The second challenge is to optimise treatment verification, which due to the complex nature of IMRT delivery, is a time-consuming and therefore resource-heavy process. Groups, including the ICR Group, have pioneered the use of electronic portal imaging devices for IMRT verification. Imaging solutions for the verification of the position of the radiation beam during dynamic beam delivery with respect to the treatment plan have been presented, however, fully automated verification has yet to be implemented in day-to-day clinical practice.It is against this background that the project will address the question: given the image quality limitations of current portal imaging devices and the high dose rates imparted by fluoroscopic systems, is it feasible to increase tracking accuracy of intra-fractional tumour motion and the efficiency of IMRT verification using intelligent pixels sensors? Intelligent sensors will be built that have novel functionality that will potentially be useful for speeding up IMRT verification and optimising the tracking process. For example, these sensors will have region of interest read out and self-triggered detector read out that have advantages for increased speed of acquisition and data reduction. These functions can be used to make better use of the radiation available allowing us to acquire the minimum amount of data required to make decisions regarding tumour marker position or field leaf position for IMRT verification. A prototype imaging system will be designed, constructed and tested in order to investigate data acquisition methods that can be implemented with active pixel sensors to fully optimise image acquisition and hence make portal/fluoroscopic imaging a viable solution for IGRT and efficient verification. Optimisation of tracking methods through the use of intelligent sensors will be carried out firstly through the development of tracking algorithms that can be implemented within the image sensor, through simulation of this hard ware processes and through testing using the novel sensors and field programmable gate arrays. The final stage of this project will include the development of a concept demonstrator for intelligent sensor based radiotherapy imaging.","4":"4C3FE563-5A0A-4E3A-95B2-48B6FADB41EC"},{"1":"971560","2":"Nu-Decom","3":"Nuvia Limited, a UK based, international nuclear engineering, project management and services contractor has teamed up with a number of the UK's top innovative technology providers in response to Innovate UK's SBRI IIND funding competition. The title of the project is &quot;Nu-Decom&quot; and that reflects the aim of the project which is to demonstrate a range of new decommissioning technologies that will deliver the safer, cheaper and quicker decommissioning of the UK's nuclear legacy. Nuvia have assembled a comprehensive team suited for the complex and wide ranging challenges associated with the decommissioning of the active process plants on the Sellafield site. The team includes expertise from both industry and academia, and the fields of expertise include: (1) Teleoperable systems, advanced robotics &amp; semi-autonomous systems for challenging environments. (2) Remote radiometrics and geometric characterisation. (3) The post processing of geometric data into 3D modelling and visualisation software packages. (4) The use of games technology, virtual (VR) and augmented reality (AR) technologies to provide immersive environments for training and communications purposes. (5) The application of Artificial Intelligence (AI) to provide a knowledge base and expert systems that can be used to support and guide remote operations across the UK's decommissioning program. (6) Application of Human Factors principles. These innovative new technologies and techniques will be integrated with Nuvia's own tried and tested nuclear decommissioning expertise and knowhow which has been successfully deployed throughout the UK and on the global decommissioning market thus providing an established route to a global market for UK Technologies. The Project Team's industrial partners include:- Nuvia (Project Lead), Clicks &amp; Links (C&amp;L), Hu-Tech, MOOG, PaR Systems, Tacit-Connections (TC), UKAEA RACE and Imitec. The Project Team's academic partners include: University of Manchester (UoM) and the University of Bristol (UoB). As part of Phase 1 the project team will assess each step in a typical decommissioning process including:- initial characterisation, decontamination, the design &amp; manufacture process, operator training, HAZOP and safety case processes, site preparation, installation &amp; commissioning, Safe Systems of Work (SSoW), the Human Machine Interface (HMI), remote operations, size reduction, consumable management, materials handling, routine maintenance, remote intervention &amp; repair, waste export routes, ex-situ size reduction, assay monitoring, waste sorting, waste sentencing &amp; segregation, waste packaging, waste container handling and finally export from the facility. Phase 1 will investigate the feasibility of applying the range of innovative technology platforms to each of the above steps and rank and rate each of the applications and potential benefits against the required criteria i.e. improved productivity, minimising human intervention, transferability, scalability and optimising waste treatment packing and routing into the most cost efficient waste streams. The project will also consider the Technology Readiness Levels (TRL) and the cost to develop technologies to a level suitable for (a) Inactive Demonstration (Phase 2) and (b) active demonstration. The project will prioritise and make recommendations for which platforms go forward to the Phase 2 inactive demonstration.","4":"E0F4A3CF-51E2-43A7-8E23-51C75FD48802"},{"1":"972233","2":"Nu-Decom","3":"Nuvia Limited, a UK based, international nuclear engineering, project management and services contractor has teamed up with a number of the UK's top innovative technology providers in response to Innovate UK's SBRI IIND funding competition. The title of the project is &quot;Nu-Decom&quot; and that reflects the aim of the project which is to demonstrate a range of new decommissioning technologies that will deliver the safer, cheaper and quicker decommissioning of the UK's nuclear legacy. Nuvia have assembled a comprehensive team suited for the complex and wide ranging challenges associated with the decommissioning of the active process plants on the Sellafield site. The team includes expertise from both industry and academia, and the fields of expertise include: (1) Teleoperable systems, advanced robotics &amp; semi-autonomous systems for challenging environments. (2) Remote radiometrics and geometric characterisation. (3) The post processing of geometric data into 3D modelling and visualisation software packages. (4) The use of games technology, virtual (VR) and augmented reality (AR) technologies to provide immersive environments for training and communications purposes. (5) The application of Artificial Intelligence (AI) to provide a knowledge base and expert systems that can be used to support and guide remote operations across the UK's decommissioning program. (6) Application of Human Factors principles. These innovative new technologies and techniques will be integrated with Nuvia's own tried and tested nuclear decommissioning expertise and knowhow which has been successfully deployed throughout the UK and on the global decommissioning market thus providing an established route to a global market for UK Technologies. The Project Team's industrial partners include:- Nuvia (Project Lead), Clicks &amp; Links (C&amp;L), Hu-Tech, MOOG, PaR Systems, Tacit-Connections (TC), UKAEA RACE and Imitec. The Project Team's academic partners include: University of Manchester (UoM) and the University of Bristol (UoB). As part of Phase 1 the project team will assess each step in a typical decommissioning process including:- initial characterisation, decontamination, the design &amp; manufacture process, operator training, HAZOP and safety case processes, site preparation, installation &amp; commissioning, Safe Systems of Work (SSoW), the Human Machine Interface (HMI), remote operations, size reduction, consumable management, materials handling, routine maintenance, remote intervention &amp; repair, waste export routes, ex-situ size reduction, assay monitoring, waste sorting, waste sentencing &amp; segregation, waste packaging, waste container handling and finally export from the facility. Phase 1 will investigate the feasibility of applying the range of innovative technology platforms to each of the above steps and rank and rate each of the applications and potential benefits against the required criteria i.e. improved productivity, minimising human intervention, transferability, scalability and optimising waste treatment packing and routing into the most cost efficient waste streams. The project will also consider the Technology Readiness Levels (TRL) and the cost to develop technologies to a level suitable for (a) Inactive Demonstration (Phase 2) and (b) active demonstration. The project will prioritise and make recommendations for which platforms go forward to the Phase 2 inactive demonstration.","4":"D63F003F-227A-4932-A276-4DEEFA287F98"},{"1":"10046874","2":"OBAMA-NEXT - Project Number 101081642 - Horizon-CL6-2022-BIODIV-01","3":"OBAMA-NEXT aims to develop a toolbox for generating accurate, precise and relevant information characterising marine ecosystems and their biodiversity. This will be achieved by integrating new/emerging technologies, including remote sensing, eDNA, optical instruments and citizen science, with existing marine monitoring techniques for improving our capacity to describe ecosystem function and biodiversity with higher spatial and temporal resolution. The project will contribute to shaping next generation monitoring programs and defining Essential Ocean/Biodiversity Variables(EOVs/EBVs). Stakeholders will be involved from the onset of the project to identify products needed in an iterative co-creating and specification process. These specifications will guide the application of algorithms, including advanced statistical analyses and artificial intelligence, which combine and translate different data sources into information products (i.e., maps and indicators) to fulfil stakeholders’ needs. Routines for visualisation and methods for uncertainty assessment are also important components of the toolbox development. The toolbox will be evaluated and improved based on 12 selected Learning Sites (LS), representing diverse ecosystems and data sources within the four European regional seas. The applicability of the information products, compiled with the toolbox on LS data, will be evaluated as input to models for predicting biodiversity and as support for environmental and biodiversity policies. The project will also assess the usefulness of the products with respect to the EU objective of designating an ecologically coherent MPA network and the applicability of C-burial ratesin angiosperm habitatsfor carbon offsetting and Nationally Determined Contributions. OBAMA-NEXT will strengthen Europe’s capability in acquiring and utilising biological ocean observations for better management of marine resources through strong public outreach and active stakeholder communication.","4":"013399DF-84C5-48F0-A516-F3CB7A3E6BE0"},{"1":"10061203","2":"OBSERVING AND MAPPING MARINE ECOSYSTEMS – NEXT GENERATION TOOLS (OBAMA-NEXT)","3":"OBAMA-NEXT aims to develop a toolbox for generating accurate, precise and relevant information characterising marine ecosystems and their biodiversity. This will be achieved by integrating new/emerging technologies, including remote sensing, eDNA, optical instruments and citizen science, with existing marine monitoring techniques for improving our capacity to describe ecosystem function and biodiversity with higher spatial and temporal resolution. The project will contribute to shaping next generation monitoring programs and defining Essential Ocean/Biodiversity Variables(EOVs/EBVs). Stakeholders will be involved from the onset of the project to identify products needed in an iterative co-creating and specification process. These specifications will guide the application of algorithms, including advanced statistical analyses and artificial intelligence, which combine and translate different data sources into information products (i.e., maps and indicators) to fulfil stakeholders’ needs. Routines for visualisation and methods for uncertainty assessment are also important components of the toolbox development. The toolbox will be evaluated and improved based on 12 selected Learning Sites (LS), representing diverse ecosystems and data sources within the four European regional seas. The applicability of the information products, compiled with the toolbox on LS data, will be evaluated as input to models for predicting biodiversity and as support for environmental and biodiversity policies. The project will also assess the usefulness of the products with respect to the EU objective of designating an ecologically coherent MPA network and the applicability of C-burial ratesin angiosperm habitatsfor carbon offsetting and Nationally Determined Contributions. OBAMA-NEXT will strengthen Europe’s capability in acquiring and utilising biological ocean observations for better management of marine resources through strong public outreach and active stakeholder communication.","4":"D2E346BB-4A8C-4301-9291-5620EAC87865"},{"1":"2445289","2":"Data driven intelligence for countering crime","3":"Organised crime is one of the greatest threats to the UK's national security. The role of the National Crime Agency (NCA) is to protect the public by disrupting and bringing to justice those serious and organised criminals who present the highest risk to the UK. However, the volumes of data are growing exponentially. There is therefore a need to work with these experts to automate the processing of the data, learn what these hallmarks are and then search the data for instances worthy of further human analysis. The need is growing for such data-driven intelligence.","4":"7A7077DB-2951-4A35-825E-4C305C0E33DA"},{"1":"2445280","2":"Data driven intelligence for countering crime","3":"Organised crime is one of the greatest threats to the UK's national security. The role of the National Crime Agency (NCA) is to protect the public by disrupting and bringing to justice those serious and organised criminals who present the highest risk to the UK. However, the volumes of data are growing exponentially. There is therefore a need to work with these experts to automate the processing of the data, learn what these hallmarks are and then search the data for instances worthy of further human analysis. The need is growing for such data-driven intelligence.","4":"8CEDE110-4166-403F-B7CB-85E1126E5A84"},{"1":"2447388","2":"Learning to See More: Better Bayesian Track Before Detect using Statistical Machine Learning","3":"Organised crime is one of the greatest threats to the UK's national security. The role of the National Crime Agency (NCA) is to protect the public by disrupting and bringing to justice those serious and organised criminals who present the highest risk to the UK. However, the volumes of data are growing exponentially. There is therefore a need to work with these experts to automate the processing of the data, learn what these hallmarks are and then search the data for instances worthy of further human analysis. The need is growing for such data-driven intelligence.","4":"275820B1-EC3F-4333-A50B-A99F27BB22E3"},{"1":"BB/F011296/1","2":"Peptide aptamer optical protein detection","3":"Our bodies are made up of trillions of cells, which contain billions of proteins. Proteins are the workhorses of a cell, and until we understand how they work together, we will not be able to understand life at the level of each cell, nor will we be able to identify the changes that occur in triggering a disease. While the sequencing of the human genome has given us a catalogue of these proteins, we still lack the tools that allow us to study them both individually- is protein X expressed in this cell?- and collectively- do proteins X and Y, which are both expressed in a cell, actually interact with each other? There are two major hurdles to such studies. The first is that we need probes for each protein that allow us to detect a given protein even in the presence of the billions of other protein molecules in a cell extract. The second is that we need a way to sense when a target protein has bound to its specific probe- proteins are so small that any signal they generate is minute. One way around this has been to add labels to proteins that are capable of generating very large signals, but this solution has its own problems, as the label can modify the behaviour of the protein, for example to prevent a biologically-significant binding event, or to create a new, irrelevant interaction. We have solved both problems using a technique called protein engineering to create artificial proteins that specifically recognise target proteins, and that are sufficiently robust that they can be attached to sensing surfaces to generate label-free electrical or optical signals. Recent advances in our understanding of molecular fluorescence and the optical properties of nanometre-scale structures offer much in the generation of sensitive sensory interfaces. The goal of this project is to integrate receptive artificial proteins with these interfaces and identify the best means of applying our proof-of-concept work to the generation of detection methods both compatible with the needs of researchers but also enabling currently unanswerable questions to be answered. Such questions range from asking what is the smallest number of copies of protein that can be detected in a single cell, to what is the largest number of different proteins that can be detected simultaneously using a technology that is affordable to most biological/clinical labs.","4":"EE88EFA6-D26B-41D3-B1B7-080ECC8965D6"},{"1":"BB/F011032/1","2":"Peptide aptamer optical protein detection","3":"Our bodies are made up of trillions of cells, which contain billions of proteins. Proteins are the workhorses of a cell, and until we understand how they work together, we will not be able to understand life at the level of each cell, nor will we be able to identify the changes that occur in triggering a disease. While the sequencing of the human genome has given us a catalogue of these proteins, we still lack the tools that allow us to study them both individually- is protein X expressed in this cell?- and collectively- do proteins X and Y, which are both expressed in a cell, actually interact with each other? There are two major hurdles to such studies. The first is that we need probes for each protein that allow us to detect a given protein even in the presence of the billions of other protein molecules in a cell extract. The second is that we need a way to sense when a target protein has bound to its specific probe- proteins are so small that any signal they generate is minute. One way around this has been to add labels to proteins that are capable of generating very large signals, but this solution has its own problems, as the label can modify the behaviour of the protein, for example to prevent a biologically-significant binding event, or to create a new, irrelevant interaction. We have solved both problems using a technique called protein engineering to create artificial proteins that specifically recognise target proteins, and that are sufficiently robust that they can be attached to sensing surfaces to generate label-free electrical or optical signals. Recent advances in our understanding of molecular fluorescence and the optical properties of nanometre-scale structures offer much in the generation of sensitive sensory interfaces. The goal of this project is to integrate receptive artificial proteins with these interfaces and identify the best means of applying our proof-of-concept work to the generation of detection methods both compatible with the needs of researchers but also enabling currently unanswerable questions to be answered. Such questions range from asking what is the smallest number of copies of protein that can be detected in a single cell, to what is the largest number of different proteins that can be detected simultaneously using a technology that is affordable to most biological/clinical labs.","4":"B6D80206-703B-42AC-87F0-7226D68A14CB"},{"1":"NE/J020966/1","2":"Predicting the reliability with which the geomagnetic field can be recorded in igneous rocks","3":"Palaeomagnetic recordings in ancient rocks and meteorites hold the key to answering some of the most fundamental questions in Earth Sciences. Theories regarding the evolution of the geodynamo, the thermal evolution of the Earth's core, plate tectonics and palaeogeography, and the formation of the solar system, are all constrained by observations of the ancient fields trapped in rocks that are hundreds or even thousands of millions of years old. However, not all palaeomagnetic observations are reliable, because the magnetic signal carried by most rocks and meteorites is dominated by a poorly understood thermoremanent magnetisation (TRM) in grains with non-uniform magnetic structures.\\nMost palaeomagnetic interpretations are based on the assumption that such TRMs are carried by magnetically uniform, single domain (SD) particles, whose behaviour is well described by N&eacute;el's SD TRM theories. However, slightly larger grains with non-uniform magnetic structures are ubiquitous in nature. These are termed pseudo-SD (PSD) as they display some characteristics to SD grains (such as a large magnetic remanence), but can have a significantly different recording fidelity. Presently there is no physical model for PSD TRM acquisition therefore we have no means of assessing the stability and reliability of many palaeomagnetic signals. \\nThis proposal will address the urgent need to quantify the fundamental behaviour of PSD TRM. In particular we aim to address two key issues that can affect palaeomagnetic fidelity: (a) PSD stability as a function of time and temperature, and (b) their TRM dependence on cooling rates. This will be achieved by developing a three-dimensional numerical model that incorporates the effects of thermal-fluctuations. It will then be possible to model PSD TRM acquisition and assess the accuracy with which PSD domain states can record a geomagnetic field.\\nA key aspect of the numerical modelling is validation of the predicted domain structures, as a function of grain size and temperature, against direct nano-metric-scale experimental observations. This will be achieved using a remarkable set of highly characterised artificial samples (produced by an electron lithography process in a previous NERC-funded study) and using the advanced transmission electron microscope (TEM) technique of off-axis electron holography, which is able to image the magnetisation on a nano-metric scale. Experiments will also be conducted on bulk samples, including a suite of already collected lavas.\\nOnce validated, the numerical model will be used to explore the fidelity of TRM recordings and palaeointensity (ancient geomagnetic field intensity) determinations in a range of grain geometries applicable to natural samples containing PSD domain states.\\nThe research will result in a comprehensive understanding of TRM acquisition for PSD grains of magnetite, which are thought to the dominant carrier of palaeomagnetic recordings, and identify how accurately PSD grains can record the ancient field. The predictive micromagnetic model we develop will be able to directly address a number of key issues, for example:\\n(1) Palaeointensity estimates from PSD magnetites are used to constrain models of the Earth's core dynamics and the Solar System's formation. We will be able to determine whether these palaeointensities are likely to under or over estimate the true value of the ancient field.\\n(2) Archaen palaeointensity estimates are often determined from PSD magnetite crystals, embedded with in single-silicate crystals extracted from gabrros. The model will allow us to quantify the effect of long-term cooling-rates on TRM intensity, something which cannot be done experimentally.\\nWith increased accuracy of palaeomagnetic observations, a much clearer picture will emerge of the past behaviour of the geomagnetic field, and hence a far better hope of unravelling the true nature of the early universe and the evolution and behaviour of the Earths deep interior.","4":"9429EE24-259A-4648-B5EB-2D46B5A74872"},{"1":"NE/J020508/1","2":"Predicting the reliability with which the geomagnetic field can be recorded in igneous rocks","3":"Palaeomagnetic recordings in ancient rocks and meteorites hold the key to answering some of the most fundamental questions in Earth Sciences. Theories regarding the evolution of the geodynamo, the thermal evolution of the Earth's core, plate tectonics and palaeogeography, and the formation of the solar system, are all constrained by observations of the ancient fields trapped in rocks that are hundreds or even thousands of millions of years old. However, not all palaeomagnetic observations are reliable, because the magnetic signal carried by most rocks and meteorites is dominated by a poorly understood thermoremanent magnetisation (TRM) in grains with non-uniform magnetic structures.\\nMost palaeomagnetic interpretations are based on the assumption that such TRMs are carried by magnetically uniform, single domain (SD) particles, whose behaviour is well described by N&eacute;el's SD TRM theories. However, slightly larger grains with non-uniform magnetic structures are ubiquitous in nature. These are termed pseudo-SD (PSD) as they display some characteristics to SD grains (such as a large magnetic remanence), but can have a significantly different recording fidelity. Presently there is no physical model for PSD TRM acquisition therefore we have no means of assessing the stability and reliability of many palaeomagnetic signals. \\nThis proposal will address the urgent need to quantify the fundamental behaviour of PSD TRM. In particular we aim to address two key issues that can affect palaeomagnetic fidelity: (a) PSD stability as a function of time and temperature, and (b) their TRM dependence on cooling rates. This will be achieved by developing a three-dimensional numerical model that incorporates the effects of thermal-fluctuations. It will then be possible to model PSD TRM acquisition and assess the accuracy with which PSD domain states can record a geomagnetic field.\\nA key aspect of the numerical modelling is validation of the predicted domain structures, as a function of grain size and temperature, against direct nano-metric-scale experimental observations. This will be achieved using a remarkable set of highly characterised artificial samples (produced by an electron lithography process in a previous NERC-funded study) and using the advanced transmission electron microscope (TEM) technique of off-axis electron holography, which is able to image the magnetisation on a nano-metric scale. Experiments will also be conducted on bulk samples, including a suite of already collected lavas.\\nOnce validated, the numerical model will be used to explore the fidelity of TRM recordings and palaeointensity (ancient geomagnetic field intensity) determinations in a range of grain geometries applicable to natural samples containing PSD domain states.\\nThe research will result in a comprehensive understanding of TRM acquisition for PSD grains of magnetite, which are thought to the dominant carrier of palaeomagnetic recordings, and identify how accurately PSD grains can record the ancient field. The predictive micromagnetic model we develop will be able to directly address a number of key issues, for example:\\n(1) Palaeointensity estimates from PSD magnetites are used to constrain models of the Earth's core dynamics and the Solar System's formation. We will be able to determine whether these palaeointensities are likely to under or over estimate the true value of the ancient field.\\n(2) Archaen palaeointensity estimates are often determined from PSD magnetite crystals, embedded with in single-silicate crystals extracted from gabrros. The model will allow us to quantify the effect of long-term cooling-rates on TRM intensity, something which cannot be done experimentally.\\nWith increased accuracy of palaeomagnetic observations, a much clearer picture will emerge of the past behaviour of the geomagnetic field, and hence a far better hope of unravelling the true nature of the early universe and the evolution and behaviour of the Earths deep interior.","4":"E9832BDD-F848-4CD6-9C2C-34FE69EF166C"},{"1":"EP/K038214/1","2":"Periodicity-Enhanced Attenuating Layers and Structures","3":"Periodicity-enhanced (meta) materials and surfaces are artificial structures that possess properties not found in naturally-occurring materials and surfaces. The periodicity stems from the regular spacing of inclusions in a host matrix or roughness on a surface. Inclusions range from solid cylinders in air such as encountered in 'sonic crystals' to a grid framework in a poroelastic material such as an air-filled foam used for sound absorption. Roughness elements can be of various shapes and profiles ranging from identical rectangular grooves to arrays with fractal profiles. Without further modification, periodicity-enhanced materials stop the passage of some incident wavelengths (or frequencies) and enhance the transmission of others. By modifying the roughness of a surface, the interference between waves travelling directly from a source to a receiver above the surface and waves reflected from the surface can be controlled. \\nThe proposal is concerned with ways of extending the frequency range over which the periodicity-enhanced materials and surfaces reduce the transmission of sound and vibration. The methods to be investigated include use of locally resonant inclusions or roughness elements, use of multiple resonances, exploitiation of interactions and overlaps between resonances periodicity-related transmission loss and spatial variation of periodicity and other characteristics thereby producing graded systems and roughness profiles. The work will provide a basis for the design of more efficient sound and vibration absorbing devices that are lightweight yet offer high transmission loss and vibration damping properties. The resulting surface designs will include alternatives to conventional noise barriers, while allowing access and preserving line of sight, and cost-effective methods for protecting buildings against ground-borne vibrations.","4":"B04A32C7-8904-4875-ACC0-949121653BBC"},{"1":"EP/K03720X/1","2":"Periodicity-Enhanced Attenuating Layers and Structures","3":"Periodicity-enhanced (meta) materials and surfaces are artificial structures that possess properties not found in naturally-occurring materials and surfaces. The periodicity stems from the regular spacing of inclusions in a host matrix or roughness on a surface. Inclusions range from solid cylinders in air such as encountered in 'sonic crystals' to a grid framework in a poroelastic material such as an air-filled foam used for sound absorption. Roughness elements can be of various shapes and profiles ranging from identical rectangular grooves to arrays with fractal profiles. Without further modification, periodicity-enhanced materials stop the passage of some incident wavelengths (or frequencies) and enhance the transmission of others. By modifying the roughness of a surface, the interference between waves travelling directly from a source to a receiver above the surface and waves reflected from the surface can be controlled. \\nThe proposal is concerned with ways of extending the frequency range over which the periodicity-enhanced materials and surfaces reduce the transmission of sound and vibration. The methods to be investigated include use of locally resonant inclusions or roughness elements, use of multiple resonances, exploitiation of interactions and overlaps between resonances periodicity-related transmission loss and spatial variation of periodicity and other characteristics thereby producing graded systems and roughness profiles. The work will provide a basis for the design of more efficient sound and vibration absorbing devices that are lightweight yet offer high transmission loss and vibration damping properties. The resulting surface designs will include alternatives to conventional noise barriers, while allowing access and preserving line of sight, and cost-effective methods for protecting buildings against ground-borne vibrations.","4":"AA5A773E-5D92-4A73-A7E5-FBE2A34FE0FD"},{"1":"NE/D006287/1","2":"Impact of combined iodine and bromine release on the Arctic atmosphere (COBRA).","3":"Polar sunrise ozone and mercury depletion events are yearly phenomena that occur throughout the Arctic and Antarctic coastal regions, and have implications for the atmospheric oxidative capacity, climate and health. These events are believed to be caused by oxidation of ozone and mercury by bromine-containing radicals formed from photolysis of inorganic bromine (Br2, BrCl) released from the sea-ice surface. Recent studies suggest that 'frost flowers' (FF) - ice crystals that grow on newly formed sea ice - may be the dominant source of polar bromine. The exact nature of emissions from frost flowers is not well established and so far there are no field studies to confirm or otherwise the important role of FF in bromine release, compared to sea salt on sea-ice/snow-pack. Further, little is known about the role and sources of iodine in polar boundary layer chemistry. Iodine-containing aerosol has been associated with ozone depletion at polar sunrise but also appears in autumn - this is not consistent with the only known source of Arctic iodine, the under-ice spring bloom of ice algae. COBRA investigators have recently observed iodine oxide radicals in Antarctica and reactive organic iodine compounds in the Arctic. These so far unpublished observations, in separate polar locations, suggest a widespread and likely abiotic/photochemical source of iodine to the polar atmosphere. Recent theoretical studies indicate that iodine compounds emitted to the Arctic atmosphere have a significantly greater ozone and mercury depletion effect than additional bromine molecules, so our observations may be significant for polar halogen chemistry research. COBRA (Impact of combined iodine and bromine release on the Arctic atmosphere) is essentially a targeted process study, combining field, laboratory and modelling techniques in a consortium of scientists with strong track records in halogen and polar chemistry and physics to: develop understanding of the role of iodine (in concert with bromine) in Arctic gas phase photochemistry and aerosol production and evolution; investigate the relative/combined roles of frost flowers, older sea-ice/snow pack, sea salt aerosol and biological sources in releasing halogens to the Arctic atmosphere; increase understanding of the temporal and spatial variability of halogen-related ozone and mercury depletion events in the Arctic; and develop and evaluate parameterisations for emission of halogens to the Arctic atmosphere based upon observable ice and meteorological conditions, and use these to develop improved models of Arctic chemistry and emissions and their effect and feedbacks on regional/global atmospheric chemistry and climate. We will undertake two ground-based field campaigns, deploying a range of trace gas and aerosol techniques to measure inorganic and halogen compounds and a comprehensive suite of supporting data, in spring and autumn at a coastal site in the north of Hudson Bay, an area with high potential for frost flower growth and a bromine 'hot spot'. The autumn campaign will be augmented by ship-based measurements to determine the wider extent of mercury and ozone depletion episodes and organic halogen concentrations in the region. In addition to concentration measurements at the coastal site, we will measure particle, ozone and halogen concentrations and fluxes from frost flowers formed on artificial leads created in the sea-ice, and from older sea-ice/snow pack and any identified surface diatoms. We will characterise the various sea-ice surfaces in the field and investigate chemical mechanisms of formation from frost flowers in the laboratory. The combined impact of various forms of halogens on depletion of ozone and mercury will be investigated using a detailed process model, and on a wider scale using a global chemistry-transport model.","4":"1B169CCB-FF8F-430B-868B-C2005F9E573F"},{"1":"NE/D006104/1","2":"Impact of combined iodine and bromine release on the Arctic atmosphere (COBRA).","3":"Polar sunrise ozone and mercury depletion events are yearly phenomena that occur throughout the Arctic and Antarctic coastal regions, and have implications for the atmospheric oxidative capacity, climate and health. These events are believed to be caused by oxidation of ozone and mercury by bromine-containing radicals formed from photolysis of inorganic bromine (Br2, BrCl) released from the sea-ice surface. Recent studies suggest that 'frost flowers' (FF) - ice crystals that grow on newly formed sea ice - may be the dominant source of polar bromine. The exact nature of emissions from frost flowers is not well established and so far there are no field studies to confirm or otherwise the important role of FF in bromine release, compared to sea salt on sea-ice/snow-pack. Further, little is known about the role and sources of iodine in polar boundary layer chemistry. Iodine-containing aerosol has been associated with ozone depletion at polar sunrise but also appears in autumn - this is not consistent with the only known source of Arctic iodine, the under-ice spring bloom of ice algae. COBRA investigators have recently observed iodine oxide radicals in Antarctica and reactive organic iodine compounds in the Arctic. These so far unpublished observations, in separate polar locations, suggest a widespread and likely abiotic/photochemical source of iodine to the polar atmosphere. Recent theoretical studies indicate that iodine compounds emitted to the Arctic atmosphere have a significantly greater ozone and mercury depletion effect than additional bromine molecules, so our observations may be significant for polar halogen chemistry research. COBRA (Impact of combined iodine and bromine release on the Arctic atmosphere) is essentially a targeted process study, combining field, laboratory and modelling techniques in a consortium of scientists with strong track records in halogen and polar chemistry and physics to: develop understanding of the role of iodine (in concert with bromine) in Arctic gas phase photochemistry and aerosol production and evolution; investigate the relative/combined roles of frost flowers, older sea-ice/snow pack, sea salt aerosol and biological sources in releasing halogens to the Arctic atmosphere; increase understanding of the temporal and spatial variability of halogen-related ozone and mercury depletion events in the Arctic; and develop and evaluate parameterisations for emission of halogens to the Arctic atmosphere based upon observable ice and meteorological conditions, and use these to develop improved models of Arctic chemistry and emissions and their effect and feedbacks on regional/global atmospheric chemistry and climate. We will undertake two ground-based field campaigns, deploying a range of trace gas and aerosol techniques to measure inorganic and halogen compounds and a comprehensive suite of supporting data, in spring and autumn at a coastal site in the north of Hudson Bay, an area with high potential for frost flower growth and a bromine 'hot spot'. The autumn campaign will be augmented by ship-based measurements to determine the wider extent of mercury and ozone depletion episodes and organic halogen concentrations in the region. In addition to concentration measurements at the coastal site, we will measure particle, ozone and halogen concentrations and fluxes from frost flowers formed on artificial leads created in the sea-ice, and from older sea-ice/snow pack and any identified surface diatoms. We will characterise the various sea-ice surfaces in the field and investigate chemical mechanisms of formation from frost flowers in the laboratory. The combined impact of various forms of halogens on depletion of ozone and mercury will be investigated using a detailed process model, and on a wider scale using a global chemistry-transport model.","4":"52192F26-BFDD-4933-9666-D60D05A0D61B"},{"1":"NE/D005914/1","2":"Impact of combined iodine and bromine release on the Arctic atmosphere (COBRA).","3":"Polar sunrise ozone and mercury depletion events are yearly phenomena that occur throughout the Arctic and Antarctic coastal regions, and have implications for the atmospheric oxidative capacity, climate and health. These events are believed to be caused by oxidation of ozone and mercury by bromine-containing radicals formed from photolysis of inorganic bromine (Br2, BrCl) released from the sea-ice surface. Recent studies suggest that 'frost flowers' (FF) - ice crystals that grow on newly formed sea ice - may be the dominant source of polar bromine. The exact nature of emissions from frost flowers is not well established and so far there are no field studies to confirm or otherwise the important role of FF in bromine release, compared to sea salt on sea-ice/snow-pack. Further, little is known about the role and sources of iodine in polar boundary layer chemistry. Iodine-containing aerosol has been associated with ozone depletion at polar sunrise but also appears in autumn - this is not consistent with the only known source of Arctic iodine, the under-ice spring bloom of ice algae. COBRA investigators have recently observed iodine oxide radicals in Antarctica and reactive organic iodine compounds in the Arctic. These so far unpublished observations, in separate polar locations, suggest a widespread and likely abiotic/photochemical source of iodine to the polar atmosphere. Recent theoretical studies indicate that iodine compounds emitted to the Arctic atmosphere have a significantly greater ozone and mercury depletion effect than additional bromine molecules, so our observations may be significant for polar halogen chemistry research. COBRA (Impact of combined iodine and bromine release on the Arctic atmosphere) is essentially a targeted process study, combining field, laboratory and modelling techniques in a consortium of scientists with strong track records in halogen and polar chemistry and physics to: develop understanding of the role of iodine (in concert with bromine) in Arctic gas phase photochemistry and aerosol production and evolution; investigate the relative/combined roles of frost flowers, older sea-ice/snow pack, sea salt aerosol and biological sources in releasing halogens to the Arctic atmosphere; increase understanding of the temporal and spatial variability of halogen-related ozone and mercury depletion events in the Arctic; and develop and evaluate parameterisations for emission of halogens to the Arctic atmosphere based upon observable ice and meteorological conditions, and use these to develop improved models of Arctic chemistry and emissions and their effect and feedbacks on regional/global atmospheric chemistry and climate. We will undertake two ground-based field campaigns, deploying a range of trace gas and aerosol techniques to measure inorganic and halogen compounds and a comprehensive suite of supporting data, in spring and autumn at a coastal site in the north of Hudson Bay, an area with high potential for frost flower growth and a bromine 'hot spot'. The autumn campaign will be augmented by ship-based measurements to determine the wider extent of mercury and ozone depletion episodes and organic halogen concentrations in the region. In addition to concentration measurements at the coastal site, we will measure particle, ozone and halogen concentrations and fluxes from frost flowers formed on artificial leads created in the sea-ice, and from older sea-ice/snow pack and any identified surface diatoms. We will characterise the various sea-ice surfaces in the field and investigate chemical mechanisms of formation from frost flowers in the laboratory. The combined impact of various forms of halogens on depletion of ozone and mercury will be investigated using a detailed process model, and on a wider scale using a global chemistry-transport model.","4":"8EBDE039-D19C-4F2B-BCF8-19499DEA026D"},{"1":"NE/D006015/1","2":"Impact of combined iodine and bromine release on the Arctic atmosphere (COBRA).","3":"Polar sunrise ozone and mercury depletion events are yearly phenomena that occur throughout the Arctic and Antarctic coastal regions, and have implications for the atmospheric oxidative capacity, climate and health. These events are believed to be caused by oxidation of ozone and mercury by bromine-containing radicals formed from photolysis of inorganic bromine (Br2, BrCl) released from the sea-ice surface. Recent studies suggest that 'frost flowers' (FF) - ice crystals that grow on newly formed sea ice - may be the dominant source of polar bromine. The exact nature of emissions from frost flowers is not well established and so far there are no field studies to confirm or otherwise the important role of FF in bromine release, compared to sea salt on sea-ice/snow-pack. Further, little is known about the role and sources of iodine in polar boundary layer chemistry. Iodine-containing aerosol has been associated with ozone depletion at polar sunrise but also appears in autumn - this is not consistent with the only known source of Arctic iodine, the under-ice spring bloom of ice algae. COBRA investigators have recently observed iodine oxide radicals in Antarctica and reactive organic iodine compounds in the Arctic. These so far unpublished observations, in separate polar locations, suggest a widespread and likely abiotic/photochemical source of iodine to the polar atmosphere. Recent theoretical studies indicate that iodine compounds emitted to the Arctic atmosphere have a significantly greater ozone and mercury depletion effect than additional bromine molecules, so our observations may be significant for polar halogen chemistry research. COBRA (Impact of combined iodine and bromine release on the Arctic atmosphere) is essentially a targeted process study, combining field, laboratory and modelling techniques in a consortium of scientists with strong track records in halogen and polar chemistry and physics to: develop understanding of the role of iodine (in concert with bromine) in Arctic gas phase photochemistry and aerosol production and evolution; investigate the relative/combined roles of frost flowers, older sea-ice/snow pack, sea salt aerosol and biological sources in releasing halogens to the Arctic atmosphere; increase understanding of the temporal and spatial variability of halogen-related ozone and mercury depletion events in the Arctic; and develop and evaluate parameterisations for emission of halogens to the Arctic atmosphere based upon observable ice and meteorological conditions, and use these to develop improved models of Arctic chemistry and emissions and their effect and feedbacks on regional/global atmospheric chemistry and climate. We will undertake two ground-based field campaigns, deploying a range of trace gas and aerosol techniques to measure inorganic and halogen compounds and a comprehensive suite of supporting data, in spring and autumn at a coastal site in the north of Hudson Bay, an area with high potential for frost flower growth and a bromine 'hot spot'. The autumn campaign will be augmented by ship-based measurements to determine the wider extent of mercury and ozone depletion episodes and organic halogen concentrations in the region. In addition to concentration measurements at the coastal site, we will measure particle, ozone and halogen concentrations and fluxes from frost flowers formed on artificial leads created in the sea-ice, and from older sea-ice/snow pack and any identified surface diatoms. We will characterise the various sea-ice surfaces in the field and investigate chemical mechanisms of formation from frost flowers in the laboratory. The combined impact of various forms of halogens on depletion of ozone and mercury will be investigated using a detailed process model, and on a wider scale using a global chemistry-transport model.","4":"B8C93AEF-25E6-4D85-A489-215BE9ED24D1"},{"1":"NE/D011035/1","2":"Predator-prey interactions and the evolution of prey aggregation","3":"Predation is a key factor in the structuring of ecological populations and communities. The behavioural adaptations of predators and prey to each other can have a fundamental effect on population processes by affecting how individuals in a population interact. This project considers the behavioural adaptations and counter-adaptations of predator and prey and how these translate into larger scale phenomena at the group and population levels. Specifically, we consider how the sensory systems of predators can become confused by large groups of prey, how this can be made worse by prey behaviour and appearance, how it can be ameliorated by predator behaviour, and how the behaviours investigated translate into observed natural variation in the form and composition of animal groups at the single-group and population scale. We will use the powerful computational modelling techniques of artificial neural networks and genetic algorithms, fully validated with experiments on predation by sticklebacks and humans. The predatory sensory system is represented by a multilayer artificial neural network with a static, previously obtained, mapping unit that generates the aforementioned 'confusion effect', and a trainable decision making network that interprets the cognitive map and chooses prey items in a way that ameliorates predator confusion. This decision-making unit is trained using a process similar to natural selection (a genetic algorithm) while the whole network is presented with numerous images of prey groups. Previous modelling and experimental studies indicate that the prey individuals chosen will be from the edge of the group, of odd appearance, from small groups etc. Here we will fully investigate key questions addresses by the project: How does prey grouping ecology affect predator success? How might natural population-level variation in animal groups (group size, composition etc) affect predator success? How does complexity of the prey group affect predator success and under what circumstances might a predator choose to specialise on a particular prey type? These simulations will generate multiple predictions that will be subject to full experimental validation with experiments on sticklebacks predating real and computer generated swarms of Daphnia, and humans predating virtual groups of prey on a computer screen. Images of the real and virtual prey groups immediately prior to strike by the study organism will be fed into the predatory neural network and the choice of network and real organism compared. Having validated the neural network model, and investigated the influence of prey group behaviour on predator success, we will consider the counter case of how predator prey choice may effect group formation of prey. Individual-based models of the prey shoal, herd, and swarm will be developed from previous publications and parameters given a genetic basis so that virtual prey groups can evolve in form and composition in response to predation by the neural network predator. The predator will remove individuals from the prey group, and evolution of prey group form examined. How does the optimal predator strike strategy affect the ultimate form and composition of a prey group? Can between-group variation in the form and composition of prey groups be explained by variation in predator characteristic and/or initial form of the groups? Other fundamental questions will be addressed. Ultimately, the whole predator-prey modelling framework will be integrated into a full coevolutionary system in which predator and prey evolve simultaneously. These sophisticated simulations will be used to probe fundamental population-level questions such as: Can natural variation between animals groups in form and composition be explained by predation? What types of predator might be associated with different types of prey structure in natural populations? How might predator strategy and prey group form change through evolutionary time?","4":"9D723FEF-FC38-4BC9-A06D-182A34F1F80B"},{"1":"NE/D010772/1","2":"Predator-prey interactions and the evolution of prey aggregation","3":"Predation is a key factor in the structuring of ecological populations and communities. The behavioural adaptations of predators and prey to each other can have a fundamental effect on population processes by affecting how individuals in a population interact. This project considers the behavioural adaptations and counter-adaptations of predator and prey and how these translate into larger scale phenomena at the group and population levels. Specifically, we consider how the sensory systems of predators can become confused by large groups of prey, how this can be made worse by prey behaviour and appearance, how it can be ameliorated by predator behaviour, and how the behaviours investigated translate into observed natural variation in the form and composition of animal groups at the single-group and population scale. We will use the powerful computational modelling techniques of artificial neural networks and genetic algorithms, fully validated with experiments on predation by sticklebacks and humans. The predatory sensory system is represented by a multilayer artificial neural network with a static, previously obtained, mapping unit that generates the aforementioned 'confusion effect', and a trainable decision making network that interprets the cognitive map and chooses prey items in a way that ameliorates predator confusion. This decision-making unit is trained using a process similar to natural selection (a genetic algorithm) while the whole network is presented with numerous images of prey groups. Previous modelling and experimental studies indicate that the prey individuals chosen will be from the edge of the group, of odd appearance, from small groups etc. Here we will fully investigate key questions addresses by the project: How does prey grouping ecology affect predator success? How might natural population-level variation in animal groups (group size, composition etc) affect predator success? How does complexity of the prey group affect predator success and under what circumstances might a predator choose to specialise on a particular prey type? These simulations will generate multiple predictions that will be subject to full experimental validation with experiments on sticklebacks predating real and computer generated swarms of Daphnia, and humans predating virtual groups of prey on a computer screen. Images of the real and virtual prey groups immediately prior to strike by the study organism will be fed into the predatory neural network and the choice of network and real organism compared. Having validated the neural network model, and investigated the influence of prey group behaviour on predator success, we will consider the counter case of how predator prey choice may effect group formation of prey. Individual-based models of the prey shoal, herd, and swarm will be developed from previous publications and parameters given a genetic basis so that virtual prey groups can evolve in form and composition in response to predation by the neural network predator. The predator will remove individuals from the prey group, and evolution of prey group form examined. How does the optimal predator strike strategy affect the ultimate form and composition of a prey group? Can between-group variation in the form and composition of prey groups be explained by variation in predator characteristic and/or initial form of the groups? Other fundamental questions will be addressed. Ultimately, the whole predator-prey modelling framework will be integrated into a full coevolutionary system in which predator and prey evolve simultaneously. These sophisticated simulations will be used to probe fundamental population-level questions such as: Can natural variation between animals groups in form and composition be explained by predation? What types of predator might be associated with different types of prey structure in natural populations? How might predator strategy and prey group form change through evolutionary time?","4":"2D06756E-2368-4D9F-B8FC-A0AE77492647"},{"1":"EP/F025645/1","2":"ADEPT: Adaptive Dynamic Ensemble Prediction Techniques","3":"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.","4":"BE25CD09-195A-40C6-95EF-FB1FC64A438E"},{"1":"EP/F023871/1","2":"ADEPT: Adaptive Dynamic Ensemble Prediction Techniques","3":"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.","4":"5232AEE1-5F8A-4C5A-81FE-F667E38CE8F4"},{"1":"EP/F023855/1","2":"ADEPT: Adaptive Dynamic Ensemble Prediction Techniques","3":"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.","4":"40BD749F-2611-41AD-A249-6F0DFB5CF8DB"},{"1":"NE/H013962/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage","3":"Proposal to Research Councils Energy Program: Carbon Capture and Storage / Potential ecosystem impacts of geological carbon storage call. Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS). Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"27642E64-F32F-46F3-B395-DCB0E3E25D58"},{"1":"NE/H013849/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage","3":"Proposal to Research Councils Energy Program: Carbon Capture and Storage / Potential ecosystem impacts of geological carbon storage call. Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS). Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"ED9C3219-829C-4227-A180-E5B8A7C8DD4E"},{"1":"NE/H013954/1","2":"Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage","3":"Proposal to Research Councils Energy Program: Carbon Capture and Storage / Potential ecosystem impacts of geological carbon storage call. Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS). Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"852A601D-CD42-4CE6-901A-8DCF9652D2DB"},{"1":"NE/H013970/1","2":"Qualifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS)","3":"Proposal to Research Councils Energy Program: Carbon Capture and Storage / Potential ecosystem impacts of geological carbon storage call. Quantifying and Monitoring Potential Ecosystem Impacts of Geological Carbon Storage (QICS). Climate change caused by increasing emissions of CO2, principally the burning of fossil fuels for power generation, is one of the most pressing concerns for society. Currently around 90% of the UK's energy needs are met by fossil fuels which will probably continue to be the predominant source of energy for decades to come. Developing our understanding of the pros and cons of a range of strategies designed to reduce CO2 emissions is vital. Of the available strategies such as wind, wave and solar renewables and Carbon Capture and Storage (CCS) none are without potential problems or limitations. The concept of CCS simply put is to capture CO2 during the process of power generation and to store it permanently in deep geological structures beneath the land or sea surface. If CCS is successful existing fossil fuel reserves could be used whilst new forms of power generation with low CO2 emissions are developed. A few projects have been successfully demonstrating either capture or storage on limited scales, so it is established that the technical challenges are surmountable. Research is also demonstrating that the geological structures are in general robust for long term storage (for example oil deposits remain in place within geological strata). However geological structures are complex and natural sub surface gas deposits are known to outgas at the surface. Consequently it would be irresponsible to develop full scale CCS programmes without an understanding of the likelihood of leakage and the severity of impacts which might occur. The aim of this proposal is to greatly improve the understanding of the scale of impact a leakage from CCS systems might inflict on the ecosystem and to enable a comprehensive risk assessment of CCS. The main location of stored CO2 in the UK will be in geo-formations under the North Sea and our research concentrates on impacts to the marine environment, although our work will also be relevant to all geological formations. Research to date has shown that hypothetical large leaks would significantly alter sediment and water chemistry and consequently some marine creatures would be vulnerable. What is not yet understood is how resilient species are, and how big an impact would stem from a given leak. Our project will investigate for the first time the response of a real marine community (both within and above the sediments) to a small scale tightly controlled artificial leak. We will look at chemical and biological effects and importantly investigate the recovery time needed. We will be able to relate the footprint of the impact to the known input rate of CO2. The results will allow us to develop and test models of flow and impact that can be applied to other scenarios and we will assess a number of monitoring methods. The project will also investigate the nature of flow through geological formations to give us an understanding of the spread of a rising CO2 plume should it breach the reservoir. The work proposed here would amount to a significant advance in the understanding and scientific tools necessary to form CCS risk assessments and quantitative knowledge of the ecological impacts of leaks. We will develop model tools that can predict the transfer, fate and impact of leaks from reservoir to ecosystem, which may be applied when specific CCS operations are planned. An important product of our work will be a recommendation of the best monitoring strategy to ensure the early detection of leaks. We will work alongside interested parties from industry, government and public to ensure that the information we produce is accessible and effective.","4":"E7556C5F-83D1-4A1B-AEF6-BC70D5BA54DB"},{"1":"EP/E015018/1","2":"Calculation of Protein Dielectric Constants using Molecular Dynamics Simulation","3":"Proteins are highly complex molecules held together by a large number of relatively weak interactions in solution. These interactions are critical in maintaining the folded structure of proteins. Some of the amino acids that make up the protein are charged and this charge can vary in a pH dependent way. Therefore, proteins are extremely sensitive to changes in the pH of their environment. In a highly acidic solution for instance, many of the negative charges within the protein may be neutralised. This change in the electrostatic interactions within the protein can significantly affect the structure and function of the biomolecule. Many enzymes are only catalytic over a relatively narrow range of pH as the chemistry and charge distribution of the active site needs to be highly specific. Such processes are extremely difficult to understand theoretically as there are a large number of interacting charges in these highly complex systems. The aim of this project is to improve current methods for calculating the response of proteins to changes in pH using computer simulation. A charge inserted into a protein results in a local rearrangement of neighbouring charged groups to maximise favourable electrostatic interactions. This is known as the protein dielectric response. This project will use an artificial charge probe to measure the dielectric response in various regions of the protein, and in particular will investigate the differences between the interior of proteins and the protein/water interface. The surface of the biomolecule is expected to have a higher ability to respond to charge insertion than the interior as water has an anomalously high dielectric response. These effects are important because there is a connection between the dielectric response and the pH dependent behaviour of the protein which remains poorly understood. This study will develop current theories by starting from the most basic physical definition of the dielectric response and building towards a full description of its role in pH dependent structure and function.","4":"0DE0CF8A-1046-494A-B364-AA03E38689D0"},{"1":"EP/E015190/1","2":"Calculation of Protein Dielectric Constants using Molecular Dynamics Simulation","3":"Proteins are highly complex molecules held together by a large number of relatively weak interactions in solution. These interactions are critical in maintaining the folded structure of proteins. Some of the amino acids that make up the protein are charged and this charge can vary in a pH dependent way. Therefore, proteins are extremely sensitive to changes in the pH of their environment. In a highly acidic solution for instance, many of the negative charges within the protein may be neutralised. This change in the electrostatic interactions within the protein can significantly affect the structure and function of the biomolecule. Many enzymes are only catalytic over a relatively narrow range of pH as the chemistry and charge distribution of the active site needs to be highly specific. Such processes are extremely difficult to understand theoretically as there are a large number of interacting charges in these highly complex systems. The aim of this project is to improve current methods for calculating the response of proteins to changes in pH using computer simulation. A charge inserted into a protein results in a local rearrangement of neighbouring charged groups to maximise favourable electrostatic interactions. This is known as the protein dielectric response. This project will use an artificial charge probe to measure the dielectric response in various regions of the protein, and in particular will investigate the differences between the interior of proteins and the protein/water interface. The surface of the biomolecule is expected to have a higher ability to respond to charge insertion than the interior as water has an anomalously high dielectric response. These effects are important because there is a connection between the dielectric response and the pH dependent behaviour of the protein which remains poorly understood. This study will develop current theories by starting from the most basic physical definition of the dielectric response and building towards a full description of its role in pH dependent structure and function.","4":"4DF7E439-A701-4507-AB1C-08A0174E7C4C"},{"1":"10044780","2":"BUILDING A SUSTAINABLE EUROPEAN INNOVATION PLATFORM TOENHANCE THE REPURPOSING OF MEDICINES FOR ALL -REMEDI4ALL","3":"REMEDI4ALL aims to establish Europe's leadership globally in the repurposing of medicines by creating a vibrant community of practice covering all relevant sectors and disciplines. REMEDI4ALL will establish and operate a permanent European research and innovation platform comprising the complete value chain for cutting edge, patient-focused repurposing, collaborating with users to execute high potential projects at any phase of development, upskilling all stakeholder groups through a comprehensive education and training portfolio, and advancing cross-sectoral policy dialogue with all relevant stakeholders and thought leaders. The tools and processes developed assembled in REMEDI4ALL will be validated in a portfolio of 4 ambitious preclinical and clinical phase demonstrators, representing high patient need in a variety of disease areas, including oncology, rare and infectious diseases. The REMEDI4ALL platform will operate a complete, harmonised and accessible value chain integrating the scientific, methodological, financial, legal, regulatory, and intellectual property aspects of the repurposing approach in a goal-oriented and patient-centric approach. Working closely together with research funders and the patient community, REMEDI4ALL will engage and support a substantial user base and concomitant investments through a permanently sustainable entity, working closely with the permanent European Infrastructures and partners. The community of practice comprises leading academic and clinical facilities and infrastructures, medicines regulators, health technology assessors, public agencies and funders, small and large industry, state-of-the-art artificial intelligence and in silico tools, and a global network of collaborators for optimal knowledge exchange. REMEDI4ALL will substantially increase Europe's capacity to develop high quality repurposed medicines and implement them into market, through its complete value chain and consensus-based policy development mission.","4":"665C3940-CED9-4F0C-B83B-082DBF85BBBE"},{"1":"10039985","2":"BUILDING A SUSTAINABLE EUROPEAN INNOVATION PLATFORM TO ENHANCE THE REPURPOSING OF MEDICINES FOR ALL","3":"REMEDI4ALL aims to establish Europe's leadership globally in the repurposing of medicines by creating a vibrant community of practice covering all relevant sectors and disciplines. REMEDI4ALL will establish and operate a permanent European research and innovation platform comprising the complete value chain for cutting edge, patient-focused repurposing, collaborating with users to execute high potential projects at any phase of development, upskilling all stakeholder groups through a comprehensive education and training portfolio, and advancing cross-sectoral policy dialogue with all relevant stakeholders and thought leaders. The tools and processes developed assembled in REMEDI4ALL will be validated in a portfolio of 4 ambitious preclinical and clinical phase demonstrators, representing high patient need in a variety of disease areas, including oncology, rare and infectious diseases. The REMEDI4ALL platform will operate a complete, harmonised and accessible value chain integrating the scientific, methodological, financial, legal, regulatory, and intellectual property aspects of the repurposing approach in a goal-oriented and patient-centric approach. Working closely together with research funders and the patient community, REMEDI4ALL will engage and support a substantial user base and concomitant investments through a permanently sustainable entity, working closely with the permanent European Infrastructures and partners. The community of practice comprises leading academic and clinical facilities and infrastructures, medicines regulators, health technology assessors, public agencies and funders, small and large industry, state-of-the-art artificial intelligence and in silico tools, and a global network of collaborators for optimal knowledge exchange. REMEDI4ALL will substantially increase Europe's capacity to develop high quality repurposed medicines and implement them into market, through its complete value chain and consensus-based policy development mission.","4":"DEFC4C46-6E4C-487B-AAE9-2E7CC824D148"},{"1":"10042461","2":"NEXTAIR - multi-disciplinary digital - enablers for NEXT-generation AIRcraft design and operations","3":"Radical changes in aircraft configurations and operations are required to meet the target of climate-neutral aviation. To foster this transformation, innovative digital methodologies are of utmost importance to enable the optimisation of aircraft performances. NEXTAIR will develop and demonstrate innovative design methodologies, data-fusion techniques and smart health-assessment tools enabling the digital transformation of aircraft design, manufacturing and maintenance. NEXTAIR proposes digital enablers covering the whole aircraft life-cycle devoted to ease breakthrough technology maturation, their flawless entry into service and smart health assessment. They will be demonstrated in 8 industrial test cases, representative of multi-physics industrial design, maintenance problems and environmental challenges and interest for aircraft and engines manufacturers. NEXTAIR will increase high-fidelity modelling and simulation capabilities to accelerate and derisk new disruptive configurations and breakthrough technologies design. NEXTAIR will also improve the efficiency of uncertainty quantification and robust optimisation techniques to effectively account for manufacturing uncertainty and operational variability in the industrial multi-disciplinary design of aircraft and engine components. Finally, NEXTAIR will extend the usability of machine learning-driven methodologies to contribute to aircraft and engine components' digital twinning for smart prototyping and maintenance. NEXTAIR brings together 16 partners from 6 countries specialised in various disciplines: digital tools, advanced modelling and simulation, artificial intelligence, machine learning, aerospace design, and innovative manufacturing. The consortium includes 9 research organisations, 4 leading aeronautical industries providing digital-physical scaled demonstrator aircraft and engines and 2 highTech SME providing expertise in industrial scientific computing and data intelligence.","4":"2DDBBF19-7156-49FB-B885-70EE1B83E0B9"},{"1":"10039074","2":"NEXTAIR","3":"Radical changes in aircraft configurations and operations are required to meet the target of climate-neutral aviation. To foster this transformation, innovative digital methodologies are of utmost importance to enable the optimisation of aircraft performances. NEXTAIR will develop and demonstrate innovative design methodologies, data-fusion techniques and smart health-assessment tools enabling the digital transformation of aircraft design, manufacturing and maintenance. NEXTAIR proposes digital enablers covering the whole aircraft life-cycle devoted to ease breakthrough technology maturation, their flawless entry into service and smart health assessment. They will be demonstrated in 8 industrial test cases, representative of multi-physics industrial design, maintenance problems and environmental challenges and interest for aircraft and engines manufacturers. NEXTAIR will increase high-fidelity modelling and simulation capabilities to accelerate and derisk new disruptive configurations and breakthrough technologies design. NEXTAIR will also improve the efficiency of uncertainty quantification and robust optimisation techniques to effectively account for manufacturing uncertainty and operational variability in the industrial multi-disciplinary design of aircraft and engine components. Finally, NEXTAIR will extend the usability of machine learning-driven methodologies to contribute to aircraft and engine components' digital twinning for smart prototyping and maintenance. NEXTAIR brings together 16 partners from 6 countries specialised in various disciplines: digital tools, advanced modelling and simulation, artificial intelligence, machine learning, aerospace design, and innovative manufacturing. The consortium includes 9 research organisations, 4 leading aeronautical industries providing digital-physical scaled demonstrator aircraft and engines and 2 highTech SME providing expertise in industrial scientific computing and data intelligence.","4":"2B991E72-3AC3-460F-AD14-FA627E8ADE54"},{"1":"10038896","2":"NEXTAIR: Multi-disciplinary digital - enablers for NEXT-generation AIRcraft design and operations","3":"Radical changes in aircraft configurations and operations are required to meet the target of climate-neutral aviation. To foster this transformation, innovative digital methodologies are of utmost importance to enable the optimisation of aircraft performances. NEXTAIR will develop and demonstrate innovative design methodologies, data-fusion techniques and smart health-assessment tools enabling the digital transformation of aircraft design, manufacturing and maintenance. NEXTAIR proposes digital enablers covering the whole aircraft life-cycle devoted to ease breakthrough technology maturation, their flawless entry into service and smart health assessment. They will be demonstrated in 8 industrial test cases, representative of multi-physics industrial design, maintenance problems and environmental challenges and interest for aircraft and engines manufacturers. NEXTAIR will increase high-fidelity modelling and simulation capabilities to accelerate and derisk new disruptive configurations and breakthrough technologies design. NEXTAIR will also improve the efficiency of uncertainty quantification and robust optimisation techniques to effectively account for manufacturing uncertainty and operational variability in the industrial multi-disciplinary design of aircraft and engine components. Finally, NEXTAIR will extend the usability of machine learning-driven methodologies to contribute to aircraft and engine components' digital twinning for smart prototyping and maintenance. NEXTAIR brings together 16 partners from 6 countries specialised in various disciplines: digital tools, advanced modelling and simulation, artificial intelligence, machine learning, aerospace design, and innovative manufacturing. The consortium includes 9 research organisations, 4 leading aeronautical industries providing digital-physical scaled demonstrator aircraft and engines and 2 highTech SME providing expertise in industrial scientific computing and data intelligence.","4":"704F770F-7331-4B1B-8E3E-8442DA62DE7C"},{"1":"1878602","2":"Forced Convection Heat Transfer in Unconventional Geothermal Systems: Numerical Investigation of Complex Flow Processes near Magmatic Chambers","3":"Recent research into frontier Enhanced Geothermal Systems (EGS) has highlighted the need to better understand the associated heat exchange and fluid dynamics to converge between unconventional well designs and the definition of where their performance would be best. Whilst convection and mixing within magma chambers and hydrothermal circulation in natural groundwater systems have been extensively investigated, there is limited understanding of the heat transfer from magma to water, the behaviour of supercritical/superheated fluids and the convective forces triggered by stimulation and artificial injection near magmatic chambers. Convective heat transport is further modified by multiphase flow phenomena induced by phase change.\\nBeginning with the fundamental differences between conduction- vs. convention-dominated geothermal systems, this PhD research will focus on the steady-state and transient heat transfer processes and fluid dynamics associated with free and forced convection near magmatic chambers.\\nDifferent scenarios will be defined considering the various deep geothermal solutions that have already been implemented or proposed in the literature. Numerical tools will be used to reproduce the settings from different volcano sites, where relevant parameters will be extracted from the literature available in the public domain.\\nThe outcome of this PhD will contribute to re-evaluating the geothermal resource base worldwide and aims to improve the current understanding of heat transfer from magma to water under a combination of free and forced convection","4":"A15C22BA-DFC7-43D1-A3FA-D4AA3BA8D1BC"},{"1":"2625020","2":"Forced Convection Heat Transfer in Unconventional Geothermal Systems: Numerical Investigation of Complex Flow Processes near Magmatic Chambers","3":"Recent research into frontier Enhanced Geothermal Systems (EGS) has highlighted the need to better understand the associated heat exchange and fluid dynamics to converge between unconventional well designs and the definition of where their performance would be best. Whilst convection and mixing within magma chambers and hydrothermal circulation in natural groundwater systems have been extensively investigated, there is limited understanding of the heat transfer from magma to water, the behaviour of supercritical/superheated fluids and the convective forces triggered by stimulation and artificial injection near magmatic chambers. Convective heat transport is further modified by multiphase flow phenomena induced by phase change.\\nBeginning with the fundamental differences between conduction- vs. convention-dominated geothermal systems, this PhD research will focus on the steady-state and transient heat transfer processes and fluid dynamics associated with free and forced convection near magmatic chambers.\\nDifferent scenarios will be defined considering the various deep geothermal solutions that have already been implemented or proposed in the literature. Numerical tools will be used to reproduce the settings from different volcano sites, where relevant parameters will be extracted from the literature available in the public domain.\\nThe outcome of this PhD will contribute to re-evaluating the geothermal resource base worldwide and aims to improve the current understanding of heat transfer from magma to water under a combination of free and forced convection","4":"BA8DECA9-35F8-4149-A6E5-66DE2A810EB1"},{"1":"2744707","2":"Efficient Exploration for Model-Based Reinforcement Learning","3":"Reinforcement Learning (RL) is a branch of Artificial Intelligence (AI) research that focuses on developing algorithms which enable an agent to learn a certain task in the environment they are placed in, such as maximizing the score in a game. A crucial component of this learning process is for the agent to explore strategies that result in the best possible score. When these agents are not able to explore efficiently, their applicability becomes very limited for many use cases. The proposed study will focus on developing novel approaches that allow RL agents to efficiently perform exploration, using the most promising direction of the sub-field: model-based RL. On successful completion, this study will be of invaluable impact to the gaming industry, as it will allow feasible development of RL systems that can acquire high-level playing skills in many types of games, allowing them to be used as challenging opponents, and for extensive game testing.","4":"38C320B8-1A51-4E0D-8906-0D08B81DBCB6"},{"1":"2595272","2":"Efficient Exploration for Model-Based Reinforcement Learning","3":"Reinforcement Learning (RL) is a branch of Artificial Intelligence (AI) research that focuses on developing algorithms which enable an agent to learn a certain task in the environment they are placed in, such as maximizing the score in a game. A crucial component of this learning process is for the agent to explore strategies that result in the best possible score. When these agents are not able to explore efficiently, their applicability becomes very limited for many use cases. The proposed study will focus on developing novel approaches that allow RL agents to efficiently perform exploration, using the most promising direction of the sub-field: model-based RL. On successful completion, this study will be of invaluable impact to the gaming industry, as it will allow feasible development of RL systems that can acquire high-level playing skills in many types of games, allowing them to be used as challenging opponents, and for extensive game testing.","4":"CCCEF3EE-A4F0-4B55-B1FB-CEA4CF8F05B5"},{"1":"2104260","2":"Application of Artificial Intelligence-Driven Design of Function-Directed Ligands for Selective Retinoic Acid Receptor Binding","3":"Retinoic acid (RA) ligands bind to the retinoic acid receptor (RAR) class of nuclear receptor. The shape and structure of RAR ligands though that optimally activate RARs is poorly understood and this project aims to model RARs to a degree not so far obtained to be able to design ligands that activate the receptors and understand the triggering routes for RARs for both genomic and non-genomic signalling. New approaches will be developed to these ends that will be applicable not just to RARs but to many members of the nuclear receptor class of receptors. \\n\\nWe are world-leaders in understanding the function of RA in the brain (1), making the discovery that several mechanisms by which RAR ligands act are crucial for their action: both genomic activity, to turn on gene transcription, and rapid non-genomic action, involving kinase activation.\\n\\nIn this project a radically different approach will be taken to ligand design, to modelling and to understanding binding selectivities to the different RARs. This will use a combination of molecular docking, atomistic molecular dynamics simulations and machine learning techniques, to move beyond static 2D or 3D ligand descriptors and develop complex Quantitative Structure-Activity Relationship (QSAR) models which incorporate dynamics alongside shape and chemical selectivity.\\n\\nThe techniques employed will include an AI approach to ligand design including the use of domain-specific technologies such as DeepChem and more generic tools such as Keras and TensorFlow. From the chemical and biological side, synthetic retinoids predicted from the above work will be prepared and applied in a variety of assays for RAR activity such as transcriptional activity, non-genomic signalling via a variety of kinases and control of protein translation using cell lines and primary neural cells. A complete understanding of the ligand binding pocket of RAR and how different ligands may be designed to trigger different molecular pathways may have future potential for design of ligands for the treatment of neurodegenerative diseases. \\n\\nThe project is highly collaborative and interdisciplinary, involving a large and diverse consortium of researchers at different Universities, and a number of industrial partners, allowing all branches of the fundamental science of nuclear receptors to be addressed. The student will work on employing AI and modelling, designing new RAR activator ligands in collaboration with Coveney at UCL, drug target synthesis with Whiting at Durham, and biology with McCaffery and Greig at Aberdeen. The shape and properties of the designed drugs will be correlated with their biological function.\\n\\nFrom this project, the student will become familiar with AI based techniques (such as convolutional neural networks, random forest and support vector machines) to study and manipulate receptor proteins and understand more completely ligand activation of RAR and nuclear receptor function. \\n\\nReferences:\\n\\n1. Shearer KD, Stoney PN, Morgan PJ, McCaffery PJ. A vitamin for the brain. Trends Neurosci. 2012;35:733-41","4":"AD0473F7-2650-4127-BFD6-938B386E8183"},{"1":"2116078","2":"NPIF Application of Artificial Intelligence-Driven Design of Function-Directed Ligands for Selective Retinoic Acid Receptor Binding","3":"Retinoic acid (RA) ligands bind to the retinoic acid receptor (RAR) class of nuclear receptor. The shape and structure of RAR ligands though that optimally activate RARs is poorly understood and this project aims to model RARs to a degree not so far obtained to be able to design ligands that activate the receptors and understand the triggering routes for RARs for both genomic and non-genomic signalling. New approaches will be developed to these ends that will be applicable not just to RARs but to many members of the nuclear receptor class of receptors. \\n\\nWe are world-leaders in understanding the function of RA in the brain (1), making the discovery that several mechanisms by which RAR ligands act are crucial for their action: both genomic activity, to turn on gene transcription, and rapid non-genomic action, involving kinase activation.\\n\\nIn this project a radically different approach will be taken to ligand design, to modelling and to understanding binding selectivities to the different RARs. This will use a combination of molecular docking, atomistic molecular dynamics simulations and machine learning techniques, to move beyond static 2D or 3D ligand descriptors and develop complex Quantitative Structure-Activity Relationship (QSAR) models which incorporate dynamics alongside shape and chemical selectivity.\\n\\nThe techniques employed will include an AI approach to ligand design including the use of domain-specific technologies such as DeepChem and more generic tools such as Keras and TensorFlow. From the chemical and biological side, synthetic retinoids predicted from the above work will be prepared and applied in a variety of assays for RAR activity such as transcriptional activity, non-genomic signalling via a variety of kinases and control of protein translation using cell lines and primary neural cells. A complete understanding of the ligand binding pocket of RAR and how different ligands may be designed to trigger different molecular pathways may have future potential for design of ligands for the treatment of neurodegenerative diseases. \\n\\nThe project is highly collaborative and interdisciplinary, involving a large and diverse consortium of researchers at different Universities, and a number of industrial partners, allowing all branches of the fundamental science of nuclear receptors to be addressed. The student will work on employing AI and modelling, designing new RAR activator ligands in collaboration with Coveney at UCL, drug target synthesis with Whiting at Durham, and biology with McCaffery and Greig at Aberdeen. The shape and properties of the designed drugs will be correlated with their biological function.\\n\\nFrom this project, the student will become familiar with AI based techniques (such as convolutional neural networks, random forest and support vector machines) to study and manipulate receptor proteins and understand more completely ligand activation of RAR and nuclear receptor function. \\n\\nReferences:\\n\\n1. Shearer KD, Stoney PN, Morgan PJ, McCaffery PJ. A vitamin for the brain. Trends Neurosci. 2012;35:733-41","4":"8AF63BCC-D483-4F94-A72A-BC4FB8E2677F"},{"1":"EP/F006500/1","2":"D-SCENT: Raising challenges to deception attempts using data scent trails","3":"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.","4":"4CF9D715-408B-4AD6-87BC-6529E685514E"},{"1":"EP/F008562/1","2":"D-SCENT: Raising challenges to deception attempts using data scent trails","3":"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.","4":"F906DD95-20ED-4699-A8A6-7BD38C8DB5FB"},{"1":"EP/F014112/1","2":"D-SCENT: Raising challenges to deception attempts using data scent trails","3":"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.","4":"256B8523-5B5E-4ED8-B1EB-B1FDE7E9AB96"},{"1":"EP/F008686/1","2":"D-SCENT: Raising challenges to deception attempts using data scent trails","3":"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.","4":"BC828E2E-C339-4886-B8D3-C9782F3D6F8B"},{"1":"NE/J006629/1","2":"The role of natural and artificial pools in northern peatland carbon cycling","3":"Since the end of the last ice age large amounts of atmospheric carbon dioxide have been slowly locked up in peat soils in the cool, wet, northern regions of the world as partially decomposed plant remains. If this huge reservoir of carbon was to be released back in to the atmosphere it would cause a significant rise in carbon dioxide and release methane and result in further global warming. At the moment scientists are trying to unravel the mechanisms that control the losses and gains of carbon from this large area of the Earth's surface.\\n\\nPeatlands often have natural pools of water within them. However, we know little about what role these pools play in the peatland carbon cycle. Furthermore, many new pools are being created in peatlands by humans as they try to restore degraded peatlands. This is because many degraded peatlands have previously been drained using ditches. These ditches are now being blocked up with small dams thereby creating tens of thousands of new pools along the length of the blocked ditches.\\n\\nOur project seeks to examine the role of natural and artificial pools in the peatland carbon cycle. It will measure flows of water and carbon into and out of pools in order to understand whether the pools play an important role in processing the carbon.\\n\\nAdditionally it is known that many peatlands contain connected cavities within them known as soil pipes. These are not artificial pipes, but just natural connected holes within the peat. Some peatland pools which are connected to pipes may therefore be connected to large carbon sources from some distance away, but no-one has ever studied this before. \\n\\nIn summary, this project will investigate the role of pools in carbon cycling within northern peatlands. It will seek to understand processes operating in and around pools and the role of natural pipes in mediating these processes. It will focus on two pool types: natural pools and pools that have been created through peatland restoration by damming of drainage ditches. For both pool types we will examine those that are connected to pipes and those that are unconnected to pipes.\\n\\nThis work is fundamental to our understanding of the role of peatlands in the global carbon cycle. Without this research we will be unable to properly predict carbon fluxes from peatlands under climate change scenarios because we will not understand what drives carbon cycling in peatland pools. The work is urgent: without it we will be unable to explain how the large number of pools currently being created by practitioners via drain blocking is affecting carbon cycling within, and carbon release from, peatlands. The results from this project are also vital to informing the future management and restoration of peatlands in order to optimise their potential to mitigate global warming.","4":"99E0891B-9CDA-46D5-9735-37D96864DAC8"},{"1":"NE/J007609/1","2":"The role of natural and artificial pools in northern peatland carbon cycling","3":"Since the end of the last ice age large amounts of atmospheric carbon dioxide have been slowly locked up in peat soils in the cool, wet, northern regions of the world as partially decomposed plant remains. If this huge reservoir of carbon was to be released back in to the atmosphere it would cause a significant rise in carbon dioxide and release methane and result in further global warming. At the moment scientists are trying to unravel the mechanisms that control the losses and gains of carbon from this large area of the Earth's surface.\\n\\nPeatlands often have natural pools of water within them. However, we know little about what role these pools play in the peatland carbon cycle. Furthermore, many new pools are being created in peatlands by humans as they try to restore degraded peatlands. This is because many degraded peatlands have previously been drained using ditches. These ditches are now being blocked up with small dams thereby creating tens of thousands of new pools along the length of the blocked ditches.\\n\\nOur project seeks to examine the role of natural and artificial pools in the peatland carbon cycle. It will measure flows of water and carbon into and out of pools in order to understand whether the pools play an important role in processing the carbon.\\n\\nAdditionally it is known that many peatlands contain connected cavities within them known as soil pipes. These are not artificial pipes, but just natural connected holes within the peat. Some peatland pools which are connected to pipes may therefore be connected to large carbon sources from some distance away, but no-one has ever studied this before. \\n\\nIn summary, this project will investigate the role of pools in carbon cycling within northern peatlands. It will seek to understand processes operating in and around pools and the role of natural pipes in mediating these processes. It will focus on two pool types: natural pools and pools that have been created through peatland restoration by damming of drainage ditches. For both pool types we will examine those that are connected to pipes and those that are unconnected to pipes.\\n\\nThis work is fundamental to our understanding of the role of peatlands in the global carbon cycle. Without this research we will be unable to properly predict carbon fluxes from peatlands under climate change scenarios because we will not understand what drives carbon cycling in peatland pools. The work is urgent: without it we will be unable to explain how the large number of pools currently being created by practitioners via drain blocking is affecting carbon cycling within, and carbon release from, peatlands. The results from this project are also vital to informing the future management and restoration of peatlands in order to optimise their potential to mitigate global warming.","4":"DA9A0967-C7E3-4B5E-A040-5F4EF5BFA8C7"},{"1":"NE/S005560/1","2":"FUSED - Functionality of Urban Soils supporting Ecosystem service Delivery","3":"Soil ecosystems provide critical ecosystem services that underpin human societies and wellbeing. Among others, these include: nutrient cycling, carbon sequestration, waste detoxification, and supporting primary productivity. The delivery of these ecosystem services is dependent upon the biodiversity contained within the soil, and the ecosystem processes and functions it regulates. A positive relationship between biodiversity and ecosystem processes/function has been well documented in natural and agricultural soils, and importantly has been demonstrated to change following even very small or localised modifications to the soils environment. Thus modifications to soils that change the biodiversity present, will alter the delivery of ecosystem services on which humanity depends. However, a large amount of soil within the UK (and N. Europe) is contained within urban environments, and despite these comprising a huge variety of different soils types with a vast range of internal modifications, we know practically nothing about the biodiversity contained in these urban soils, the relationships between this and the ecosystem processes/functions it supports, and how this links to the delivery of key ecosystem services. \\n\\nOur project (FUSED) will address all these knowledge gaps, by examining the links between the physical and chemical structure of urban soil, the biodiversity it contains, the ecosystem processes and functions this supports and the delivery of four key ecosystem services (nutrient cycling, carbon sequestration, waste detoxification, and primary productivity). To achieve this we will focus on existing gradients of urbanisation in SE England. This region comprises areas of long-term industrial use, areas redeveloped upon an industrial legacy, green spaces, new urban developments, alongside urban conurbations ranging from small villages with low human population densities, to high-density cities, and with smaller areas of pristine habitat and large swaths of agricultural land - providing the full catalogue of urban soil types, from entirely natural, those modified for agriculture and urban green space, to Technosols formed from artificial material and soils with sealed (e.g. under paving/roads) surfaces. Importantly this region, as a historical and contemporary focus of urban development, provides a window into the future of what the wider UK landscape will look like and the novel urban ecosystems it will contain. \\n\\nFirst we will identify generalisations by examining how the environmental context (degree of urbanisation, combined with contemporary and historical land-use, alongside urban soil type present) the urban soil is present in influences biodiversity, ecosystem processes/functions and ecosystem service delivery. Building upon this, we will establish a series of experimental manipulations that allow examination of how resilient and resistance urban soil biodiversity and associated ecosystem processes/functions are to modification and changes in environmental context. This combined, with tracking urban soil ecosystem through time, will provide a unique understanding of the ecological stability of these systems. Finally, we will integrate all collected data into a modelling framework that will provide a mechanistic understanding of factors underpinning the responses of soil biodiversity, ecosystem processes/functions and delivery of ecosystem services to current and future modifications.","4":"273FEBA8-E821-4AB1-884D-342A148AB3B1"},{"1":"NE/S004920/1","2":"FUSED-Functionality of Urban Soils supporting Ecosystem service Delivery","3":"Soil ecosystems provide critical ecosystem services that underpin human societies and wellbeing. Among others, these include: nutrient cycling, carbon sequestration, waste detoxification, and supporting primary productivity. The delivery of these ecosystem services is dependent upon the biodiversity contained within the soil, and the ecosystem processes and functions it regulates. A positive relationship between biodiversity and ecosystem processes/function has been well documented in natural and agricultural soils, and importantly has been demonstrated to change following even very small or localised modifications to the soils environment. Thus modifications to soils that change the biodiversity present, will alter the delivery of ecosystem services on which humanity depends. However, a large amount of soil within the UK (and N. Europe) is contained within urban environments, and despite these comprising a huge variety of different soils types with a vast range of internal modifications, we know practically nothing about the biodiversity contained in these urban soils, the relationships between this and the ecosystem processes/functions it supports, and how this links to the delivery of key ecosystem services. \\n\\nOur project (FUSED) will address all these knowledge gaps, by examining the links between the physical and chemical structure of urban soil, the biodiversity it contains, the ecosystem processes and functions this supports and the delivery of four key ecosystem services (nutrient cycling, carbon sequestration, waste detoxification, and primary productivity). To achieve this we will focus on existing gradients of urbanisation in SE England. This region comprises areas of long-term industrial use, areas redeveloped upon an industrial legacy, green spaces, new urban developments, alongside urban conurbations ranging from small villages with low human population densities, to high-density cities, and with smaller areas of pristine habitat and large swaths of agricultural land - providing the full catalogue of urban soil types, from entirely natural, those modified for agriculture and urban green space, to Technosols formed from artificial material and soils with sealed (e.g. under paving/roads) surfaces. Importantly this region, as a historical and contemporary focus of urban development, provides a window into the future of what the wider UK landscape will look like and the novel urban ecosystems it will contain. \\n\\nFirst we will identify generalisations by examining how the environmental context (degree of urbanisation, combined with contemporary and historical land-use, alongside urban soil type present) the urban soil is present in influences biodiversity, ecosystem processes/functions and ecosystem service delivery. Building upon this, we will establish a series of experimental manipulations that allow examination of how resilient and resistance urban soil biodiversity and associated ecosystem processes/functions are to modification and changes in environmental context. This combined, with tracking urban soil ecosystem through time, will provide a unique understanding of the ecological stability of these systems. Finally, we will integrate all collected data into a modelling framework that will provide a mechanistic understanding of factors underpinning the responses of soil biodiversity, ecosystem processes/functions and delivery of ecosystem services to current and future modifications.","4":"261F4F3F-A24D-48CF-B0AD-295C290795A4"},{"1":"10047343","2":"Achieving Ecological Resilient Dynamism for the European food system through consumer-driven policies, socio-ecological challenges, biodiversity, data-driven policy, sustainable futures","3":"The ECO-READY project will develop a real-time surveillance system, an Observatory offered as an e-platform and as a mobile application. This will function as the necessary singular source of information, provide real-time assessments for the food system, and update forecasts frequently and consistently. The Observatory will be available to society, policymakers, the scientific community, and the agri-food industry, and integrated with a network of 10 Living Labs, supported through the third party funding process, covering all bioclimatic regions in Europe, forming the ECO-READY project knowledge infrastructure. ECO-READY will produce knowledge based resilience strategies, and develop tools that will be embedded on the Observatory. The underlining principle behind the ECO READY approach is, resilient dynamism, or tackling immediate problems and long-term challenges at the same time. The Living Labs network will facilitate ‘concept to action’ through the co-creation of scenarios addressing their regional needs, the development of policy recommendations, contingency plans, and resilience strategies, and embed them on the Observatory. Furthermore, ECO-READY will develop an early warning system and decision support tools using innovative Artificial Intelligence based on holistic prediction models and Life Cycle Assessment results. ECO-READY will ensure that European farmers and society’s interests be reflected in future policy making and monitoring, through early-stage active engagement incorporating bottom-up recommendations, facilitated by the increased usership of the digital tools developed, and resulting in increased awareness for climate-adaptive and mitigating agri-food products. Furthermore, the Observatory smart application will include tools that will empower the citizens to actively engage in policy making, and interact directly with the scientific community, farmers, and industry and policy makers, thus driving change in consumption habits.","4":"8AAE57A1-FB3C-4CD6-8D38-8E6B4C1D2E76"},{"1":"10064558","2":"ECO-READY: Achieving Ecological Resilient Dynamism for the European food system through consumer-driven policies, socio-ecological challenges, biodiversity, data-driven policy, sustainable futures","3":"The ECO-READY project will develop a real-time surveillance system, an Observatory offered as an e-platform and as a mobile application. This will function as the necessary singular source of information, provide real-time assessments for the food system, and update forecasts frequently and consistently. The Observatory will be available to society, policymakers, the scientific community, and the agri-food industry, and integrated with a network of 10 Living Labs, supported through the third party funding process, covering all bioclimatic regions in Europe, forming the ECO-READY project knowledge infrastructure. ECO-READY will produce knowledge based resilience strategies, and develop tools that will be embedded on the Observatory. The underlining principle behind the ECO READY approach is, resilient dynamism, or tackling immediate problems and long-term challenges at the same time. The Living Labs network will facilitate ‘concept to action’ through the co-creation of scenarios addressing their regional needs, the development of policy recommendations, contingency plans, and resilience strategies, and embed them on the Observatory. Furthermore, ECO-READY will develop an early warning system and decision support tools using innovative Artificial Intelligence based on holistic prediction models and Life Cycle Assessment results. ECO-READY will ensure that European farmers and society’s interests be reflected in future policy making and monitoring, through early-stage active engagement incorporating bottom-up recommendations, facilitated by the increased usership of the digital tools developed, and resulting in increased awareness for climate-adaptive and mitigating agri-food products. Furthermore, the Observatory smart application will include tools that will empower the citizens to actively engage in policy making, and interact directly with the scientific community, farmers, and industry and policy makers, thus driving change in consumption habits.","4":"E1498DDE-6C71-465A-9BE9-7C9672DEFCF7"},{"1":"EP/X030210/1","2":"MIcroelectronics RELiability driven by Artificial Intelligence","3":"The European microelectronics (ME) industry has a direct impact on approximately 20% of the European GDP and employs over 250,000 people, with more than 64,000 job vacancies. The main technological challenges are 1) to increase both the reliability of the manufactured electronic components and systems (ECS) and sustainability to meet the requirements of the new EU directive Right to Repair, and 2) to reduce the huge product verification effort (70% of total product development time) that represents a substantial \\nburden on costs and resources. To compete with China and North America, the European ME industry is in critical need of crossdiscipline experts in electronic manufacturing and digital innovations: software, data and artificial intelligence. \\n\\nThe main goal of the MIRELAI industrial doctorate programme is to train the next generation of engineers and scientists for the next generation of reliable and repairable ECS developed within a trustworthy European value chain. The scientific approach is based on three pillars: 1) physics of degradation, 2) multi-scale modelling and 3) AI-based reliability which in sum will add up to a reinvention and new level of efficiency for the 'design for repair and reliability' for ECS. \\n\\nOur unique industry-academia partnership of 8 industries, 4 SMEs and 9 research organisations and academic institutions from 7 European countries has all the expertise, experience and capacity along the electronics system value chain to deliver this ambitious research and training programme. Shared hosting and joint supervision by industry and academia of each of the 13 doctoral candidates ensures optimal knowledge transfer. Together, we will pave the way for sustainable, repairable and energy efficient electronic system designs and resource-friendly smart electronics applications. MIRELAI is thus perfectly aligned with the European research agenda and the EU Pact for Skills, a shared engagement model for skills development in Europe.","4":"45B7F6B9-E6AE-40A5-8AE5-64117167F0FD"},{"1":"EP/X030148/1","2":"MIcroelectronics RELiability driven by Artificial Intelligence","3":"The European microelectronics (ME) industry has a direct impact on approximately 20% of the European GDP and employs over 250,000 people, with more than 64,000 job vacancies. The main technological challenges are 1) to increase both the reliability of the manufactured electronic components and systems (ECS) and sustainability to meet the requirements of the new EU directive Right to Repair, and 2) to reduce the huge product verification effort (70% of total product development time) that represents a substantial \\nburden on costs and resources. To compete with China and North America, the European ME industry is in critical need of crossdiscipline experts in electronic manufacturing and digital innovations: software, data and artificial intelligence. \\n\\nThe main goal of the MIRELAI industrial doctorate programme is to train the next generation of engineers and scientists for the next generation of reliable and repairable ECS developed within a trustworthy European value chain. The scientific approach is based on three pillars: 1) physics of degradation, 2) multi-scale modelling and 3) AI-based reliability which in sum will add up to a reinvention and new level of efficiency for the 'design for repair and reliability' for ECS. \\n\\nOur unique industry-academia partnership of 8 industries, 4 SMEs and 9 research organisations and academic institutions from 7 European countries has all the expertise, experience and capacity along the electronics system value chain to deliver this ambitious research and training programme. Shared hosting and joint supervision by industry and academia of each of the 13 doctoral candidates ensures optimal knowledge transfer. Together, we will pave the way for sustainable, repairable and energy efficient electronic system designs and resource-friendly smart electronics applications. MIRELAI is thus perfectly aligned with the European research agenda and the EU Pact for Skills, a shared engagement model for skills development in Europe.","4":"111152CA-04E3-479E-8278-92F7EEB037AA"},{"1":"PP/E000940/1","2":"Seismic studies of the global Sun with BiSON","3":"The Sun is vital to life on Earth. Our research aims are to 'look' deep inside the Sun to help us understand how it works. Although you can't see it without special equipment, the Sun is shaking. Bubbling motion just under the visible surface of the Sun is constantly feeding in energy, and the Sun responds by vibrating just like a (very) large musical instrument. The 'notes' of the Sun's music are very interesting to astronomers. They are produced by sound waves which have travelled deep inside and their frequencies (the 'pitches' of the notes) depend on the conditions they meet on the way. For example, the frequencies depend on the density and temperature of the material inside the Sun and studying them allows astronomers to essentially get an 'ultrasound scan' of our nearest star. Actually turning these ideas into scientific reality is very difficult. The shaking of the Sun's surface is very small - it moves to and fro at a couple of metres per second (a slow walking pace), and takes about 5 minutes to go through a cycle. Just detecting this small movement takes specially designed instruments (called spectrometers, because they analyse the spectrum of sunlight). Some interesting effects on the Sun can last for several years. For example, there is a cycle of magnetic activity (which we don't fully understand yet) which makes the Sun have periods of 'spottiness' every 11 years or so. For this and other reasons, you would really like to observe the Sun's vibrations all the time over many years. One way of doing this is to have a number of robotic spectrometers in different sunny locations around the world, so that when the Sun sets on one instrument it has already risen on another. The Birmingham Solar Oscillations Network (BiSON) is just such a 'network' of spectrometers - there are 6 instruments in good sites for solar viewing in the Americas, Australia, Tenerife and South Africa. We designed and built these dedicated instruments, and we maintain and update them with local support and by visits by team members. The data are returned to our base in Birmingham via the Internet, and the remote link also lets us monitor the performance of the instruments and upgrade some of the software. Results from BiSON have been very important in helping astronomers to understand the Sun. The spectrum of the Sun's deepest vibrations was first detected by BiSON, and we continue to provide and analyse data which can help us to improve our understanding of everything from what makes the vibrations in the first place, all the way to information about the nuclear reactor at the Sun's core. Astronomers are beginning to measure the interiors of other stars, and our work on the Sun provides an important reference point for this new work. The effects we study are very subtle, and we spend much of our effort in analysing the data. To confirm that our results are 'real', we compare our results with those obtained by the small number of other astronomers who have similar equipment, and we also test our analysis programs on sophisticated artificial data sets which mimic as closely as we can the real thing. The beauty of simulated data is that you know exactly what went into them, so if you run your analysis programs you know what the output should look like. We have used simulated data for some years now, but as we have learned more about our instruments, we now know that there are several subtle effects which occur in real data that we've not yet built into our simulation programs. So our new proposal is to continue to maintain the BiSON network and collect high quality data. We will use these and other data to provide new information on the deep interior of the Sun (and inform the study of other stars), improve our understanding of the solar cycle and of the outer convective layers of the Sun where the vibrations are generated.","4":"BFE97A70-2060-4D32-A4B6-4F008C8EA3D0"},{"1":"PP/E001084/1","2":"Seismic Studies of the global Sun with BiSON","3":"The Sun is vital to life on Earth. Our research aims are to 'look' deep inside the Sun to help us understand how it works. Although you can't see it without special equipment, the Sun is shaking. Bubbling motion just under the visible surface of the Sun is constantly feeding in energy, and the Sun responds by vibrating just like a (very) large musical instrument. The 'notes' of the Sun's music are very interesting to astronomers. They are produced by sound waves which have travelled deep inside and their frequencies (the 'pitches' of the notes) depend on the conditions they meet on the way. For example, the frequencies depend on the density and temperature of the material inside the Sun and studying them allows astronomers to essentially get an 'ultrasound scan' of our nearest star. Actually turning these ideas into scientific reality is very difficult. The shaking of the Sun's surface is very small - it moves to and fro at a couple of metres per second (a slow walking pace), and takes about 5 minutes to go through a cycle. Just detecting this small movement takes specially designed instruments (called spectrometers, because they analyse the spectrum of sunlight). Some interesting effects on the Sun can last for several years. For example, there is a cycle of magnetic activity (which we don't fully understand yet) which makes the Sun have periods of 'spottiness' every 11 years or so. For this and other reasons, you would really like to observe the Sun's vibrations all the time over many years. One way of doing this is to have a number of robotic spectrometers in different sunny locations around the world, so that when the Sun sets on one instrument it has already risen on another. The Birmingham Solar Oscillations Network (BiSON) is just such a 'network' of spectrometers - there are 6 instruments in good sites for solar viewing in the Americas, Australia, Tenerife and South Africa. We designed and built these dedicated instruments, and we maintain and update them with local support and by visits by team members. The data are returned to our base in Birmingham via the Internet, and the remote link also lets us monitor the performance of the instruments and upgrade some of the software. Results from BiSON have been very important in helping astronomers to understand the Sun. The spectrum of the Sun's deepest vibrations was first detected by BiSON, and we continue to provide and analyse data which can help us to improve our understanding of everything from what makes the vibrations in the first place, all the way to information about the nuclear reactor at the Sun's core. Astronomers are beginning to measure the interiors of other stars, and our work on the Sun provides an important reference point for this new work. The effects we study are very subtle, and we spend much of our effort in analysing the data. To confirm that our results are 'real', we compare our results with those obtained by the small number of other astronomers who have similar equipment, and we also test our analysis programs on sophisticated artificial data sets which mimic as closely as we can the real thing. The beauty of simulated data is that you know exactly what went into them, so if you run your analysis programs you know what the output should look like. We have used simulated data for some years now, but as we have learned more about our instruments, we now know that there are several subtle effects which occur in real data that we've not yet built into our simulation programs. So our new proposal is to continue to maintain the BiSON network and collect high quality data. We will use these and other data to provide new information on the deep interior of the Sun (and inform the study of other stars), improve our understanding of the solar cycle and of the outer convective layers of the Sun where the vibrations are generated.","4":"4F63C826-055B-456D-BAAB-73922DC13EB9"},{"1":"10023490","2":"A.I. forecasting to reduce food waste - COVID-19 impact","3":"The UK hospitality industry generates over 1 million tonnes of food waste each year, equivalent to approximately &pound;3 billion in cost and over 4.5 million tonnes of CO2\\\\. At the same time, UK restaurants yield a meager 3-5% average profit margin, making it one of the least profitable industries in the country. This economic struggle has been dramatically exacerbated by the COVID crisis. Through effective food waste reduction, restaurants can boost profitability by up to 2pp, while drastically reducing the environmental impact of their operations.\\n\\nThe key to reducing food waste lies in accurate demand forecasting. Restaurants order their perishable inventory days and weeks before selling dishes to their customers. Given that most restaurants rely on rigid 4-week demand averages and gut instinct to make their procurement decisions, food orders are routinely in excess of real demand - creating food waste.\\n\\nTenzo will research &amp; develop a cutting-edge forecasting tool, which will allow restaurant businesses to accurately forecast customer demand. Utilising powerful artificial intelligence algorithms, the tool will achieve the following objectives:\\n\\n* Generate accurate restaurant sales forecasts based on historical sales, weather, public events and other features\\n* Split daily forecasts into item-level and hourly projections\\n* Provide those forecasts to frontline workers (chefs, section managers, etc.) within a user-friendly mobile app - enabling them to know easily how much food to:\\n * prepare and when (e.g. how much chicken to grill every hour); and\\n * order and when (e.g. how much fish to order for next week).\\n\\nTenzo's project will focus on finding the most accurate forecasting algorithms and combining them with a user-friendly software interface to ensure frontline workers are empowered to reduce food waste in their day-to-day operations. By 2024, our tool could reduce annual UK hospitality food waste by over 38,000 tonnes, CO2 by over 175,000 tonnes and restaurant costs by over &pound;20 million.","4":"000D271E-24B5-4A35-A555-3D5392531460"},{"1":"73755","2":"A.I. forecasting to reduce food waste","3":"The UK hospitality industry generates over 1 million tonnes of food waste each year, equivalent to approximately &pound;3 billion in cost and over 4.5 million tonnes of CO2\\\\. At the same time, UK restaurants yield a meager 3-5% average profit margin, making it one of the least profitable industries in the country. This economic struggle has been dramatically exacerbated by the COVID crisis. Through effective food waste reduction, restaurants can boost profitability by up to 2pp, while drastically reducing the environmental impact of their operations.\\n\\nThe key to reducing food waste lies in accurate demand forecasting. Restaurants order their perishable inventory days and weeks before selling dishes to their customers. Given that most restaurants rely on rigid 4-week demand averages and gut instinct to make their procurement decisions, food orders are routinely in excess of real demand - creating food waste.\\n\\nTenzo will research &amp; develop a cutting-edge forecasting tool, which will allow restaurant businesses to accurately forecast customer demand. Utilising powerful artificial intelligence algorithms, the tool will achieve the following objectives:\\n\\n* Generate accurate restaurant sales forecasts based on historical sales, weather, public events and other features\\n* Split daily forecasts into item-level and hourly projections\\n* Provide those forecasts to frontline workers (chefs, section managers, etc.) within a user-friendly mobile app - enabling them to know easily how much food to:\\n * prepare and when (e.g. how much chicken to grill every hour); and\\n * order and when (e.g. how much fish to order for next week).\\n\\nTenzo's project will focus on finding the most accurate forecasting algorithms and combining them with a user-friendly software interface to ensure frontline workers are empowered to reduce food waste in their day-to-day operations. By 2024, our tool could reduce annual UK hospitality food waste by over 38,000 tonnes, CO2 by over 175,000 tonnes and restaurant costs by over &pound;20 million.","4":"139883DA-2A7E-409E-BCAF-610A0360BAA6"},{"1":"10050243","2":"PIANOFORTE: Partnership for European research in radiation protection and detection of ionising radiation : towards a safer use and improved protection of the environment and human health.","3":"The ambition of the PIANOFORTE Partnership is to improve radiological protection of members of the public, patients, workers and environment in all exposure scenarios and provide solutions and recommendations for optimised protection in accordance with the Basic Safety Standards. Research projects focusing on identified research and innovation priorities will be selected through a serie of three competitive open calls. The input to define the research priorities will be based on the priorities defined in the Joint Road Map (JRM) developed during the H2020 CONCERT EJP but also on the results of ongoing H2020 projects and on the expectations expressed by other actions carried out in other European programmes, in particular the SAMIRA action plan. High priority will be dedicated to medical applications considering that 1) medical exposures are, by far, the largest artificial source of exposure of the European population and 2) the fight against cancer is a top priority of the present European Commission. In order to ensure an appropriate continuity in the research goals and methodologies, in line with the contents of the CONCERT JRM, two other priorities have been identified to further understand and reduce uncertainties associated with health risk estimates for exposure at low doses in order to consolidate regulations and improve practices and to further enhance a science-based European methodology for emergency management and long-term recovery. Once the research priorities defined, the open call system will promote excellence in science and widening participation through a process open to the whole radiation protection community. Beyond the research actions, the selected projects will be able to benefit from the system of sharing and mutualisation of infrastructures that will be implemented at the European level. This will be accompanied by education and training schemes for health workforce and young scientists to increase Europe’s research capacity in the field.","4":"6805887C-C198-466A-9C18-5F3C03F4031E"},{"1":"10064809","2":"Partnership for european research in radiation protection and detection of ionising radiation : towards a safer use and improved protection of the environment and human health.","3":"The ambition of the PIANOFORTE Partnership is to improve radiological protection of members of the public, patients, workers and environment in all exposure scenarios and provide solutions and recommendations for optimised protection in accordance with the Basic Safety Standards. Research projects focusing on identified research and innovation priorities will be selected through a serie of three competitive open calls. The input to define the research priorities will be based on the priorities defined in the Joint Road Map (JRM) developed during the H2020 CONCERT EJP but also on the results of ongoing H2020 projects and on the expectations expressed by other actions carried out in other European programmes, in particular the SAMIRA action plan. High priority will be dedicated to medical applications considering that 1) medical exposures are, by far, the largest artificial source of exposure of the European population and 2) the fight against cancer is a top priority of the present European Commission. In order to ensure an appropriate continuity in the research goals and methodologies, in line with the contents of the CONCERT JRM, two other priorities have been identified to further understand and reduce uncertainties associated with health risk estimates for exposure at low doses in order to consolidate regulations and improve practices and to further enhance a science-based European methodology for emergency management and long-term recovery. Once the research priorities defined, the open call system will promote excellence in science and widening participation through a process open to the whole radiation protection community. Beyond the research actions, the selected projects will be able to benefit from the system of sharing and mutualisation of infrastructures that will be implemented at the European level. This will be accompanied by education and training schemes for health workforce and young scientists to increase Europe’s research capacity in the field.","4":"7DD97815-1CA6-41A4-BC41-2A6D5BCEA8FB"},{"1":"10063804","2":"Partnership for European research in radiation protection and detection of ionising radiation: PIANOFORTE","3":"The ambition of the PIANOFORTE Partnership is to improve radiological protection of members of the public, patients, workers and environment in all exposure scenarios and provide solutions and recommendations for optimised protection in accordance with the Basic Safety Standards. Research projects focusing on identified research and innovation priorities will be selected through a serie of three competitive open calls. The input to define the research priorities will be based on the priorities defined in the Joint Road Map (JRM) developed during the H2020 CONCERT EJP but also on the results of ongoing H2020 projects and on the expectations expressed by other actions carried out in other European programmes, in particular the SAMIRA action plan. High priority will be dedicated to medical applications considering that 1) medical exposures are, by far, the largest artificial source of exposure of the European population and 2) the fight against cancer is a top priority of the present European Commission. In order to ensure an appropriate continuity in the research goals and methodologies, in line with the contents of the CONCERT JRM, two other priorities have been identified to further understand and reduce uncertainties associated with health risk estimates for exposure at low doses in order to consolidate regulations and improve practices and to further enhance a science-based European methodology for emergency management and long-term recovery. Once the research priorities defined, the open call system will promote excellence in science and widening participation through a process open to the whole radiation protection community. Beyond the research actions, the selected projects will be able to benefit from the system of sharing and mutualisation of infrastructures that will be implemented at the European level. This will be accompanied by education and training schemes for health workforce and young scientists to increase Europe’s research capacity in the field.","4":"B996E96F-E638-46C3-9172-832CDA94EBF8"},{"1":"2605902","2":"Statistical methods for ovarian cancer diagnosis and prognosis","3":"The development of high-throughput sequencing technologies has led to the production of large-scale profiling data; allowing us to gain insight into underlying biological processes. Available at different levels, sequencing allows us to collect data about DNA, RNA, proteins, metabolites and so forth, providing complementary information when characterising a biological object. Individually each source of data, referred to as omics data, characterises a specific part of an organism. For instance, genomics based at the DNA level characterises the genome, whilst transcriptomics, based at the RNA level characterises the transcriptome. Notably, each level is related to one another, for instance, mRNA is translated to proteins, driving the behaviour of cells thereby leading to the expression of phenotypes. Due to the high-dimensionality and heterogeneity within omics datasets, the analysis is ripe with statistical challenges. \\n\\nThroughout my PhD I will be working on novel statistical methods tackling the issues of dimensionality reduction and variable selection for omics datasets, both in supervised and unsupervised settings. One such method, currently under development, is a high-dimensional Bayesian survival analysis model that uses a spike-and-slab prior. Our method enables us to perform variable selection in a high-dimensional setting, whilst also offering mechanisms for uncertainty quantification and effect estimation. Within the biomedical sciences, survival analysis is a task of key importance, and when performed with transcriptomics data enables the creation of prognostic model and the discovery of biomarkers.\\n \\nA second aspect of my PhD will focus on the development of methodology for data-integration. Where data-integration involves the joint analysis of multiple datasets with the goal of understanding the relationships between them. Motivated by our collaborators at Imperial's CRUK centre, we will be applying these methods to radiomics data (image features constructed from medical images), and other omics datasets collected from patients with ovarian cancer. Thereby, providing biological interpretations to affordable and easy to collect (CT/MRI) scans. Currently, we are considering extending the probabilistic framing of canonical correlation analysis. Such extensions will enable these methods to work in a high-dimensional setting and simultaneously provide uncertainty quantification.\\n \\nAligning with EPSRC strategies in artificial intelligence and healthcare, the proposed methodological developments seek to improve health services by optimising patient treatments. Ultimately, the focus of my PhD is based on data from patients with ovarian cancer, however the general applicability of biologically relevant methods extends beyond single disease.","4":"7AAC9A63-1CA2-4A16-BEC7-5B34BB7AF2D4"},{"1":"2632835","2":"Statistical methods for ovarian cancer diagnosis and prognosis","3":"The development of high-throughput sequencing technologies has led to the production of large-scale profiling data; allowing us to gain insight into underlying biological processes. Available at different levels, sequencing allows us to collect data about DNA, RNA, proteins, metabolites and so forth, providing complementary information when characterising a biological object. Individually each source of data, referred to as omics data, characterises a specific part of an organism. For instance, genomics based at the DNA level characterises the genome, whilst transcriptomics, based at the RNA level characterises the transcriptome. Notably, each level is related to one another, for instance, mRNA is translated to proteins, driving the behaviour of cells thereby leading to the expression of phenotypes. Due to the high-dimensionality and heterogeneity within omics datasets, the analysis is ripe with statistical challenges. \\n\\nThroughout my PhD I will be working on novel statistical methods tackling the issues of dimensionality reduction and variable selection for omics datasets, both in supervised and unsupervised settings. One such method, currently under development, is a high-dimensional Bayesian survival analysis model that uses a spike-and-slab prior. Our method enables us to perform variable selection in a high-dimensional setting, whilst also offering mechanisms for uncertainty quantification and effect estimation. Within the biomedical sciences, survival analysis is a task of key importance, and when performed with transcriptomics data enables the creation of prognostic model and the discovery of biomarkers.\\n \\nA second aspect of my PhD will focus on the development of methodology for data-integration. Where data-integration involves the joint analysis of multiple datasets with the goal of understanding the relationships between them. Motivated by our collaborators at Imperial's CRUK centre, we will be applying these methods to radiomics data (image features constructed from medical images), and other omics datasets collected from patients with ovarian cancer. Thereby, providing biological interpretations to affordable and easy to collect (CT/MRI) scans. Currently, we are considering extending the probabilistic framing of canonical correlation analysis. Such extensions will enable these methods to work in a high-dimensional setting and simultaneously provide uncertainty quantification.\\n \\nAligning with EPSRC strategies in artificial intelligence and healthcare, the proposed methodological developments seek to improve health services by optimising patient treatments. Ultimately, the focus of my PhD is based on data from patients with ovarian cancer, however the general applicability of biologically relevant methods extends beyond single disease.","4":"E6F0B51C-15F1-4E03-BEBC-1EB2449726A7"},{"1":"10059948","2":"Systematic and orchestrated deployment of safety solutions in complex urban environments for ageing and vulnerable societies","3":"The emergence of complex urban mobility environments where unknown interactions between different types of VRUs and between VRUs and motorised vehicles poses the need for a clear understanding of user behaviours, fair and optimised use of public spaces as well as age friendly urban safety action plans and assessments, capitalising on the benefits that technological innovations and the plethora of available data can offer in advanced accident analysis, towards achieving EU’s ‘Vision Zero’ for zero fatalities in road transport by 2050. SOTERIA aims to accelerate the attainment of this Vision for vulnerable road users through a holistic framework of innovative models, tools and services that enable data driven urban safety intelligence, facilitate safe travelling of VRUs and foster the safe integration of micro-mobility services in complex environments. At the operational level SOTERIA uncovers unexplored behavioural characteristics of VRUs and engages Living Lab communities in social innovation activities for the co-creation of urban safety solutions and infrastructure designs. Simulation models and explainable AI driven analytics are developed for supporting policy decisions and informing interconnected services that support VRUs in safe and clean travelling. On-vehicle sensors and connectivity is fostered enabling minimisation of risky situations and behaviours. The approach is validated in four thematic demonstrations and through the SOTERIA network of 8 cities, addressing different types of VRUs.","4":"E873AECE-FEFC-49E0-89D7-76C2AA21A863"},{"1":"10052969","2":"Systematic and orchestrated deployment of safety solutions in complex urban environments for ageing and vulnerable societies","3":"The emergence of complex urban mobility environments where unknown interactions between different types of VRUs and between VRUs and motorised vehicles poses the need for a clear understanding of user behaviours, fair and optimised use of public spaces as well as age friendly urban safety action plans and assessments, capitalising on the benefits that technological innovations and the plethora of available data can offer in advanced accident analysis, towards achieving EU’s ‘Vision Zero’ for zero fatalities in road transport by 2050. SOTERIA aims to accelerate the attainment of this Vision for vulnerable road users through a holistic framework of innovative models, tools and services that enable data driven urban safety intelligence, facilitate safe travelling of VRUs and foster the safe integration of micro-mobility services in complex environments. At the operational level SOTERIA uncovers unexplored behavioural characteristics of VRUs and engages Living Lab communities in social innovation activities for the co-creation of urban safety solutions and infrastructure designs. Simulation models and explainable AI driven analytics are developed for supporting policy decisions and informing interconnected services that support VRUs in safe and clean travelling. On-vehicle sensors and connectivity is fostered enabling minimisation of risky situations and behaviours. The approach is validated in four thematic demonstrations and through the SOTERIA network of 8 cities, addressing different types of VRUs.","4":"D80B2345-6ADA-4495-94BD-8DFE0CEE811C"},{"1":"10058247","2":"Systematic and orchestrated deployment of safety solutions in complex urban environments for ageing and vulnerable societies","3":"The emergence of complex urban mobility environments where unknown interactions between different types of VRUs and between VRUs and motorised vehicles poses the need for a clear understanding of user behaviours, fair and optimised use of public spaces as well as age friendly urban safety action plans and assessments, capitalising on the benefits that technological innovations and the plethora of available data can offer in advanced accident analysis, towards achieving EU’s ‘Vision Zero’ for zero fatalities in road transport by 2050. SOTERIA aims to accelerate the attainment of this Vision for vulnerable road users through a holistic framework of innovative models, tools and services that enable data driven urban safety intelligence, facilitate safe travelling of VRUs and foster the safe integration of micro-mobility services in complex environments. At the operational level SOTERIA uncovers unexplored behavioural characteristics of VRUs and engages Living Lab communities in social innovation activities for the co-creation of urban safety solutions and infrastructure designs. Simulation models and explainable AI driven analytics are developed for supporting policy decisions and informing interconnected services that support VRUs in safe and clean travelling. On-vehicle sensors and connectivity is fostered enabling minimisation of risky situations and behaviours. The approach is validated in four thematic demonstrations and through the SOTERIA network of 8 cities, addressing different types of VRUs.","4":"8E03B802-9E2F-422D-B0EC-CB8F09B3D1A0"},{"1":"10064506","2":"Systematic and orchestrated deployment of safety solutions in complex urban environments for ageing and vulnerable societies","3":"The emergence of complex urban mobility environments where unknown interactions between different types of VRUs and between VRUs and motorised vehicles poses the need for a clear understanding of user behaviours, fair and optimised use of public spaces as well as age friendly urban safety action plans and assessments, capitalising on the benefits that technological innovations and the plethora of available data can offer in advanced accident analysis, towards achieving EU’s ‘Vision Zero’ for zero fatalities in road transport by 2050. SOTERIA aims to accelerate the attainment of this Vision for vulnerable road users through a holistic framework of innovative models, tools and services that enable data driven urban safety intelligence, facilitate safe travelling of VRUs and foster the safe integration of micro-mobility services in complex environments. At the operational level SOTERIA uncovers unexplored behavioural characteristics of VRUs and engages Living Lab communities in social innovation activities for the co-creation of urban safety solutions and infrastructure designs. Simulation models and explainable AI driven analytics are developed for supporting policy decisions and informing interconnected services that support VRUs in safe and clean travelling. On-vehicle sensors and connectivity is fostered enabling minimisation of risky situations and behaviours. The approach is validated in four thematic demonstrations and through the SOTERIA network of 8 cities, addressing different types of VRUs.","4":"1F41F6BF-0528-4162-AAAA-37DA6EDDA727"},{"1":"NE/M020843/1","2":"Impacts of global warming in sentinel systems: from genes to ecosystems","3":"The impacts of climate change, and warming in particular, on natural ecosystems remain poorly understood, and research to date has focused on individual species (e.g. range shifts of polar bears). Multispecies systems (food webs, ecosystems), however, can possess emergent properties that can only be understood using a system-level perspective. Within a given food web, the microbial world is the engine that drives key ecosystem processes, biogeochemical cycles (e.g. the carbon-cycle) and network properties, but has been hidden from view due to difficulties with identifying which microbes are present and what they are doing. The recent revolution in Next Generation Sequencing has removed this bottleneck and we can now open the microbial &quot;black box&quot; to characterise the metagenome (&quot;who is there?&quot;) and metatranscriptome (&quot;what are they doing?&quot;) of the community for the first time. These advances will allow us to address a key overarching question: should we expect a global response to global warming? There are bodies of theory that suggest this might be the case, including the &quot;Metabolic Theory of Ecology&quot; and the &quot;Everything is Everywhere&quot; hypothesis of global microbial biogeography, yet these ideas have yet to be tested rigorously at appropriate scales and in appropriate experimental contexts that allow us to identify patterns and causal relationships in real multispecies systems. \\n\\nWe will assess the impacts of warming across multiple levels of biological organisation, from genes to food webs and whole ecosystems, using geothermally warmed freshwaters in 5 high-latitude regions (Svalbard, Iceland, Greenland, Alaska, Kamchatka), where warming is predicted to be especially rapid,. Our study will be the first to characterise the impacts of climate change on multispecies systems at such an unprecedented scale. Surveys of these &quot;sentinel systems&quot; will be complemented with modelling and experiments conducted in these field sites, as well as in 100s of large-scale &quot;mesocosms&quot; (artificial streams and ponds) in the field and 1,000s of &quot;microcosms&quot; of robotically-assembled microbial communities in the laboratory. \\n\\nOur novel genes-to-ecosystems approach will allow us to integrate measures of biodiversity and ecosystem functioning. For instance, we will quantify key functional genes as well as quantifying which genes are switched on (the &quot;metatranscriptome&quot;) in addition to measuring ecosystem functioning (e.g. processes related to the carbon cycle). We will also measure the impacts of climate change on the complex networks of interacting species we find in nature - what Darwin called &quot;the entangled bank&quot; - because food webs and other types of networks can produce counterintuitive responses that cannot be predicted from studying species in isolation. \\n\\n\\nOne general objective is to assess the scope for &quot;biodiversity insurance&quot; and resilience of natural systems in the face of climate change. We will combine our intercontinental surveys with natural experiments, bioassays, manipulations and mathematical models to do this. For instance, we will characterise how temperature-mediated losses to biodiversity can compromise key functional attributes of the gene pool and of the ecosystem as a whole. \\n\\nThere is an assumption in the academic literature and in policy that freshwater ecosystems are relatively resilient because the apparently huge scope for functional redundancy could allow for compensation for species loss in the face of climate change. However, this has not been quantified empirically in natural systems, and errors in estimating the magnitude of functional redundancy could have substantial environmental and economic repercussions. The research will address a set of key specific questions and hypotheses within our 5 themed Workpackages, of broad significance to both pure and applied ecology, and which also combine to provide a more holistic perspective than has ever been attempted previously.","4":"046BF896-4EDC-4E5C-BE57-A591C7B272BB"},{"1":"NE/M020886/1","2":"Impacts of global warming in sentinel systems: from genes to ecosystems","3":"The impacts of climate change, and warming in particular, on natural ecosystems remain poorly understood, and research to date has focused on individual species (e.g. range shifts of polar bears). Multispecies systems (food webs, ecosystems), however, can possess emergent properties that can only be understood using a system-level perspective. Within a given food web, the microbial world is the engine that drives key ecosystem processes, biogeochemical cycles (e.g. the carbon-cycle) and network properties, but has been hidden from view due to difficulties with identifying which microbes are present and what they are doing. The recent revolution in Next Generation Sequencing has removed this bottleneck and we can now open the microbial &quot;black box&quot; to characterise the metagenome (&quot;who is there?&quot;) and metatranscriptome (&quot;what are they doing?&quot;) of the community for the first time. These advances will allow us to address a key overarching question: should we expect a global response to global warming? There are bodies of theory that suggest this might be the case, including the &quot;Metabolic Theory of Ecology&quot; and the &quot;Everything is Everywhere&quot; hypothesis of global microbial biogeography, yet these ideas have yet to be tested rigorously at appropriate scales and in appropriate experimental contexts that allow us to identify patterns and causal relationships in real multispecies systems. \\n\\nWe will assess the impacts of warming across multiple levels of biological organisation, from genes to food webs and whole ecosystems, using geothermally warmed freshwaters in 5 high-latitude regions (Svalbard, Iceland, Greenland, Alaska, Kamchatka), where warming is predicted to be especially rapid,. Our study will be the first to characterise the impacts of climate change on multispecies systems at such an unprecedented scale. Surveys of these &quot;sentinel systems&quot; will be complemented with modelling and experiments conducted in these field sites, as well as in 100s of large-scale &quot;mesocosms&quot; (artificial streams and ponds) in the field and 1,000s of &quot;microcosms&quot; of robotically-assembled microbial communities in the laboratory. \\n\\nOur novel genes-to-ecosystems approach will allow us to integrate measures of biodiversity and ecosystem functioning. For instance, we will quantify key functional genes as well as quantifying which genes are switched on (the &quot;metatranscriptome&quot;) in addition to measuring ecosystem functioning (e.g. processes related to the carbon cycle). We will also measure the impacts of climate change on the complex networks of interacting species we find in nature - what Darwin called &quot;the entangled bank&quot; - because food webs and other types of networks can produce counterintuitive responses that cannot be predicted from studying species in isolation. \\n\\n\\nOne general objective is to assess the scope for &quot;biodiversity insurance&quot; and resilience of natural systems in the face of climate change. We will combine our intercontinental surveys with natural experiments, bioassays, manipulations and mathematical models to do this. For instance, we will characterise how temperature-mediated losses to biodiversity can compromise key functional attributes of the gene pool and of the ecosystem as a whole. \\n\\nThere is an assumption in the academic literature and in policy that freshwater ecosystems are relatively resilient because the apparently huge scope for functional redundancy could allow for compensation for species loss in the face of climate change. However, this has not been quantified empirically in natural systems, and errors in estimating the magnitude of functional redundancy could have substantial environmental and economic repercussions. The research will address a set of key specific questions and hypotheses within our 5 themed Workpackages, of broad significance to both pure and applied ecology, and which also combine to provide a more holistic perspective than has ever been attempted previously.","4":"5250DAA7-436F-43FB-B54C-3BD3F36EBF87"},{"1":"NE/M02086X/1","2":"Impacts of global warming in sentinel systems: from genes to ecosystems","3":"The impacts of climate change, and warming in particular, on natural ecosystems remain poorly understood, and research to date has focused on individual species (e.g. range shifts of polar bears). Multispecies systems (food webs, ecosystems), however, can possess emergent properties that can only be understood using a system-level perspective. Within a given food web, the microbial world is the engine that drives key ecosystem processes, biogeochemical cycles (e.g. the carbon-cycle) and network properties, but has been hidden from view due to difficulties with identifying which microbes are present and what they are doing. The recent revolution in Next Generation Sequencing has removed this bottleneck and we can now open the microbial &quot;black box&quot; to characterise the metagenome (&quot;who is there?&quot;) and metatranscriptome (&quot;what are they doing?&quot;) of the community for the first time. These advances will allow us to address a key overarching question: should we expect a global response to global warming? There are bodies of theory that suggest this might be the case, including the &quot;Metabolic Theory of Ecology&quot; and the &quot;Everything is Everywhere&quot; hypothesis of global microbial biogeography, yet these ideas have yet to be tested rigorously at appropriate scales and in appropriate experimental contexts that allow us to identify patterns and causal relationships in real multispecies systems. \\n\\nWe will assess the impacts of warming across multiple levels of biological organisation, from genes to food webs and whole ecosystems, using geothermally warmed freshwaters in 5 high-latitude regions (Svalbard, Iceland, Greenland, Alaska, Kamchatka), where warming is predicted to be especially rapid,. Our study will be the first to characterise the impacts of climate change on multispecies systems at such an unprecedented scale. Surveys of these &quot;sentinel systems&quot; will be complemented with modelling and experiments conducted in these field sites, as well as in 100s of large-scale &quot;mesocosms&quot; (artificial streams and ponds) in the field and 1,000s of &quot;microcosms&quot; of robotically-assembled microbial communities in the laboratory. \\n\\nOur novel genes-to-ecosystems approach will allow us to integrate measures of biodiversity and ecosystem functioning. For instance, we will quantify key functional genes as well as quantifying which genes are switched on (the &quot;metatranscriptome&quot;) in addition to measuring ecosystem functioning (e.g. processes related to the carbon cycle). We will also measure the impacts of climate change on the complex networks of interacting species we find in nature - what Darwin called &quot;the entangled bank&quot; - because food webs and other types of networks can produce counterintuitive responses that cannot be predicted from studying species in isolation. \\n\\n\\nOne general objective is to assess the scope for &quot;biodiversity insurance&quot; and resilience of natural systems in the face of climate change. We will combine our intercontinental surveys with natural experiments, bioassays, manipulations and mathematical models to do this. For instance, we will characterise how temperature-mediated losses to biodiversity can compromise key functional attributes of the gene pool and of the ecosystem as a whole. \\n\\nThere is an assumption in the academic literature and in policy that freshwater ecosystems are relatively resilient because the apparently huge scope for functional redundancy could allow for compensation for species loss in the face of climate change. However, this has not been quantified empirically in natural systems, and errors in estimating the magnitude of functional redundancy could have substantial environmental and economic repercussions. The research will address a set of key specific questions and hypotheses within our 5 themed Workpackages, of broad significance to both pure and applied ecology, and which also combine to provide a more holistic perspective than has ever been attempted previously.","4":"74C6D687-B820-4A93-8944-E31C3F5CB77E"},{"1":"10053130","2":"Supervised morphogenesis in gastruloids (SUMO)","3":"The lack of realistic in vitro organ models that can faithfully represent in vivo physiological processes is a major obstacle affecting the biological and medical sciences. The current gold standard is animal experiments, but it is increasingly clear that these models mostly fail to recapitulate the human physiology. Moreover, animal experiments are controversial, and it is a common goal in the scientific community to minimize the use of animals to a strictly necessary minimum. The emergence of stem cell engineered organ models called organoids represents the only viable alternative to animal research. However, current organoid technology is yet to produce the larger physiologically relevant organ models that the medical sciences really need. Specifically, current organoids are too small, not vascularized and lack the 3-dimensional organization found in vivo. In this interdisciplinary project we aim to challenge all these limitations by using the recently developed gastruloid technology guided by cutting edge bioengineering and artificial intelligence. Gastruloids are formed by initiating the very early developmental processes and develops along a highly coordinated three axial process that closely resembles mammalian embryogenesis. Moreover, gastruloids can develop several organ precursors simultaneously and thus constitutes important improvements over conventional single-tissue organoids. To harvest the potential of gastruloid technology we will first implement large sequencing and imaging experiments to optimize the developmental trajectory of gastruloids for organ inductions. We will then build these datasets into a multimodal data matrix to identify gastruloid candidates for cardiovascular and foregut development. Specifically, we will identify candidates that show strong vasculogenesis as candidates for later vascularisation by anastomose with endothelial cells.","4":"6F70C360-D3D1-43B5-B643-967FF3382161"},{"1":"10050213","2":"Supervised morphogenesis in gastruloids","3":"The lack of realistic in vitro organ models that can faithfully represent in vivo physiological processes is a major obstacle affecting the biological and medical sciences. The current gold standard is animal experiments, but it is increasingly clear that these models mostly fail to recapitulate the human physiology. Moreover, animal experiments are controversial, and it is a common goal in the scientific community to minimize the use of animals to a strictly necessary minimum. The emergence of stem cell engineered organ models called organoids represents the only viable alternative to animal research. However, current organoid technology is yet to produce the larger physiologically relevant organ models that the medical sciences really need. Specifically, current organoids are too small, not vascularized and lack the 3-dimensional organization found in vivo. In this interdisciplinary project we aim to challenge all these limitations by using the recently developed gastruloid technology guided by cutting edge bioengineering and artificial intelligence. Gastruloids are formed by initiating the very early developmental processes and develops along a highly coordinated three axial process that closely resembles mammalian embryogenesis. Moreover, gastruloids can develop several organ precursors simultaneously and thus constitutes important improvements over conventional single-tissue organoids. To harvest the potential of gastruloid technology we will first implement large sequencing and imaging experiments to optimize the developmental trajectory of gastruloids for organ inductions. We will then build these datasets into a multimodal data matrix to identify gastruloid candidates for cardiovascular and foregut development. Specifically, we will identify candidates that show strong vasculogenesis as candidates for later vascularisation by anastomose with endothelial cells.","4":"9FCB8BA7-9D57-4806-AE8B-924EF31B019D"},{"1":"EP/F009461/1","2":"Advancing Machine Learning Methodology for New Classes of Prediction Problems","3":"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.","4":"79560C8F-2A0A-44EF-970B-19B4321DAE28"},{"1":"EP/F010508/1","2":"Advancing Machine Learning Methodology for New Classes of Prediction Problems","3":"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.","4":"A97C4610-59F6-4D85-852A-04BE6BDFDB44"},{"1":"EP/F009429/1","2":"Advancing Machine Learning Methodology for New Classes of Prediction Problems","3":"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.","4":"5BF81F11-CDF4-4505-B8D5-1B567B374865"},{"1":"10041445","2":"HOLiFOOD - Holistic approach for tackling food systems risks in a changing global environment","3":"The overall objective of HOLiFOOD is to improve the integrated food safety risk analysis (RA) framework in Europe to i) meet future challenges arising from Green Deal policy driven transitions in particular in relation to climate driven changes, ii) contribute to the UN's Sustainable Development Goals and iii) support the realization of a truly secure and sustainable food production. HOLiFOOD will apply a system approach, which take the whole environment into account in which food is being produced, including economic, environmental and social aspects. Three supply chains will be considered (i.e. cereals [maize], legumes [lentils] and poultry [chicken]). Artificial Intelligence (AI) and big data technologies will be used in the development of early warning and emerging risks prediction systems for known and unknown food safety hazards. In addition, tools, methods and approaches will be developed for hazard detection and will be targeted and non-targeted and new holistic risk assessment methods will be develop in which food safety risk will be embedded in a comprehensive cost-benefit analysis of the food system including positive and negative health, environment and economic dimensions. An effective impact pathway will be developed and implemented through integration of the HOLiFOOD outputsinto the legal framework associated with the food risk analysis process. The impact pathway will be supported by an electronic data and knowledge sharing platform aiming at the full digitalization of food (safety) systems and supporting transparency and impact for all stakeholders. In order to align with stakeholder priorities, preferences and user requirements, the HOLiFOOD innovations will be designed and tested in a multi actor approach (i.e. Living Lab) involving all stakeholders (e.g., authorities, food producers and citizens).","4":"59784262-EA0A-4FBC-B04B-B401C6B42477"},{"1":"10040684","2":"Holistic approach for tackling food systems risks in a changing global environment","3":"The overall objective of HOLiFOOD is to improve the integrated food safety risk analysis (RA) framework in Europe to i) meet future challenges arising from Green Deal policy driven transitions in particular in relation to climate driven changes, ii) contribute to the UN's Sustainable Development Goals and iii) support the realization of a truly secure and sustainable food production. HOLiFOOD will apply a system approach, which take the whole environment into account in which food is being produced, including economic, environmental and social aspects. Three supply chains will be considered (i.e. cereals [maize], legumes [lentils] and poultry [chicken]). Artificial Intelligence (AI) and big data technologies will be used in the development of early warning and emerging risks prediction systems for known and unknown food safety hazards. In addition, tools, methods and approaches will be developed for hazard detection and will be targeted and non-targeted and new holistic risk assessment methods will be develop in which food safety risk will be embedded in a comprehensive cost-benefit analysis of the food system including positive and negative health, environment and economic dimensions. An effective impact pathway will be developed and implemented through integration of the HOLiFOOD outputsinto the legal framework associated with the food risk analysis process. The impact pathway will be supported by an electronic data and knowledge sharing platform aiming at the full digitalization of food (safety) systems and supporting transparency and impact for all stakeholders. In order to align with stakeholder priorities, preferences and user requirements, the HOLiFOOD innovations will be designed and tested in a multi actor approach (i.e. Living Lab) involving all stakeholders (e.g., authorities, food producers and citizens).","4":"C981A694-8C24-4434-A9F6-D52A39C82C82"},{"1":"BB/I020004/1","2":"Development of plant-based hydrogen peroxide YFP nanosensors targeted to multiple sub-cellular locations","3":"The oxygen (O2) we breathe is produced by plants when they photosynthesise. However, for cells that produce and/or consume O2 (by respiration) as a key part of their metabolism, there is an inherent danger and that is the production of reactive oxygen species (ROS). ROS arise as an inevitable consequence of O2 chemistry and if they accumulate, they cause oxidative damage to cell components (in particular chloroplasts and mitochondria) and can trigger the death of the cell. This is why plants make antioxidants, to limit the accumulation of ROS. One ROS, hydrogen peroxide (H2O2), is relatively stable while still being a powerful oxidant. H2O2 is used as a bleaching agent because of its powerful oxidising activity. H2O2 is made in plants as a bi-product of photosynthesis, respiration and many other chemical reactions that plant cells carry out. If it accumulates then, as with other ROS, it will cause oxidative damage. Evolution, though, has a habit of turning the potentially damaging into something useful. This is the case for H2O2. The accumulation of H2O2 in different parts of cells, before it attains damaging levels, acts to alter the expression of hundreds of genes by stimulating cellular signalling systems. H2O2 is an important cellular signalling molecule in bacteria, animal cells and especially plant cells. H2O2 stimulates cell signalling both internally and from cell-to-cell in response to many changes in the plant's environment, such as changes in light levels, wounding by herbivores and attack by pathogens. H2O2 is also used to regulate growth and development in plants, such as the development of secondary roots, the growth of pollen tubes and the hardening of cell walls. The intimate involvement of H2O2 in many aspects of plants' lives means it is imperative that we are able to locate and determine the changes in the level of H2O2 in different parts of the plant from the tissue down to the sub-cellular level. Until very recently this has not been possible. Knowing where, when and how much H2O2 accumulates is important in understanding if a plant is suffering oxidative damage or is actively signalling. The lack of technology for measuring H2O2 in real time, non-invasively and accurately means there are serious gaps in our understanding of how plants grow, reproduce and interact with their environment. Our aim is to provide the plant science community with means to locate and measure H2O2 at different sub-cellular locations in plant cells in real time. We can do this because a novel technology has been developed in which H2O2 can be specifically detected in cells using a genetically encoded protein sensor called HyPer. HyPer is a novel artificial protein which consists of a part (called a domain) of a bacterial protein called OxyR which changes shape when it specifically binds H2O2 .This OxyR domain is linked to a greatly modified fluorescent protein from a jellyfish, which changes its fluorescence characteristics in response to the change in shape of the OxyR domain. This fluorescence change, in response to H2O2, can be visualised by one of several types of microscope which allows the researcher to locate and measure changes in H2O2 concentration over time. HyPer has been shown to work in animal cells, bacteria and fish embryos. We have shown that HyPer works in exactly the same way in cells of roots and leaves of young seedlings. We aim to construct HyPer variants that will go to different locations in the cell so that researchers can build up a comprehensive picture of H2O2 accumulation in different tissues and conditions. However, HyPer expression is silent in older plants, which is common with other types of fluorescent sensors in plants. We have provided a number of solutions to this problem which will be deployed in this project to allow maximum and rapid uptake of this technology by the global plant science community, advancing knowledge of plant functions on a wide front.","4":"979179F4-3365-42AC-8D5C-1327CECC7262"},{"1":"BB/I020071/1","2":"Development of plant-based hydrogen peroxide YFP nanosensors targeted to multiple sub-cellular locations","3":"The oxygen (O2) we breathe is produced by plants when they photosynthesise. However, for cells that produce and/or consume O2 (by respiration) as a key part of their metabolism, there is an inherent danger and that is the production of reactive oxygen species (ROS). ROS arise as an inevitable consequence of O2 chemistry and if they accumulate, they cause oxidative damage to cell components (in particular chloroplasts and mitochondria) and can trigger the death of the cell. This is why plants make antioxidants, to limit the accumulation of ROS. One ROS, hydrogen peroxide (H2O2), is relatively stable while still being a powerful oxidant. H2O2 is used as a bleaching agent because of its powerful oxidising activity. H2O2 is made in plants as a bi-product of photosynthesis, respiration and many other chemical reactions that plant cells carry out. If it accumulates then, as with other ROS, it will cause oxidative damage. Evolution, though, has a habit of turning the potentially damaging into something useful. This is the case for H2O2. The accumulation of H2O2 in different parts of cells, before it attains damaging levels, acts to alter the expression of hundreds of genes by stimulating cellular signalling systems. H2O2 is an important cellular signalling molecule in bacteria, animal cells and especially plant cells. H2O2 stimulates cell signalling both internally and from cell-to-cell in response to many changes in the plant's environment, such as changes in light levels, wounding by herbivores and attack by pathogens. H2O2 is also used to regulate growth and development in plants, such as the development of secondary roots, the growth of pollen tubes and the hardening of cell walls. The intimate involvement of H2O2 in many aspects of plants' lives means it is imperative that we are able to locate and determine the changes in the level of H2O2 in different parts of the plant from the tissue down to the sub-cellular level. Until very recently this has not been possible. Knowing where, when and how much H2O2 accumulates is important in understanding if a plant is suffering oxidative damage or is actively signalling. The lack of technology for measuring H2O2 in real time, non-invasively and accurately means there are serious gaps in our understanding of how plants grow, reproduce and interact with their environment. Our aim is to provide the plant science community with means to locate and measure H2O2 at different sub-cellular locations in plant cells in real time. We can do this because a novel technology has been developed in which H2O2 can be specifically detected in cells using a genetically encoded protein sensor called HyPer. HyPer is a novel artificial protein which consists of a part (called a domain) of a bacterial protein called OxyR which changes shape when it specifically binds H2O2 .This OxyR domain is linked to a greatly modified fluorescent protein from a jellyfish, which changes its fluorescence characteristics in response to the change in shape of the OxyR domain. This fluorescence change, in response to H2O2, can be visualised by one of several types of microscope which allows the researcher to locate and measure changes in H2O2 concentration over time. HyPer has been shown to work in animal cells, bacteria and fish embryos. We have shown that HyPer works in exactly the same way in cells of roots and leaves of young seedlings. We aim to construct HyPer variants that will go to different locations in the cell so that researchers can build up a comprehensive picture of H2O2 accumulation in different tissues and conditions. However, HyPer expression is silent in older plants, which is common with other types of fluorescent sensors in plants. We have provided a number of solutions to this problem which will be deployed in this project to allow maximum and rapid uptake of this technology by the global plant science community, advancing knowledge of plant functions on a wide front.","4":"8283B118-36FA-42B3-9168-B932EDC03D48"},{"1":"EP/F016611/1","2":"Inhomogeneous magnetism and superconductivity","3":"The past fifteen years has seen considerable research into the coupling of superconductivity and magnetism. These two effects are both mediated by coupling between electrons, but ferromagnetism leads to the parallel alignment of spins while conventional (so called spin-singlet) superconductivity requires anti-parallel spin alignment. As a result the coupling of superconductivity into ferromagnets is generally much weaker than the coupling into non-magnetic metals (the so-called proximity effect). However, at very short-range (a few nanometres) the coupling between superconductivity and ferromagnetism at the interface between the two materials results in complex behaviour which is distinct from that of either material. Most notably, the pairs of electrons which are responsible for superconductivity have a rapidly oscillating phase in the ferromagnet which can lead to negative rather than positive supercurrents appearing in devices in which a thin ferromagnetic barrier separates two superconductors. Devices based on this effect are currently being developed for quantum computation. More controversially, a few very recent experiments have detected a much longer-ranged proximity effect in which superconductivity can penetrate a ferromagnet over distances of hundreds on nanometres. This effect seems to be confirmation of theoretical predications that if the magnetism is inhomogeneous (i.e. all the spins do not point in a single direction) or the electrons are 100% spin polarised then a so-called spin-triplet state of superconductivity should appear. The aim of our proposed project is to investigate carefully the conditions required for the formation of this spin-triplet state and to understand how to control it so that potential applications can be developed. In particular we will look at classes of ferromagnet which have a spiral rather than linear magnetic order, we will also grow artificial magnetic structures in which such spirals can be changed by applying a magnetic field and we will explore how the presence of magnetic domain walls (which are regions in which the magnetism changes direction in a material) affects the superconducting properties.","4":"9F247321-137E-4405-8265-AC26F226CD28"},{"1":"EP/F016646/1","2":"Inhomogeneous magnetism and superconductivity","3":"The past fifteen years has seen considerable research into the coupling of superconductivity and magnetism. These two effects are both mediated by coupling between electrons, but ferromagnetism leads to the parallel alignment of spins while conventional (so called spin-singlet) superconductivity requires anti-parallel spin alignment. As a result the coupling of superconductivity into ferromagnets is generally much weaker than the coupling into non-magnetic metals (the so-called proximity effect). However, at very short-range (a few nanometres) the coupling between superconductivity and ferromagnetism at the interface between the two materials results in complex behaviour which is distinct from that of either material. Most notably, the pairs of electrons which are responsible for superconductivity have a rapidly oscillating phase in the ferromagnet which can lead to negative rather than positive supercurrents appearing in devices in which a thin ferromagnetic barrier separates two superconductors. Devices based on this effect are currently being developed for quantum computation. More controversially, a few very recent experiments have detected a much longer-ranged proximity effect in which superconductivity can penetrate a ferromagnet over distances of hundreds on nanometres. This effect seems to be confirmation of theoretical predications that if the magnetism is inhomogeneous (i.e. all the spins do not point in a single direction) or the electrons are 100% spin polarised then a so-called spin-triplet state of superconductivity should appear. The aim of our proposed project is to investigate carefully the conditions required for the formation of this spin-triplet state and to understand how to control it so that potential applications can be developed. In particular we will look at classes of ferromagnet which have a spiral rather than linear magnetic order, we will also grow artificial magnetic structures in which such spirals can be changed by applying a magnetic field and we will explore how the presence of magnetic domain walls (which are regions in which the magnetism changes direction in a material) affects the superconducting properties.","4":"8389490E-12D5-4CA3-819B-5038AF710679"},{"1":"EP/F016891/1","2":"Inhomogeneous magnetism and superconductivity","3":"The past fifteen years has seen considerable research into the coupling of superconductivity and magnetism. These two effects are both mediated by coupling between electrons, but ferromagnetism leads to the parallel alignment of spins while conventional (so called spin-singlet) superconductivity requires anti-parallel spin alignment. As a result the coupling of superconductivity into ferromagnets is generally much weaker than the coupling into non-magnetic metals (the so-called proximity effect). However, at very short-range (a few nanometres) the coupling between superconductivity and ferromagnetism at the interface between the two materials results in complex behaviour which is distinct from that of either material. Most notably, the pairs of electrons which are responsible for superconductivity have a rapidly oscillating phase in the ferromagnet which can lead to negative rather than positive supercurrents appearing in devices in which a thin ferromagnetic barrier separates two superconductors. Devices based on this effect are currently being developed for quantum computation. More controversially, a few very recent experiments have detected a much longer-ranged proximity effect in which superconductivity can penetrate a ferromagnet over distances of hundreds on nanometres. This effect seems to be confirmation of theoretical predications that if the magnetism is inhomogeneous (i.e. all the spins do not point in a single direction) or the electrons are 100% spin polarised then a so-called spin-triplet state of superconductivity should appear. The aim of our proposed project is to investigate carefully the conditions required for the formation of this spin-triplet state and to understand how to control it so that potential applications can be developed. In particular we will look at classes of ferromagnet which have a spiral rather than linear magnetic order, we will also grow artificial magnetic structures in which such spirals can be changed by applying a magnetic field and we will explore how the presence of magnetic domain walls (which are regions in which the magnetism changes direction in a material) affects the superconducting properties.","4":"A28AE7E1-D9CF-4D60-B76E-148314E3E111"},{"1":"EP/F016271/1","2":"Inhomogeneous magnetism and superconductivity","3":"The past fifteen years has seen considerable research into the coupling of superconductivity and magnetism. These two effects are both mediated by coupling between electrons, but ferromagnetism leads to the parallel alignment of spins while conventional (so called spin-singlet) superconductivity requires anti-parallel spin alignment. As a result the coupling of superconductivity into ferromagnets is generally much weaker than the coupling into non-magnetic metals (the so-called proximity effect). However, at very short-range (a few nanometres) the coupling between superconductivity and ferromagnetism at the interface between the two materials results in complex behaviour which is distinct from that of either material. Most notably, the pairs of electrons which are responsible for superconductivity have a rapidly oscillating phase in the ferromagnet which can lead to negative rather than positive supercurrents appearing in devices in which a thin ferromagnetic barrier separates two superconductors. Devices based on this effect are currently being developed for quantum computation. More controversially, a few very recent experiments have detected a much longer-ranged proximity effect in which superconductivity can penetrate a ferromagnet over distances of hundreds on nanometres. This effect seems to be confirmation of theoretical predications that if the magnetism is inhomogeneous (i.e. all the spins do not point in a single direction) or the electrons are 100% spin polarised then a so-called spin-triplet state of superconductivity should appear. The aim of our proposed project is to investigate carefully the conditions required for the formation of this spin-triplet state and to understand how to control it so that potential applications can be developed. In particular we will look at classes of ferromagnet which have a spiral rather than linear magnetic order, we will also grow artificial magnetic structures in which such spirals can be changed by applying a magnetic field and we will explore how the presence of magnetic domain walls (which are regions in which the magnetism changes direction in a material) affects the superconducting properties.","4":"D70CB451-F4B0-4F7D-AD5A-EFD844D64768"},{"1":"1946238","2":"3D Bioprinting Engineering Artificial Respiratory Tract Tissue","3":"The proposed project fits in Priority Area: Leading Edge Healthcare\\nBackground:\\nThere is a need to develop and improve human relevant tools enabling the study of mechanisms not yet fully understood in human biology. Current in vivo models almost exclusively utilise non primate animals which are limited by frequent poor correlation between human and animal systems. The ideal model would allow the direct interrogation and comparison of genetically equivalent human tissues under different experimental conditions. Advances in growing human tissues ex vivo, have led to the development of ex vivo human organ culture. To date this has focused on generating human organs for transplantation. However, the techniques developed also offer the potential to revolutionise in vitro studies of human biology. Whilst decellularised scaffolds hold promise for transplantation, the requirement for human or animal tissue to generate scaffolds limits the broad scale application of the technology to other areas such as science discovery.\\n3D bio-printing is the process of creating spatially-controlled cell patterns, in which the behaviour of biological tissues can be reproduced. This ideally extends to printing complete, viable organs for in vitro model, tissue repair and organ transplant. Printing viable organs is not possible as of yet, as organs are very complex. Most printed tissue constructs are not viable for very long, printable cell-laden bioink and perfusion of the construct after printing are still open issues. Despite challenges, the functioning micro- organs have been produced by 3D bioprinting, promising for further development.\\nResearch Plan:\\nThe aim of this project is to develop and optimise engineered artificial scaffolds amenable to ex vivo human respiratory tract organ culture. We propose to investigate approaches to enable the growth of organs in test tubes, by making use of advances in 3D bioprinting technologies with an ultimate goal to significantly reduce/entirely replace the use of animal organs in research. Specifically the programme of research would consist of:\\n(i) Printing artificial human epithelial tissue\\n1.2 IMPACT SUMMARY (Up to 1500 characters)\\n1.3 DATA SHARING (Up to 2000 characters)\\n2\\nThe first step is to develop printable hydrogels and cell loaded bio-ink for fabricating airway tissue. Artificial airway with multicellular structure consisting of fibroblast stroma with epithelium will be printed and cultured for growing a fully differentiated epithelium in air-liquid interface models. Cellular self-assembly within printed scaffold will be studied and compared with human lung scaffolds from donor tissue through histological analysis and biomarker profiling.\\n(ii) 3D printing miniaturised whole organ scaffolds\\nThe 2nd stage of the project will print artificial lung tissue with integration of airway, compared with human cell self-assembly on human whole organ scaffolds. Histological, transcriptomic and protein biomarker based profiling would be used to compare artificial tissue generated using miniaturised scaffolds to whole donor tissue.\\n(iii) Functional studies of miniaturised artificial tissue\\nThe tissue generated within this study would be tested using previously described mediators of tissue function (e.g. inhibition of airway mucus production of epithelial cells by simvastatin in a lung tissue model, or inhibition of acetylcholine induced intestinal motility by interleukin 1 beta in in a gut model). The specific functional mediators investigated would be defined based on the outcome of (i) and (ii).","4":"B1D7CCC6-55DA-4084-BE03-53E6D74149D0"},{"1":"1909407","2":"3D Bioprinting Engineering Artificial Respiratory Tract Tissue","3":"The proposed project fits in Priority Area: Leading Edge Healthcare\\nBackground:\\nThere is a need to develop and improve human relevant tools enabling the study of mechanisms not yet fully understood in human biology. Current in vivo models almost exclusively utilise non primate animals which are limited by frequent poor correlation between human and animal systems. The ideal model would allow the direct interrogation and comparison of genetically equivalent human tissues under different experimental conditions. Advances in growing human tissues ex vivo, have led to the development of ex vivo human organ culture. To date this has focused on generating human organs for transplantation. However, the techniques developed also offer the potential to revolutionise in vitro studies of human biology. Whilst decellularised scaffolds hold promise for transplantation, the requirement for human or animal tissue to generate scaffolds limits the broad scale application of the technology to other areas such as science discovery.\\n3D bio-printing is the process of creating spatially-controlled cell patterns, in which the behaviour of biological tissues can be reproduced. This ideally extends to printing complete, viable organs for in vitro model, tissue repair and organ transplant. Printing viable organs is not possible as of yet, as organs are very complex. Most printed tissue constructs are not viable for very long, printable cell-laden bioink and perfusion of the construct after printing are still open issues. Despite challenges, the functioning micro- organs have been produced by 3D bioprinting, promising for further development.\\nResearch Plan:\\nThe aim of this project is to develop and optimise engineered artificial scaffolds amenable to ex vivo human respiratory tract organ culture. We propose to investigate approaches to enable the growth of organs in test tubes, by making use of advances in 3D bioprinting technologies with an ultimate goal to significantly reduce/entirely replace the use of animal organs in research. Specifically the programme of research would consist of:\\n(i) Printing artificial human epithelial tissue\\n1.2 IMPACT SUMMARY (Up to 1500 characters)\\n1.3 DATA SHARING (Up to 2000 characters)\\n2\\nThe first step is to develop printable hydrogels and cell loaded bio-ink for fabricating airway tissue. Artificial airway with multicellular structure consisting of fibroblast stroma with epithelium will be printed and cultured for growing a fully differentiated epithelium in air-liquid interface models. Cellular self-assembly within printed scaffold will be studied and compared with human lung scaffolds from donor tissue through histological analysis and biomarker profiling.\\n(ii) 3D printing miniaturised whole organ scaffolds\\nThe 2nd stage of the project will print artificial lung tissue with integration of airway, compared with human cell self-assembly on human whole organ scaffolds. Histological, transcriptomic and protein biomarker based profiling would be used to compare artificial tissue generated using miniaturised scaffolds to whole donor tissue.\\n(iii) Functional studies of miniaturised artificial tissue\\nThe tissue generated within this study would be tested using previously described mediators of tissue function (e.g. inhibition of airway mucus production of epithelial cells by simvastatin in a lung tissue model, or inhibition of acetylcholine induced intestinal motility by interleukin 1 beta in in a gut model). The specific functional mediators investigated would be defined based on the outcome of (i) and (ii).","4":"9EB42F93-63C8-462B-B236-AE36B3526301"},{"1":"971228","2":"Tracking Real-time Intelligence in Data Streams (TRIDS)","3":"The public description for this project has been requested but has not yet been received.","4":"A6773173-52EF-4478-A4D6-647B7CEB52E9"},{"1":"971225","2":"IPL Discover - A map-based intelligence product that provides geospatial analysis of public data from social networking, news sites and transport feeds.","3":"The public description for this project has been requested but has not yet been received.","4":"06A0F105-C0D3-4078-A9EF-B8302EE35F45"},{"1":"2699073","2":"Computer vision","3":"The research are alignments are: Image and vision computing and Artificial intelligence technologies\\n\\nAn image can be accurately described by is by objects it shows and spatial relations between them. For example, an image of a bedroom may be described by the sentence: &quot;there is a bed next to a nightstand table with a lamp on the top of it&quot;. The aim of this Ph.D. research is the visual understanding of an image by detecting different objects (i.e. recognize their category and location within the image), followed by the generation of a textual description of the scene.\\nOne of the main challenges in object recognition is the collection of annotated training data. Rather than collecting and annotating images manually, we propose to develop methods for training object models from existing datasets collected in different domains or generated synthetically, which is called domain adaptation. The domain adaptation problem is two-fold. Firstly, visual domain shift needs to be addressed: objects in images from different datasets have different visual appearance e.g. objects in real images vs. drawings. The appearance may differ even between images of real objects of the same category e.g. a shirt in a stock photo differs from its appearance in a low-quality picture, or taken with a mobile phone in different illumination conditions.\\nSome of the most popular deep learning object detection models include Fast/Faster R-CNN, YOLO or SSD and use annotated training data (i.e. a bounding box around the object and its category label) to learn the appearance of the objects. However, when the models are applied in images where the objects have different appearance their performance degrades. Domain adaptation research aims to reduce this degradation, but it has mainly focused on image classification problems (one category per image), and there has been little work on both recognition and localisation of objects in different visual domains. Developing an approach to object detection with domain adaptation is the first main objective of this thesis. To do so, we aim to adapt the instance features from the two different domains similar to the problem tackled by [1].\\nThe second objective of this work is to propose methods that can describe images with natural text enriched by words that were not present in annotated training data. Current models require pairs of images and text to learn suitable descriptions, thus, their vocabulary is limited to what is present the given training pairs. A rich vocabulary is needed to generate more specific captions. For example in fashion or cloth retail market, one may want to use fashion-related attributes to describe images of a person in the street. The most popular methods are focused on using global features extracted from the image and fed to a recurrent neural network (RNN) that generates a sentence description. Recently, more attention has been directed towards methods which first detect all the objects in the image as well as their spatial relations, and then input this information into RNN. This decoupling of object detection and caption generation allows using different detectors with different results that can be trained with data which require less intensive manual annotation than image-sentence pairs typically used for training caption generations. Caption generation can then rely on the input from object detectors and other text datasets with rich object descriptions which do not include image examples. We propose to use a method similar to [2] to learn pattern-like structures of sentences and use detectors trained in other domain to output more fine-grained results.\\n\\nReferences:\\n\\n[1] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Domain adaptive Faster R-CNN for object detection in the wild. In CVPR, 2018.\\n[2] J. Lu, J. Yang, D. Batra, and D. Parikh. 2018. Neural Baby Talk. In CVPR, 2018.&quot;","4":"4A987464-8B2A-4E7D-B4CA-BCEE2ACE7496"},{"1":"2699076","2":"Computer vision","3":"The research are alignments are: Image and vision computing and Artificial intelligence technologies\\n\\nAn image can be accurately described by is by objects it shows and spatial relations between them. For example, an image of a bedroom may be described by the sentence: &quot;there is a bed next to a nightstand table with a lamp on the top of it&quot;. The aim of this Ph.D. research is the visual understanding of an image by detecting different objects (i.e. recognize their category and location within the image), followed by the generation of a textual description of the scene.\\nOne of the main challenges in object recognition is the collection of annotated training data. Rather than collecting and annotating images manually, we propose to develop methods for training object models from existing datasets collected in different domains or generated synthetically, which is called domain adaptation. The domain adaptation problem is two-fold. Firstly, visual domain shift needs to be addressed: objects in images from different datasets have different visual appearance e.g. objects in real images vs. drawings. The appearance may differ even between images of real objects of the same category e.g. a shirt in a stock photo differs from its appearance in a low-quality picture, or taken with a mobile phone in different illumination conditions.\\nSome of the most popular deep learning object detection models include Fast/Faster R-CNN, YOLO or SSD and use annotated training data (i.e. a bounding box around the object and its category label) to learn the appearance of the objects. However, when the models are applied in images where the objects have different appearance their performance degrades. Domain adaptation research aims to reduce this degradation, but it has mainly focused on image classification problems (one category per image), and there has been little work on both recognition and localisation of objects in different visual domains. Developing an approach to object detection with domain adaptation is the first main objective of this thesis. To do so, we aim to adapt the instance features from the two different domains similar to the problem tackled by [1].\\nThe second objective of this work is to propose methods that can describe images with natural text enriched by words that were not present in annotated training data. Current models require pairs of images and text to learn suitable descriptions, thus, their vocabulary is limited to what is present the given training pairs. A rich vocabulary is needed to generate more specific captions. For example in fashion or cloth retail market, one may want to use fashion-related attributes to describe images of a person in the street. The most popular methods are focused on using global features extracted from the image and fed to a recurrent neural network (RNN) that generates a sentence description. Recently, more attention has been directed towards methods which first detect all the objects in the image as well as their spatial relations, and then input this information into RNN. This decoupling of object detection and caption generation allows using different detectors with different results that can be trained with data which require less intensive manual annotation than image-sentence pairs typically used for training caption generations. Caption generation can then rely on the input from object detectors and other text datasets with rich object descriptions which do not include image examples. We propose to use a method similar to [2] to learn pattern-like structures of sentences and use detectors trained in other domain to output more fine-grained results.\\n\\nReferences:\\n\\n[1] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Domain adaptive Faster R-CNN for object detection in the wild. In CVPR, 2018.\\n[2] J. Lu, J. Yang, D. Batra, and D. Parikh. 2018. Neural Baby Talk. In CVPR, 2018.&quot;","4":"74D1EAC4-DA0A-4601-BCA9-2F32CF9CEEEE"},{"1":"EP/D031117/1","2":"PLATFORM: A Research Platform for Next Generation Process Tomography : 2005-2009","3":"The technical programme will address 4 themes. The longer term goals in each case are identified below but it is important to note that we are not proposing that these can be achieved under the level of funding provided by this Platform grant. Rather, the funding will enable us to undertake the essential underpinning research to fuel the downstream proposals that WILL deliver significant progress towards the ultimate aims. Theme T1 : Multi-Dimensional Network SensingNew opportunities to understand micro-scale to very-large scale processes will be created through low-cost autonomously communicating sensor nodes. These may be fixed or mobile and will embed local positioning systems. The ability to form heterogeneous networks of sensors, to self-organise the communication and routing of results between them, and then to integrate the collected data, will inform a new genre of process intelligence. Long Term Goal T1.1 - Fundamental Model and Systems for Multi-dimensional Sensor Networks Long Term Goal T1.2 - Sensor Network Process DemonstratorTheme T2 - New Sensing Technologies for Small-Scale Chemical and Physical State TomographyPrevious process tomography sensor systems have, typically, addressed the distribution of one parameter at the macroscopic scale, for instance conductivity. We now propose to explore novel techniques to achieve simultaneous measurements of multiple parameters from nano- to macro- scale, through multi-modal approaches exploiting photonic techniques and micro-fabrication of sensors. Long Term Goal T2.1 - Novel Photonic Techniques to Penetrate the Fundamentals of Particulate Processes Long Term Goal T2.2 - Miniaturised Electrical TomographyTheme T3 - Smart Tomography Systems and Multi-Dimensional Data FusionResearch will explore fundamental science that defines temporal and spatial resolution, relative to physical features and constraints such as sensor configurations, process dimensions and signal to noise ratio. A new spectroscopic approach is proposed that will : facilitate the extraction of signal components in various noise environments in both spatial and temporal modes; exploit spectroscopic data. Long Term Goal T3.1 Smart Spatio-temporal Spectroscopic and Process-Compliant TomographyLong Term Goal T3.2 Multi-Dimensional Data FusionTheme T4 / Extending the Spectral and Temporal Envelope for Electromagnetic TomographyOur ability to exploit the electromagnetic spectrum in order to interrogate industrial processes has taken significant steps forward over the past decade. Fundamental research into the low frequency electrical modalities has been translated into a broad base of applications, as described earlier. However, our opportunities to exploit electromagnetic techniques further can be expected to continue, possibly at an even greater pace. This theme aims to build of the use non-contact electromagnetic tomography mainly at low, and RF frequencies, exploiting both continuous wave and pulse time domain techniques.Long Term Goal T4.1 Extending the application baseLong Term Goal T4.2 Miniature electromagnetic arraysLong Term Goal T4.3 Time resolved tomographyThe Platform grant will also enable us to pursue the following 6 Outreach activities :1. Continue to deliver and expand the World Congress in Industrial Process Tomography / the 4th meeting is in Japan in September 2005 and we anticipate more than 200 delegates.2. Double the membership of The Virtual Centre Industrial Consortium / this currently comprises 10 companies.3. Apply for a Partnership for Public Awareness Award.4. Seek funding to commission a multi-media presentation - for web and general dissemination to present technical horizons, benefits and impact on society.5. Bid for funding for an exhibit for loan to external bodies / for instance professional science and technology centres, museums etc.6. Maintain the VCIPT web-site http://www.vcipt.org","4":"6CB1D37C-B866-449F-8609-0F2E592CB00C"},{"1":"EP/D031257/1","2":"PLATFORM: A Research Platform for Next Generation Process Tomography : 2005-2009","3":"The technical programme will address 4 themes. The longer term goals in each case are identified below but it is important to note that we are not proposing that these can be achieved under the level of funding provided by this Platform grant. Rather, the funding will enable us to undertake the essential underpinning research to fuel the downstream proposals that WILL deliver significant progress towards the ultimate aims. Theme T1 : Multi-Dimensional Network SensingNew opportunities to understand micro-scale to very-large scale processes will be created through low-cost autonomously communicating sensor nodes. These may be fixed or mobile and will embed local positioning systems. The ability to form heterogeneous networks of sensors, to self-organise the communication and routing of results between them, and then to integrate the collected data, will inform a new genre of process intelligence. Long Term Goal T1.1 - Fundamental Model and Systems for Multi-dimensional Sensor Networks Long Term Goal T1.2 - Sensor Network Process DemonstratorTheme T2 - New Sensing Technologies for Small-Scale Chemical and Physical State TomographyPrevious process tomography sensor systems have, typically, addressed the distribution of one parameter at the macroscopic scale, for instance conductivity. We now propose to explore novel techniques to achieve simultaneous measurements of multiple parameters from nano- to macro- scale, through multi-modal approaches exploiting photonic techniques and micro-fabrication of sensors. Long Term Goal T2.1 - Novel Photonic Techniques to Penetrate the Fundamentals of Particulate Processes Long Term Goal T2.2 - Miniaturised Electrical TomographyTheme T3 - Smart Tomography Systems and Multi-Dimensional Data FusionResearch will explore fundamental science that defines temporal and spatial resolution, relative to physical features and constraints such as sensor configurations, process dimensions and signal to noise ratio. A new spectroscopic approach is proposed that will : facilitate the extraction of signal components in various noise environments in both spatial and temporal modes; exploit spectroscopic data. Long Term Goal T3.1 Smart Spatio-temporal Spectroscopic and Process-Compliant TomographyLong Term Goal T3.2 Multi-Dimensional Data FusionTheme T4 / Extending the Spectral and Temporal Envelope for Electromagnetic TomographyOur ability to exploit the electromagnetic spectrum in order to interrogate industrial processes has taken significant steps forward over the past decade. Fundamental research into the low frequency electrical modalities has been translated into a broad base of applications, as described earlier. However, our opportunities to exploit electromagnetic techniques further can be expected to continue, possibly at an even greater pace. This theme aims to build of the use non-contact electromagnetic tomography mainly at low, and RF frequencies, exploiting both continuous wave and pulse time domain techniques.Long Term Goal T4.1 Extending the application baseLong Term Goal T4.2 Miniature electromagnetic arraysLong Term Goal T4.3 Time resolved tomographyThe Platform grant will also enable us to pursue the following 6 Outreach activities :1. Continue to deliver and expand the World Congress in Industrial Process Tomography / the 4th meeting is in Japan in September 2005 and we anticipate more than 200 delegates.2. Double the membership of The Virtual Centre Industrial Consortium / this currently comprises 10 companies.3. Apply for a Partnership for Public Awareness Award.4. Seek funding to commission a multi-media presentation - for web and general dissemination to present technical horizons, benefits and impact on society.5. Bid for funding for an exhibit for loan to external bodies / for instance professional science and technology centres, museums etc.6. Maintain the VCIPT web-site http://www.vcipt.org","4":"54164F69-D1B4-457A-9010-6DB508B0C784"},{"1":"NE/E003885/1","2":"Cloud System Resolving Modelling of the Tropical Atmosphere","3":"The tropics are often described as the engine room of the Earth's climate system, powering the global circulations of the atmosphere and oceans. Absorption of sunlight heats the land and ocean surfaces strongly at low latitudes, producing convection that carries the energy from the surface into the atmosphere. Except over the deserts, the convection generates deep clouds that transport moisture evaporated from the ocean into the upper atmosphere. When these clouds rain, the release of 'latent heat' by condensing water produces further heating that drives the weather systems of the tropics and influences the winds all around the globe. But such deep convective clouds rarely exist in isolation; they are almost always organised into structures ranging from squall lines and cloud clusters to tropical storms, hurricanes and super-clusters; and convection varies on a wide range of timescales from that of an individual cloud element (hours), through the daily cycle to a plethora of waves with periods ranging up to the intra-seasonal oscillation, which can propagate around the world in 30-60 days. The tropical atmosphere thus organises itself on a huge range of space and timescales; the effect on the climate system is very different from that of random, or disorganised turbulence so that all these scales should be represented in a computer model if it is to reproduce the real world accurately. Until now, many of these effects have had to be represented in a highly simplified way, through a process known as parametrization. This tries to mimic the effect of the convection on the scales smaller than those represented explicitly in the model. The trouble is that across the cloud system from a few to a few hundreds of kilometres there is no preferred scale, and these scales span the range from those that are 'sub-grid' and therefore need parametrization to those which are resolved by the model. Even with the availability of modern supercomputers, this transition from parametrization to resolved motion takes place at around 100km in the latest climate models. All of the structures that occur in the real world below this scale are parametrized, so a completely artificial break occurs between what is resolved and what is parametrized. To compound the problem, the parametrization assumes that organization on scales which cannot be resolved is not important for determining the properties of the convection or its influence on the large-scale flow. But there is mounting evidence that this creates all sorts of problems in the models. Several decades have been invested in developing convection parametrizations and even after all this time and effort no fully satisfactory solution has been found. We therefore propose to take a radical but entirely logical approach to this problem. It is now possible to run models that do resolve convective systems explicitly (at least down to scales of around 1km) over very large domains that encompass all of the important scales mentioned above. Such cloud system resolving models provide a new tool for understanding how convection really works and organises itself, and how it should be parametrized in climate models. Our proposal links these models with new data from satellites and from the surface that will give us an unprecedented view of the evolution of clouds and rain-producing systems. We will bring this unique combination of modelling and observations to bear on what is regarded as one of the most fundamental problems in weather and climate. The results of the work will inform the development of a new generation of more accurate atmospheric models that will find employment in both climate prediction and weather forecasting.","4":"968106AC-E24F-4432-A143-5B53BC88CC49"},{"1":"NE/E00525X/1","2":"Cloud System Resolving Modelling of the Tropical Atmosphere","3":"The tropics are often described as the engine room of the Earth's climate system, powering the global circulations of the atmosphere and oceans. Absorption of sunlight heats the land and ocean surfaces strongly at low latitudes, producing convection that carries the energy from the surface into the atmosphere. Except over the deserts, the convection generates deep clouds that transport moisture evaporated from the ocean into the upper atmosphere. When these clouds rain, the release of 'latent heat' by condensing water produces further heating that drives the weather systems of the tropics and influences the winds all around the globe. But such deep convective clouds rarely exist in isolation; they are almost always organised into structures ranging from squall lines and cloud clusters to tropical storms, hurricanes and super-clusters; and convection varies on a wide range of timescales from that of an individual cloud element (hours), through the daily cycle to a plethora of waves with periods ranging up to the intra-seasonal oscillation, which can propagate around the world in 30-60 days. The tropical atmosphere thus organises itself on a huge range of space and timescales; the effect on the climate system is very different from that of random, or disorganised turbulence so that all these scales should be represented in a computer model if it is to reproduce the real world accurately. Until now, many of these effects have had to be represented in a highly simplified way, through a process known as parametrization. This tries to mimic the effect of the convection on the scales smaller than those represented explicitly in the model. The trouble is that across the cloud system from a few to a few hundreds of kilometres there is no preferred scale, and these scales span the range from those that are 'sub-grid' and therefore need parametrization to those which are resolved by the model. Even with the availability of modern supercomputers, this transition from parametrization to resolved motion takes place at around 100km in the latest climate models. All of the structures that occur in the real world below this scale are parametrized, so a completely artificial break occurs between what is resolved and what is parametrized. To compound the problem, the parametrization assumes that organization on scales which cannot be resolved is not important for determining the properties of the convection or its influence on the large-scale flow. But there is mounting evidence that this creates all sorts of problems in the models. Several decades have been invested in developing convection parametrizations and even after all this time and effort no fully satisfactory solution has been found. We therefore propose to take a radical but entirely logical approach to this problem. It is now possible to run models that do resolve convective systems explicitly (at least down to scales of around 1km) over very large domains that encompass all of the important scales mentioned above. Such cloud system resolving models provide a new tool for understanding how convection really works and organises itself, and how it should be parametrized in climate models. Our proposal links these models with new data from satellites and from the surface that will give us an unprecedented view of the evolution of clouds and rain-producing systems. We will bring this unique combination of modelling and observations to bear on what is regarded as one of the most fundamental problems in weather and climate. The results of the work will inform the development of a new generation of more accurate atmospheric models that will find employment in both climate prediction and weather forecasting.","4":"5C7020EE-2CDA-4E89-8864-ACCDE70C4FF5"},{"1":"NE/E003826/1","2":"Cloud System Resolving Modelling of the Tropical Atmosphere","3":"The tropics are often described as the engine room of the Earth's climate system, powering the global circulations of the atmosphere and oceans. Absorption of sunlight heats the land and ocean surfaces strongly at low latitudes, producing convection that carries the energy from the surface into the atmosphere. Except over the deserts, the convection generates deep clouds that transport moisture evaporated from the ocean into the upper atmosphere. When these clouds rain, the release of 'latent heat' by condensing water produces further heating that drives the weather systems of the tropics and influences the winds all around the globe. But such deep convective clouds rarely exist in isolation; they are almost always organised into structures ranging from squall lines and cloud clusters to tropical storms, hurricanes and super-clusters; and convection varies on a wide range of timescales from that of an individual cloud element (hours), through the daily cycle to a plethora of waves with periods ranging up to the intra-seasonal oscillation, which can propagate around the world in 30-60 days. The tropical atmosphere thus organises itself on a huge range of space and timescales; the effect on the climate system is very different from that of random, or disorganised turbulence so that all these scales should be represented in a computer model if it is to reproduce the real world accurately. Until now, many of these effects have had to be represented in a highly simplified way, through a process known as parametrization. This tries to mimic the effect of the convection on the scales smaller than those represented explicitly in the model. The trouble is that across the cloud system from a few to a few hundreds of kilometres there is no preferred scale, and these scales span the range from those that are 'sub-grid' and therefore need parametrization to those which are resolved by the model. Even with the availability of modern supercomputers, this transition from parametrization to resolved motion takes place at around 100km in the latest climate models. All of the structures that occur in the real world below this scale are parametrized, so a completely artificial break occurs between what is resolved and what is parametrized. To compound the problem, the parametrization assumes that organization on scales which cannot be resolved is not important for determining the properties of the convection or its influence on the large-scale flow. But there is mounting evidence that this creates all sorts of problems in the models. Several decades have been invested in developing convection parametrizations and even after all this time and effort no fully satisfactory solution has been found. We therefore propose to take a radical but entirely logical approach to this problem. It is now possible to run models that do resolve convective systems explicitly (at least down to scales of around 1km) over very large domains that encompass all of the important scales mentioned above. Such cloud system resolving models provide a new tool for understanding how convection really works and organises itself, and how it should be parametrized in climate models. Our proposal links these models with new data from satellites and from the surface that will give us an unprecedented view of the evolution of clouds and rain-producing systems. We will bring this unique combination of modelling and observations to bear on what is regarded as one of the most fundamental problems in weather and climate. The results of the work will inform the development of a new generation of more accurate atmospheric models that will find employment in both climate prediction and weather forecasting.","4":"24042157-1C77-4192-BE45-7638411651D8"},{"1":"EP/E061931/1","2":"The EmergeNET: Towards a Unifying Investigation in Emergence, Emergent Phenomena and Complexity","3":"The very definition of complexity and emergence is itself a non-trivial problem. Complexity refers to situations where many simple interacting parts produce an unexpected collective behaviour. This calls for another imprecise concept that is emergence. Complex systems can display the emergence of properties at the macroscopic level that are not found at the microscopic level. One important example of emergence is self-organization. Self-organisation occurs as parts of a complex adaptive system, such as oil molecules in a thin layer, self-organise to form patterns in a state that is statistically stable. The basic mechanism for self organisation comes from feedback. Each part can communicate with its neighbours and arrange into a common collective behaviour. Sometimes, regardless the precise dynamics of the interactions, the evolution of the system is represented by some statistically stable state. This means that this steady state is an 'attractor' in the phase space for the system dynamics and accounts for the robustness of complex systems with respect to external perturbation. The Properties of a complex physical system are emergent just in case they are neither (i) properties had by any parts of the system taken in isolation nor (ii) resultant of a mere summation of properties of parts of the system. The above definition of emergence shows how this process may apply to many systems across all length scales and complexity scales. However, when one moves from physical to social, medical, or even artificial systems, the ability to spot and work with / around this concept becomes more important.Further, the ability to spot emergent entities occurring in very different situations would seem to be vital to allow this concept to grow and be developed. A substantial trans-disciplinary theory of emergence would greatly contribute to the development of a broader application and understanding of complexity science. The EPSRC IDEAS Factory on emergence tackled all of these issues, resulting in a number of funded projects. In order to maintain good communication between those involved in the projects, to further address the outcomes of the sandpit, and to encourage interdisplinary communication surrounding complexity and emergence, a network to cover emergence across disciplines is required.","4":"97F2C95C-3DBC-490C-8E32-6ABD6E6281FC"},{"1":"EP/E062814/1","2":"The EmergeNET: Towards a Unifying Investigation in Emergence, Emergent Phenomena and Complexity","3":"The very definition of complexity and emergence is itself a non-trivial problem. Complexity refers to situations where many simple interacting parts produce an unexpected collective behaviour. This calls for another imprecise concept that is emergence. Complex systems can display the emergence of properties at the macroscopic level that are not found at the microscopic level. One important example of emergence is self-organization. Self-organisation occurs as parts of a complex adaptive system, such as oil molecules in a thin layer, self-organise to form patterns in a state that is statistically stable. The basic mechanism for self organisation comes from feedback. Each part can communicate with its neighbours and arrange into a common collective behaviour. Sometimes, regardless the precise dynamics of the interactions, the evolution of the system is represented by some statistically stable state. This means that this steady state is an 'attractor' in the phase space for the system dynamics and accounts for the robustness of complex systems with respect to external perturbation. The Properties of a complex physical system are emergent just in case they are neither (i) properties had by any parts of the system taken in isolation nor (ii) resultant of a mere summation of properties of parts of the system. The above definition of emergence shows how this process may apply to many systems across all length scales and complexity scales. However, when one moves from physical to social, medical, or even artificial systems, the ability to spot and work with / around this concept becomes more important.Further, the ability to spot emergent entities occurring in very different situations would seem to be vital to allow this concept to grow and be developed. A substantial trans-disciplinary theory of emergence would greatly contribute to the development of a broader application and understanding of complexity science. The EPSRC IDEAS Factory on emergence tackled all of these issues, resulting in a number of funded projects. In order to maintain good communication between those involved in the projects, to further address the outcomes of the sandpit, and to encourage interdisplinary communication surrounding complexity and emergence, a network to cover emergence across disciplines is required.","4":"9F445DD2-138B-4F90-8271-3C3038428AB3"},{"1":"EP/E061850/1","2":"The EmergeNET: Towards a Unifying Investigation in Emergence, Emergent Phenomena and Complexity","3":"The very definition of complexity and emergence is itself a non-trivial problem. Complexity refers to situations where many simple interacting parts produce an unexpected collective behaviour. This calls for another imprecise concept that is emergence. Complex systems can display the emergence of properties at the macroscopic level that are not found at the microscopic level. One important example of emergence is self-organization. Self-organisation occurs as parts of a complex adaptive system, such as oil molecules in a thin layer, self-organise to form patterns in a state that is statistically stable. The basic mechanism for self organisation comes from feedback. Each part can communicate with its neighbours and arrange into a common collective behaviour. Sometimes, regardless the precise dynamics of the interactions, the evolution of the system is represented by some statistically stable state. This means that this steady state is an 'attractor' in the phase space for the system dynamics and accounts for the robustness of complex systems with respect to external perturbation. The Properties of a complex physical system are emergent just in case they are neither (i) properties had by any parts of the system taken in isolation nor (ii) resultant of a mere summation of properties of parts of the system. The above definition of emergence shows how this process may apply to many systems across all length scales and complexity scales. However, when one moves from physical to social, medical, or even artificial systems, the ability to spot and work with / around this concept becomes more important.Further, the ability to spot emergent entities occurring in very different situations would seem to be vital to allow this concept to grow and be developed. A substantial trans-disciplinary theory of emergence would greatly contribute to the development of a broader application and understanding of complexity science. The EPSRC IDEAS Factory on emergence tackled all of these issues, resulting in a number of funded projects. In order to maintain good communication between those involved in the projects, to further address the outcomes of the sandpit, and to encourage interdisplinary communication surrounding complexity and emergence, a network to cover emergence across disciplines is required.","4":"B93FB4BD-B2CA-41E8-9266-1BC48B9106A5"},{"1":"2842444","2":"Machine learning discovery of post-transcriptionally regulated gene candidates using imaging and genomic data","3":"The volume of microscopy images and their associated genome-wide metadata that are generated by many biologists is too large to be effectively browsed and interpreted in order to formulate novel testable hypotheses. Overcoming this challenge is of key importance for future biological discovery as data volumes continue to grow exponentially. The project will bridge this important technological gap in a unique partnership between a biomedical research lab lead by Prof. Ilan and Zegami (https://zegami.com), an Oxford University spinout company, whose founder and Chief Scientific Officer is Stephen Taylor. Zegami provides innovative cloud-based software to display vast databases of images sorted interactively in real time with complex metadata (example described in: https://www.youtube.com/watch?v=32bqn-Agt08). The main aim of the project is to develop machine learning software (using supervised random forest algorithms) that automatically generates scientific hypotheses based on correlations between existing high quality imaging data and genome-wide bioinformatics data, with guidance from the user. BBSRC REMIT AND FUNDING PORTFOLIO: &quot;Tools and technology underpinning biological research&quot;, specifically &quot;data driven biology&quot; and &quot;exploiting new ways of working&quot; and the area of artificial intelligence using supervised machine learning algorithms.","4":"BBD12261-5E4A-490D-ABEF-F148252FD197"},{"1":"2108069","2":"Machine learning discovery of post-transcriptionally regulated gene candidates using imaging and genomic data","3":"The volume of microscopy images and their associated genome-wide metadata that are generated by many biologists is too large to be effectively browsed and interpreted in order to formulate novel testable hypotheses. Overcoming this challenge is of key importance for future biological discovery as data volumes continue to grow exponentially. The project will bridge this important technological gap in a unique partnership between a biomedical research lab lead by Prof. Ilan and Zegami (https://zegami.com), an Oxford University spinout company, whose founder and Chief Scientific Officer is Stephen Taylor. Zegami provides innovative cloud-based software to display vast databases of images sorted interactively in real time with complex metadata (example described in: https://www.youtube.com/watch?v=32bqn-Agt08). The main aim of the project is to develop machine learning software (using supervised random forest algorithms) that automatically generates scientific hypotheses based on correlations between existing high quality imaging data and genome-wide bioinformatics data, with guidance from the user. BBSRC REMIT AND FUNDING PORTFOLIO: &quot;Tools and technology underpinning biological research&quot;, specifically &quot;data driven biology&quot; and &quot;exploiting new ways of working&quot; and the area of artificial intelligence using supervised machine learning algorithms.","4":"29BEA246-1E19-46B3-8650-22FD773E24FB"},{"1":"NE/H006818/1","2":"The role of immune-mediated female sperm selection in temporal dynamics of fertilisation bias","3":"The way paternity is distributed across males has considerable impact on the ecology and evolution of a population, regulating the effective population size and the amount of standing genetic variance. Despite intense selection on reproductive success paternity remains highy variable, and understanding the mechanisms underpinning this variation is a fundamental challenge in Biology. Increasing evidence indicates that an important source of variation in paternity originates from processes occurring after insemination. In most organisms the ejaculates of multiple males often compete to fertilise a set of eggs, and females can drastically influence the outcome of this competition through biased responses to the sperm of different males. Females are expected to bias fertilisation in favour of males of higher genetic quality in order to increase the success of their offspring. Because the genetic diversity (heterozyosity) of an individual promotes survival, a increasingly topical hypothesis is that females preferentially utilise the sperm of males that are genetically different from the female to promote offspring heterozygosity. However, evidence for this hypothesis is ambiguous and -contrary to theory- females may often bias fertilisation in favour of genetically similar rather than dissimilar males. Recent evidence from different species including our own work in the red junglefowl indicates that this counterintuitive response may be regulated by genetic similarity at the Major Histocompatibility Complex (MHC). The MHC is a complex of genes that play a fundamental role in immune responses allowing the organism to recognise self from non-self and respond against cells that are not recognised as self. While this immune response enables the organism to combat pathogens and parasites it may also result in a side-effect differential response to sperm of different males. Namely, we expect the female immune system to tolerate sperm of males that share MHC genes with the female and discriminate against the sperm of males that have a different MHC profile. As it is typical of similar immune responses, we also expect female response to the sperm of a certain MHC similarity with the female to change as the female is exposed to successive inseminations with the same type of sperm. It is plausible that, through continued exposure, the female immune system 'learns' to recognise MHC-similar sperm type, thus reducing the bias in paternity. This novel hypothesis is founded on well established immunological mechanisms and represents a biologically plausible proximate explanation consistent with an emergent trend of studies indicating that fertilisation may be biased in favour of genetically similar partners. Elucidating the consequences of MHC-mediated immune responses for female sperm selection would therefore contribute to unravel the mechanisms underpinning variation in paternity in natural populations. The aim of the proposed research is to test experimentally different key predictions of the hypothesis that fertilisation is biased by MHC-mediated immunological responses of the female reproductive tract to the ejaculates of different males. We will test these predictions in a well characterised population of red junglefowl. Red junglefowl are the wild ancestor of the domestic chicken and an ideal system to study MHC-mediated female sperm selection. First, females typically obtain ejaculates from multiple males and are known to bias fertilisation in different ways, including in favour of MHC-similar sperm. Second, poultry techniques of artificial insemination and sperm assays enable us to study post-insemination processes non-invasively and under controlled conditions. Third, the MHC of the fowl is very simple and extremely well characterised. Finally, we have a deep understanding of the mechanisms that modulate paternity skews in this species, including the role of MHC similarity.","4":"2CDDE71A-3336-4CF2-B27F-F8A962B0217C"},{"1":"NE/H008047/1","2":"The role of immune-mediated female sperm selection in temporal dynamics of fertilisation bias","3":"The way paternity is distributed across males has considerable impact on the ecology and evolution of a population, regulating the effective population size and the amount of standing genetic variance. Despite intense selection on reproductive success paternity remains highy variable, and understanding the mechanisms underpinning this variation is a fundamental challenge in Biology. Increasing evidence indicates that an important source of variation in paternity originates from processes occurring after insemination. In most organisms the ejaculates of multiple males often compete to fertilise a set of eggs, and females can drastically influence the outcome of this competition through biased responses to the sperm of different males. Females are expected to bias fertilisation in favour of males of higher genetic quality in order to increase the success of their offspring. Because the genetic diversity (heterozyosity) of an individual promotes survival, a increasingly topical hypothesis is that females preferentially utilise the sperm of males that are genetically different from the female to promote offspring heterozygosity. However, evidence for this hypothesis is ambiguous and -contrary to theory- females may often bias fertilisation in favour of genetically similar rather than dissimilar males. Recent evidence from different species including our own work in the red junglefowl indicates that this counterintuitive response may be regulated by genetic similarity at the Major Histocompatibility Complex (MHC). The MHC is a complex of genes that play a fundamental role in immune responses allowing the organism to recognise self from non-self and respond against cells that are not recognised as self. While this immune response enables the organism to combat pathogens and parasites it may also result in a side-effect differential response to sperm of different males. Namely, we expect the female immune system to tolerate sperm of males that share MHC genes with the female and discriminate against the sperm of males that have a different MHC profile. As it is typical of similar immune responses, we also expect female response to the sperm of a certain MHC similarity with the female to change as the female is exposed to successive inseminations with the same type of sperm. It is plausible that, through continued exposure, the female immune system 'learns' to recognise MHC-similar sperm type, thus reducing the bias in paternity. This novel hypothesis is founded on well established immunological mechanisms and represents a biologically plausible proximate explanation consistent with an emergent trend of studies indicating that fertilisation may be biased in favour of genetically similar partners. Elucidating the consequences of MHC-mediated immune responses for female sperm selection would therefore contribute to unravel the mechanisms underpinning variation in paternity in natural populations. The aim of the proposed research is to test experimentally different key predictions of the hypothesis that fertilisation is biased by MHC-mediated immunological responses of the female reproductive tract to the ejaculates of different males. We will test these predictions in a well characterised population of red junglefowl. Red junglefowl are the wild ancestor of the domestic chicken and an ideal system to study MHC-mediated female sperm selection. First, females typically obtain ejaculates from multiple males and are known to bias fertilisation in different ways, including in favour of MHC-similar sperm. Second, poultry techniques of artificial insemination and sperm assays enable us to study post-insemination processes non-invasively and under controlled conditions. Third, the MHC of the fowl is very simple and extremely well characterised. Finally, we have a deep understanding of the mechanisms that modulate paternity skews in this species, including the role of MHC similarity.","4":"F292C306-3A4B-4EC8-99AE-0DD36C3CF9C4"},{"1":"10053190","2":"CO-CREATIVE IMPROVED UNDERSTANDING AND AWARENESS OF MULTI-HAZARD RISKS FOR DISASTER RESILIENT SOCIETY(C2IMPRESS)","3":"There is a lack of data, understanding and awareness in Europe and in the world of compound weather and climate extremes. Insufficient governance function also affect the related vulnerability, risk and resilience of people, communities and places. The C2IMPRESS project aims to bring a radical paradigm shift to disaster and hazard research and innovation by moving away from traditional ‘hazard centric’ approach/ framework to a novel ‘place and people’ centred integrated multi-hazard risk and resilient assessment framework. Guided by this framework, the C2IMPRESS project aims to offer an ensemble of innovative revolutionary models, methods, frameworks, tools and technologies that are holistic and robust enough to provide appropriate fine-grained spatio-temporal qualitative and quantitative data, locally appropriate solutions, better prediction with lower uncertainty on risks of single or multiple hazards stemming from extreme weather events like floods, wildfires etc. under different climate change scenarios. Breakthrough innovations from C2IMPRESS project will be System-of-Systems for Multi-Hazard Risk Intelligence Network (SoS4MHRIN) platform (supported by the robust Earth System Dynamic Intelligence (ESDI) and Information Physical Artificial Intelligence (IPAI)), innovative Agent Based Model, polycentric risk governance, multi-actor decision support microservices and a suite of citizen engagement technologies and tools as well as novel co design and co-creation approach for socio-technical innovations, knowledge production and validation to empower citizens and society with climate actions for a sustainable transition to a just risk resilient society. With these social and technical innovations -as novel processes and products- the C2IMPRESS project will provide better understanding and public awareness on multi-hazard risks, the associated multidimensional impacts, vulnerabilities and resilience of extreme weather events in 4 case study areas in Europe.","4":"6843829A-AB37-4E74-A739-3C39683C28FC"},{"1":"10054862","2":"CO-CREATIVE IMPROVED UNDERSTANDING AND AWARENESS OF MULTI-HAZARD RISKS FOR DISASTER RESILIENT SOCIETY","3":"There is a lack of data, understanding and awareness in Europe and in the world of compound weather and climate extremes. Insufficient governance function also affect the related vulnerability, risk and resilience of people, communities and places. The C2IMPRESS project aims to bring a radical paradigm shift to disaster and hazard research and innovation by moving away from traditional ‘hazard centric’ approach/ framework to a novel ‘place and people’ centred integrated multi-hazard risk and resilient assessment framework. Guided by this framework, the C2IMPRESS project aims to offer an ensemble of innovative revolutionary models, methods, frameworks, tools and technologies that are holistic and robust enough to provide appropriate fine-grained spatio-temporal qualitative and quantitative data, locally appropriate solutions, better prediction with lower uncertainty on risks of single or multiple hazards stemming from extreme weather events like floods, wildfires etc. under different climate change scenarios. Breakthrough innovations from C2IMPRESS project will be System-of-Systems for Multi-Hazard Risk Intelligence Network (SoS4MHRIN) platform (supported by the robust Earth System Dynamic Intelligence (ESDI) and Information Physical Artificial Intelligence (IPAI)), innovative Agent Based Model, polycentric risk governance, multi-actor decision support microservices and a suite of citizen engagement technologies and tools as well as novel co design and co-creation approach for socio-technical innovations, knowledge production and validation to empower citizens and society with climate actions for a sustainable transition to a just risk resilient society. With these social and technical innovations -as novel processes and products- the C2IMPRESS project will provide better understanding and public awareness on multi-hazard risks, the associated multidimensional impacts, vulnerabilities and resilience of extreme weather events in 4 case study areas in Europe.","4":"27CC7440-BDEF-444C-96D5-B7EC03889FD8"},{"1":"NE/S016244/2","2":"Engineering Transformation for the Integration of Sensor Networks: A Feasibility Study - 'ENTRAIN'","3":"There is a need to make use of new digital data analysis techniques to improve our understanding of the environment. Data from a new generation of environmental sensors, combined with analyses based on Artificial Intelligence, has the potential to help us understand from human influences and long-term change are affecting the environment around us. Artificial Intelligence approaches enable computers to identify trends and relationships across different streams of data, often picking out patterns that would be too difficult or time-consuming for humans to identify manually.\\n\\nTo realise these benefits, data from diverse sensor networks must combined and analysed together. Currently many sensor networks are operated individually, and data are not readily combined due to differences in the way measurements are made (e.g. between weekly river samples and sub-second measurements of gases in the atmosphere). In addition, to combine these data in an automatic way without human intervention requires much finer and more consistent descriptions of the contents of data streams, so that machines can understand the content sufficiently. Links between sensors in space are also important, and machines will need an understanding of these links, not just in the sense of coordinates, but for example how sensors are linked along rivers. We can construct a digital representation of rivers in order to enable this.\\nWe will describe the various elements of a future environmental analysis system that will be required in order to achieve these benefits, and addressing some of these currently missing components. We will look at technologies, from databases to data transfer mechanisms, to understand how a system could be built.\\n\\nWe will use data from 3 NERC sensor networks measuring environmental variables from the atmosphere to river water quality, and show how this data can be automatically integrated in such a way that machines would be able to analyse it automatically.\\nA significant issue when monitoring with high-resolution sensors is how to handle problems in the data, which could include missing data, and erroneous values due to sensor failure. There is too much data for humans to manually view and check, and so automated approaches are needed. Currently these are often simple checks of individual data values against expected ranges, but again there are opportunities for artificial intelligence to improve this. AI approaches can look across multiple sensors, identify relationships, and find subtle changes in data signals, and this can be used to both identify data problems and to fix them through infilling. We will enhance the 3 NERC networks by testing and applying such approaches to data quality control.\\n\\nWe will investigate some fundamental limitations of high-resolution monitoring, the transfer of large amounts of data from the field site to the data centre, the security of such systems, and whether more processing could be done on the instruments themselves to reduce data transfer volumes.\\n\\nWe will meet with the public, with policy-makers, with industry and with researchers to discuss where there will be most to be gained from development of AI approaches to analysing environmental sensor data. We will develop ideas for future work to realise these gains, and will promote the benefits of an integrated system for environmental monitoring. These stakeholders are likely to include the Environment Agency, SEPA, Natural Resources Wales, Defra, Water companies, sensor network developers, and public organisations with an interest in the environment, including the National Trust, the Rivers Trusts, and local community groups.","4":"E4FF2955-557D-447F-AF5A-5C464031EE48"},{"1":"NE/S016236/1","2":"Engineering Transformation for the Integration of Sensor Networks: A Feasibility Study - 'ENTRAIN'","3":"There is a need to make use of new digital data analysis techniques to improve our understanding of the environment. Data from a new generation of environmental sensors, combined with analyses based on Artificial Intelligence, has the potential to help us understand from human influences and long-term change are affecting the environment around us. Artificial Intelligence approaches enable computers to identify trends and relationships across different streams of data, often picking out patterns that would be too difficult or time-consuming for humans to identify manually.\\n\\nTo realise these benefits, data from diverse sensor networks must combined and analysed together. Currently many sensor networks are operated individually, and data are not readily combined due to differences in the way measurements are made (e.g. between weekly river samples and sub-second measurements of gases in the atmosphere). In addition, to combine these data in an automatic way without human intervention requires much finer and more consistent descriptions of the contents of data streams, so that machines can understand the content sufficiently. Links between sensors in space are also important, and machines will need an understanding of these links, not just in the sense of coordinates, but for example how sensors are linked along rivers. We can construct a digital representation of rivers in order to enable this.\\nWe will describe the various elements of a future environmental analysis system that will be required in order to achieve these benefits, and addressing some of these currently missing components. We will look at technologies, from databases to data transfer mechanisms, to understand how a system could be built.\\n\\nWe will use data from 3 NERC sensor networks measuring environmental variables from the atmosphere to river water quality, and show how this data can be automatically integrated in such a way that machines would be able to analyse it automatically.\\nA significant issue when monitoring with high-resolution sensors is how to handle problems in the data, which could include missing data, and erroneous values due to sensor failure. There is too much data for humans to manually view and check, and so automated approaches are needed. Currently these are often simple checks of individual data values against expected ranges, but again there are opportunities for artificial intelligence to improve this. AI approaches can look across multiple sensors, identify relationships, and find subtle changes in data signals, and this can be used to both identify data problems and to fix them through infilling. We will enhance the 3 NERC networks by testing and applying such approaches to data quality control.\\n\\nWe will investigate some fundamental limitations of high-resolution monitoring, the transfer of large amounts of data from the field site to the data centre, the security of such systems, and whether more processing could be done on the instruments themselves to reduce data transfer volumes.\\n\\nWe will meet with the public, with policy-makers, with industry and with researchers to discuss where there will be most to be gained from development of AI approaches to analysing environmental sensor data. We will develop ideas for future work to realise these gains, and will promote the benefits of an integrated system for environmental monitoring. These stakeholders are likely to include the Environment Agency, SEPA, Natural Resources Wales, Defra, Water companies, sensor network developers, and public organisations with an interest in the environment, including the National Trust, the Rivers Trusts, and local community groups.","4":"32F336B8-9079-4B68-A1C2-BBC9C7162985"},{"1":"EP/F022913/1","2":"Understanding Corrosion and Passivation Processes in Light Alloys using Multinuclear NMR Techniques: A Visiting Fellowship for Prof. M. Forsyth","3":"This is a proposal for a 4 month visiting fellowship for Prof. Maria Forsyth of Monash University to visit the UK to carry out exploratory work on the use of nuclear magnetic resonance (NMR) to investigate the corrosion and passivation of aluminium and magnesium. She will be based in the Department of Physics at Warwick with Prof. Mark Smith (NMR), but also make frequent visits to Birmingham only 20 miles away to work with Dr Alison Davenport (corrosion) and Dr Melanie Britton (magnetic resonance imaging, MRI). Forsyth is without doubt an acknowledged world-leader in materials engineering, particularly related to polymer electrolytes, novel ionic electrolytes and electrochemical corrosion processes. This project will focus on the use of NMR techniques to study the corrosion and protection of light alloys based on magnesium and aluminium. The chemistry of protective passivating films will be determined with multinuclear NMR techniques to investigate the mechanism of formation and the way in which they block corrosion reactions. When corrosion does take place, corrosion pits penetrate into the metal. NMR will be used to determine the chemistry of the concentrated metal chloride solutions within these pits, and the transport of water and even ions that control pit stability. Finally, the possibility of using magnetic resonance imaging to observe the development of artificial pits will be explored. Very preliminary measurements have suggested that this challenging experimental approach is feasible, and could open up an entirely new approach for the study of pitting processes. In addition to exploring new experimental approaches to corrosion and passivation of metals, this work will develop a fundamental understanding of these processes that will be of practical value to the aerospace and automotive industries, where light alloys such as magnesium and aluminium are being increasingly used to improve fuel efficiency, and efforts are underway to extend the lifetime of structures (for example ageing aircraft). In such applications, corrosion and its prevention are critical to the safety of vehicles. This project will bring together a group of researchers with highly complementary expertise to develop NMR techniques, to include spectroscopy, relaxation, diffusion and imaging experiments to bring a novel approach to using magnetic resonance techniques to probe corrosion and passivation processes in light metal alloys. During the fellowship, the intention is to bring together the world-leading material's expertise of Forsyth with the NMR and materials expertise of the UK team to form a long-term collaboration between the UK and Australia that will make a unique contribution to this field. This will be an intensive burst of activity to kick start this new project. In addition, this will be an excellent opportunity for Prof. Forsyth to meet a range of other UK groups during her stay and to give a series of colloquia.","4":"0292E92C-B81D-4443-BB87-75B90C82849E"},{"1":"EP/F022859/1","2":"Understanding Corrosion and Passivation Processes in Light Alloys using Multinuclear NMR Techniques: A Visiting Fellowship for Prof. M. Forsyth","3":"This is a proposal for a 4 month visiting fellowship for Prof. Maria Forsyth of Monash University to visit the UK to carry out exploratory work on the use of nuclear magnetic resonance (NMR) to investigate the corrosion and passivation of aluminium and magnesium. She will be based in the Department of Physics at Warwick with Prof. Mark Smith (NMR), but also make frequent visits to Birmingham only 20 miles away to work with Dr Alison Davenport (corrosion) and Dr Melanie Britton (magnetic resonance imaging, MRI). Forsyth is without doubt an acknowledged world-leader in materials engineering, particularly related to polymer electrolytes, novel ionic electrolytes and electrochemical corrosion processes. This project will focus on the use of NMR techniques to study the corrosion and protection of light alloys based on magnesium and aluminium. The chemistry of protective passivating films will be determined with multinuclear NMR techniques to investigate the mechanism of formation and the way in which they block corrosion reactions. When corrosion does take place, corrosion pits penetrate into the metal. NMR will be used to determine the chemistry of the concentrated metal chloride solutions within these pits, and the transport of water and even ions that control pit stability. Finally, the possibility of using magnetic resonance imaging to observe the development of artificial pits will be explored. Very preliminary measurements have suggested that this challenging experimental approach is feasible, and could open up an entirely new approach for the study of pitting processes. In addition to exploring new experimental approaches to corrosion and passivation of metals, this work will develop a fundamental understanding of these processes that will be of practical value to the aerospace and automotive industries, where light alloys such as magnesium and aluminium are being increasingly used to improve fuel efficiency, and efforts are underway to extend the lifetime of structures (for example ageing aircraft). In such applications, corrosion and its prevention are critical to the safety of vehicles. This project will bring together a group of researchers with highly complementary expertise to develop NMR techniques, to include spectroscopy, relaxation, diffusion and imaging experiments to bring a novel approach to using magnetic resonance techniques to probe corrosion and passivation processes in light metal alloys. During the fellowship, the intention is to bring together the world-leading material's expertise of Forsyth with the NMR and materials expertise of the UK team to form a long-term collaboration between the UK and Australia that will make a unique contribution to this field. This will be an intensive burst of activity to kick start this new project. In addition, this will be an excellent opportunity for Prof. Forsyth to meet a range of other UK groups during her stay and to give a series of colloquia.","4":"B213E10E-817C-43A1-9C4D-961EAC6D5A8F"},{"1":"BB/E000134/1","2":"Multiple Scale and Multimodal Data and Information Fusion in Human Sensory Discrimination","3":"This is an inter-disciplinary project involving combined psychophysical studies of human uni- and cross-modal sensory discrimination and scene analysis, along with computational modeling of the data using multi-dimensional time series analysis. The psychophysical experiments will provide novel information on the use of multiple haptic inputs as a function of type and relative positions of receptors being stimulated, as well as information on how concurrent haptic and visual input is integrated. The computational studies will evaluate whether copula-based wavelet analyses are able to simulate human discrimination performance, with information being combined immediately across sensory inputs. As an alternative, this will be compared with approaches in which time series (including wavelet-based) analyses are conducted for each input before integration takes place across inputs. The work will provide a detailed account of how sensory integration operates within and across modalities, which can subsequently be used to guide the construction of artificial sensory systems.","4":"D9256449-21AB-47EB-8478-7E2739D825B2"},{"1":"BB/E000088/1","2":"Multiple Scale and Multimodal Data and Information Fusion in Human Sensory Discrimination","3":"This is an inter-disciplinary project involving combined psychophysical studies of human uni- and cross-modal sensory discrimination and scene analysis, along with computational modeling of the data using multi-dimensional time series analysis. The psychophysical experiments will provide novel information on the use of multiple haptic inputs as a function of type and relative positions of receptors being stimulated, as well as information on how concurrent haptic and visual input is integrated. The computational studies will evaluate whether copula-based wavelet analyses are able to simulate human discrimination performance, with information being combined immediately across sensory inputs. As an alternative, this will be compared with approaches in which time series (including wavelet-based) analyses are conducted for each input before integration takes place across inputs. The work will provide a detailed account of how sensory integration operates within and across modalities, which can subsequently be used to guide the construction of artificial sensory systems.","4":"587633F5-3663-4C7F-B52E-DB76C2E1B3BA"},{"1":"BB/I016961/1","2":"Bioprocessing Research For Cellular Products","3":"This project aims to develop novel preservation platform technologies required for the successful banking of human cells, an absolute prerequisite for their use as products. Many regenerative medicine products rely on the delivery of live cells to patients. At present this is exemplified by established therapeutic interventions such as bone marrow transplantation, blood transfusion and corneal grafting; future generations of products may include bio-artificial matrices that incorporate donor stem cells, for example bone replacement and repair devices, and artificial 'mini-organs' such as pancreas or liver. Current cryopreservation of stem cell based products results from historic work, using DMSO as a cryopreserving agent which is largely unsubstantiated with respect to final biological activity. DMSO can be toxic to cells, lead to low viabilities post thaw and both genetic and epigenetic instability (i.e loss of pluripotency) over long term culture. Cryopreservation of blood cells has been attempted previously, with limited success due to loss of cell integrity, primarily due to the breakdown of the cell membrane and consequent loss of overall cell structure. A variety of techniques have been investigated for delivering trehalose, a membrane impermeable cryoprotectant, into mammalian cells, including microinjection, ion channel stimulation, pore formation using mutant bacterial toxins, fluid phase endocytosis, and internal trehalose synthesis via genetic engineering but intracellular trehalose concentrations achieved in erythrocytes has not exceeded 50 mM and is therefore below thresholds for cryoprotection. Biopolymer mediated cell loading achieves substantially increased intracellular trehalose concentrations of up to 251 mM and a concomitant improvement of erythrocyte cryosurvival of up to 20.4 % as compared with conventional methods of loading trehalose into cells. The technology utilizes novel amphiphilic biopolymers that interact with the external cell membrane to enable penetration and retention of cryoprotectant agents into the cells. Membrane permeabilisation by these Cell Permeating Polymers (CPPs) is rapid and completely reversible via washing with buffer. Cellular uptake of trehalose is dependent on polymer molecular structure, concentration, pH, external trehalose concentration, incubation temperature and time. Optimization of these parameters imparts cellular osmoprotection. Overall, a total cell recovery through a single freeze-thaw cycle at -80oC of 82.6 % has been achieved, which compares with a recovery of only 0.8 % for cells frozen in PBS. This proposal aims to explore the CPP mediated loading of preservation agents into stem cells, to examine preservation by freezing and dessication and to arrive at integrated processing routes for the preparation of optimally stable stem cells.","4":"381E4C03-4349-48F5-B8E0-3A3ED78257A6"},{"1":"BB/I017062/1","2":"Bioprocessing Research For Cellular Products","3":"This project aims to develop novel preservation platform technologies required for the successful banking of human cells, an absolute prerequisite for their use as products. Many regenerative medicine products rely on the delivery of live cells to patients. At present this is exemplified by established therapeutic interventions such as bone marrow transplantation, blood transfusion and corneal grafting; future generations of products may include bio-artificial matrices that incorporate donor stem cells, for example bone replacement and repair devices, and artificial 'mini-organs' such as pancreas or liver. Current cryopreservation of stem cell based products results from historic work, using DMSO as a cryopreserving agent which is largely unsubstantiated with respect to final biological activity. DMSO can be toxic to cells, lead to low viabilities post thaw and both genetic and epigenetic instability (i.e loss of pluripotency) over long term culture. Cryopreservation of blood cells has been attempted previously, with limited success due to loss of cell integrity, primarily due to the breakdown of the cell membrane and consequent loss of overall cell structure. A variety of techniques have been investigated for delivering trehalose, a membrane impermeable cryoprotectant, into mammalian cells, including microinjection, ion channel stimulation, pore formation using mutant bacterial toxins, fluid phase endocytosis, and internal trehalose synthesis via genetic engineering but intracellular trehalose concentrations achieved in erythrocytes has not exceeded 50 mM and is therefore below thresholds for cryoprotection. Biopolymer mediated cell loading achieves substantially increased intracellular trehalose concentrations of up to 251 mM and a concomitant improvement of erythrocyte cryosurvival of up to 20.4 % as compared with conventional methods of loading trehalose into cells. The technology utilizes novel amphiphilic biopolymers that interact with the external cell membrane to enable penetration and retention of cryoprotectant agents into the cells. Membrane permeabilisation by these Cell Permeating Polymers (CPPs) is rapid and completely reversible via washing with buffer. Cellular uptake of trehalose is dependent on polymer molecular structure, concentration, pH, external trehalose concentration, incubation temperature and time. Optimization of these parameters imparts cellular osmoprotection. Overall, a total cell recovery through a single freeze-thaw cycle at -80oC of 82.6 % has been achieved, which compares with a recovery of only 0.8 % for cells frozen in PBS. This proposal aims to explore the CPP mediated loading of preservation agents into stem cells, to examine preservation by freezing and dessication and to arrive at integrated processing routes for the preparation of optimally stable stem cells.","4":"2631A9DB-0009-4032-8C90-AF38EDEE17D9"},{"1":"BB/J001694/2","2":"Extending the Boundaries of Nucleic Acid Chemistry","3":"This project introduces a new paradigm into nucleic acids research, 'click ligation', which is an extremely efficient purely chemical (as opposed to biological) method for joining DNA and RNA strands to make large biologically active constructs (DNA and RNA are the molecules in cells that store and transmit genetic information). Although the new chemistry produces an unnatural linkage, it can be read through by DNA polymerases, the enzymes that make new copies of DNA in living systems during cell division. Thus our new artificial DNA linkage is truly biocompatible. Unlike biological ligation, this chemical reaction can be carried out on both normal and chemically-modified DNA, on any scale under a wide range of physical conditions. This makes it useful for biotechnology, i.e. the large scale production of medicinally important biological constructs. We will use click ligation to make very long DNA strands, enabling the assembly of chemically-modified synthetic genes which can be used to make proteins. Our work will allow the insertion of structural motifs such as quadruplexes and chemical modifications such as methylated and hydroxymethylated bases into genes for the study of gene expression and epigenetics. These modifications are thought to switch genes on and off by mechanisms that are not yet fully understood. They are currently the focus of intense research as aberrant genetic switches are implicated in diseases such as cancer and also in ageing. We will make fluorescently labelled DNA and RNA constructs and we will use them to investigate the physical structures of genes (including gene loops) and to understand the dynamics of long-range interactions in chromatin, part of the structure of a chromosome, by super resolution microscopy. This will allow us to understand the relationship between the tight packaging of DNA in cells and its ability to regulate the synthesis of proteins. We will prepare fluorescently labelled RNA substrates to investigate mechanisms used by the influenza virus to make proteins and to replicate (copy) itself: theses phenomena will be studied by single-molecule FRET (a very sensitive technique for measuring distances between two fluorescent labels) and super-resolution imaging. This will help us to understand the biology of RNA viruses, an important step towards developing improved therapies. We will use click ligation to build artificial molecular machines that will be designed to carry out unique sets of chemical reactions in a precisely controlled manner. This technology may lead to new ways to develop biologically active compounds including drugs. An internationally-leading team from Southampton and Oxford has been assembled and extensive preliminary studies have been carried out to prove feasibility.","4":"A7B59F76-88BE-423A-AF71-20BC3A3B1AEC"},{"1":"BB/J00054X/1","2":"Extending the Boundaries of Nucleic Acid Chemistry","3":"This project introduces a new paradigm into nucleic acids research, 'click ligation', which is an extremely efficient purely chemical (as opposed to biological) method for joining DNA and RNA strands to make large biologically active constructs (DNA and RNA are the molecules in cells that store and transmit genetic information). Although the new chemistry produces an unnatural linkage, it can be read through by DNA polymerases, the enzymes that make new copies of DNA in living systems during cell division. Thus our new artificial DNA linkage is truly biocompatible. Unlike biological ligation, this chemical reaction can be carried out on both normal and chemically-modified DNA, on any scale under a wide range of physical conditions. This makes it useful for biotechnology, i.e. the large scale production of medicinally important biological constructs. We will use click ligation to make very long DNA strands, enabling the assembly of chemically-modified synthetic genes which can be used to make proteins. Our work will allow the insertion of structural motifs such as quadruplexes and chemical modifications such as methylated and hydroxymethylated bases into genes for the study of gene expression and epigenetics. These modifications are thought to switch genes on and off by mechanisms that are not yet fully understood. They are currently the focus of intense research as aberrant genetic switches are implicated in diseases such as cancer and also in ageing. We will make fluorescently labelled DNA and RNA constructs and we will use them to investigate the physical structures of genes (including gene loops) and to understand the dynamics of long-range interactions in chromatin, part of the structure of a chromosome, by super resolution microscopy. This will allow us to understand the relationship between the tight packaging of DNA in cells and its ability to regulate the synthesis of proteins. We will prepare fluorescently labelled RNA substrates to investigate mechanisms used by the influenza virus to make proteins and to replicate (copy) itself: theses phenomena will be studied by single-molecule FRET (a very sensitive technique for measuring distances between two fluorescent labels) and super-resolution imaging. This will help us to understand the biology of RNA viruses, an important step towards developing improved therapies. We will use click ligation to build artificial molecular machines that will be designed to carry out unique sets of chemical reactions in a precisely controlled manner. This technology may lead to new ways to develop biologically active compounds including drugs. An internationally-leading team from Southampton and Oxford has been assembled and extensive preliminary studies have been carried out to prove feasibility.","4":"8F6E20A4-9F09-468F-88A7-6E34125F0366"},{"1":"EP/D062934/1","2":"Learning the Structure of Music","3":"This project is aimed at the development of models and tools for the application of novel probabilistic machine learning techniques to the analysis of music. The underlying theme of the project is the learning of patterns linking different data arising simultaneously from the same piece of music. The sources of data will be as follows: a) musical scores (MIDI format), b) audio (recordings of the pieces), c) worm data (charting performance information), d) EEG data (of subjects listening to the music) and d) fMRI data (of subjects listening to the music).The linking patterns that we will be seeking involve pairs of data streams as follows: a) musical scores with worm data, b) musical scores with fMRI data and c) audio with EEG data. The first pair will be used to identify typical performance patterns of particular performers. The second and the third pairs will be used to identify the effects in the brain of particular musical patterns (such as melodic sequences, musical phrasings, harmonic progressions, etc.).The project will advance our understanding of the relationship between musical structure and performance and experience. The potential of such developments is quite wide ranging, with potential application in music therapy and entertainment. For example, it will contribute to the development of systems for artificial performance of music imitating the style of a performer on pieces that he or she may have never played before and systems for musical composition tailored to achieve specific effects (or moods) on the listener.","4":"9B21BF49-C1F6-4016-8E47-E22135DE8D62"},{"1":"EP/D063612/1","2":"Learning the Structure of Music","3":"This project is aimed at the development of models and tools for the application of novel probabilistic machine learning techniques to the analysis of music. The underlying theme of the project is the learning of patterns linking different data arising simultaneously from the same piece of music. The sources of data will be as follows: a) musical scores (MIDI format), b) audio (recordings of the pieces), c) worm data (charting performance information), d) EEG data (of subjects listening to the music) and d) fMRI data (of subjects listening to the music).The linking patterns that we will be seeking involve pairs of data streams as follows: a) musical scores with worm data, b) musical scores with fMRI data and c) audio with EEG data. The first pair will be used to identify typical performance patterns of particular performers. The second and the third pairs will be used to identify the effects in the brain of particular musical patterns (such as melodic sequences, musical phrasings, harmonic progressions, etc.).The project will advance our understanding of the relationship between musical structure and performance and experience. The potential of such developments is quite wide ranging, with potential application in music therapy and entertainment. For example, it will contribute to the development of systems for artificial performance of music imitating the style of a performer on pieces that he or she may have never played before and systems for musical composition tailored to achieve specific effects (or moods) on the listener.","4":"751B256D-4C70-4C80-9896-4278C373A2AE"},{"1":"EP/R020159/1","2":"Quantum algorithms for optimised planning/scheduling applications (Feasibility Study)","3":"This project will investigate the technical and business feasibility of exploiting quantum algorithms for optimised planning tasks, in close collaboration with key industry and academic partners. It aims to prove the technical feasibility of enhancing existing artificial intelligence (AI) planning techniques with quantum algorithms, either as fully quantum or hybrid solutions, combining both quantum and conventional computing methods. We will perform experiments to establish benchmarks for enhancing AI planning techniques with early quantum annealing algorithms, and then determine how they might be further enhanced with other universal quantum computing or 'circuit-model' approaches. In addition, this project will perform a market assessment for quantum-enhanced optimised planning solutions and determine the business feasibility of commercialising them for several markets, including telecoms network optimisation, distribution logistics and operational planning. This will help to stimulate wider interest with potential end-users and quantum computing vendors to develop optimisation tools for specific markets, and deliver potential major productivity gains for transport, logistics, energy and finance.","4":"A6C31831-A5E4-4144-8E74-2092B8F0EF24"},{"1":"133087","2":"Quantum algorithms for optimised planning/scheduling applications","3":"This project will investigate the technical and business feasibility of exploiting quantum algorithms for optimised planning tasks, in close collaboration with key industry and academic partners. It aims to prove the technical feasibility of enhancing existing artificial intelligence (AI) planning techniques with quantum algorithms, either as fully quantum or hybrid solutions, combining both quantum and conventional computing methods. We will perform experiments to establish benchmarks for enhancing AI planning techniques with early quantum annealing algorithms, and then determine how they might be further enhanced with other universal quantum computing or 'circuit-model' approaches. In addition, this project will perform a market assessment for quantum-enhanced optimised planning solutions and determine the business feasibility of commercialising them for several markets, including telecoms network optimisation, distribution logistics and operational planning. This will help to stimulate wider interest with potential end-users and quantum computing vendors to develop optimisation tools for specific markets, and deliver potential major productivity gains for transport, logistics, energy and finance.","4":"799D6821-BCCA-4B53-9F5C-A67207AC8F59"},{"1":"EP/R020426/1","2":"Quantum algorithms for optimised planning/scheduling applications","3":"This project will investigate the technical and business feasibility of exploiting quantum algorithms for optimised planning tasks, in close collaboration with key industry and academic partners. It aims to prove the technical feasibility of enhancing existing artificial intelligence (AI) planning techniques with quantum algorithms, either as fully quantum or hybrid solutions, combining both quantum and conventional computing methods. We will perform experiments to establish benchmarks for enhancing AI planning techniques with early quantum annealing algorithms, and then determine how they might be further enhanced with other universal quantum computing or 'circuit-model' approaches. In addition, this project will perform a market assessment for quantum-enhanced optimised planning solutions and determine the business feasibility of commercialising them for several markets, including telecoms network optimisation, distribution logistics and operational planning. This will help to stimulate wider interest with potential end-users and quantum computing vendors to develop optimisation tools for specific markets, and deliver potential major productivity gains for transport, logistics, energy and finance.","4":"B2538D14-C32C-45F3-B5DE-EA6E236332E6"},{"1":"EP/D040345/1","2":"Isotope Profiling of Drugs: A Tool to Disrupt Organised Crime, Detect Serious Crime and Reduce Volume Crime.","3":"This research proposal addresses the needs identified by end-users within UK law enforcement agencies for forensic data to support intelligence lead policing as well as the demands placed on them to achieve secure convictions. Isotope profiling in particular has been identified by the Forensic Science Service as a potentially powerful tool to address the intelligence needs for intelligence lead policing.Clandestine synthesis and movement of drugs are a global problem run by many international criminal networks. Increasing drug linkage information will lead to an increase in objective scientific intelligence about international networks and improve opportunities to disrupt such networks.Of the illicit drugs, homemade and imported MDMA ('Ecstasy') is of particular concern to most law enforcement agencies in the UK. For example, the most recent NCIS Scottish Strategic Assessment has determined a significant increase in the threat to Scottish communities posed by organised crime.The situation is one of increased sophistication and globalisation, with longstanding and well-established links between organised crimein the UK, Europe, the former Soviet Bloc countries, Asia and South America.","4":"4CDC8769-D5F1-49F6-9DD5-552B5B57701A"},{"1":"EP/D040116/1","2":"Isotope Profiling of Drugs: A Tool to Disrupt Organised Crime, Detect Serious Crime and Reduce Volume Crime.","3":"This research proposal addresses the needs identified by end-users within UK law enforcement agencies for forensic data to support intelligence lead policing as well as the demands placed on them to achieve secure convictions. Isotope profiling in particular has been identified by the Forensic Science Service as a potentially powerful tool to address the intelligence needs for intelligence lead policing.Clandestine synthesis and movement of drugs are a global problem run by many international criminal networks. Increasing drug linkage information will lead to an increase in objective scientific intelligence about international networks and improve opportunities to disrupt such networks.Of the illicit drugs, homemade and imported MDMA ('Ecstasy') is of particular concern to most law enforcement agencies in the UK. For example, the most recent NCIS Scottish Strategic Assessment has determined a significant increase in the threat to Scottish communities posed by organised crime.The situation is one of increased sophistication and globalisation, with longstanding and well-established links between organised crimein the UK, Europe, the former Soviet Bloc countries, Asia and South America.","4":"66AE6A92-0D7B-4D1E-AF18-3771FCA3AE45"},{"1":"EP/J021431/1","2":"University of East Anglia - Equipment Account","3":"Ultrafast laser technology has advanced to the extent that experiments of a complexity which was unimaginable only a few years ago now fall within the realms of the possible, and have the potential to become routine. Modern solid state laser sources produce ultrastable pulses a few million billionths of a second wide with extreme stability over most of the electromagnetic spectrum. This opens up almost any atomic or molecular process to real time interrogation. In this proposal we describe three experiments at the cutting edge of advanced laser spectroscopy. Our objective is to develop and apply these experiments to important problems in molecular and biomolecular science, with a view to demonstrating their utility in, and with the objective of establishing them as important new tools for, materials characterisation. To this end we have established collaborations with world leading laboratories in molecular and biomolecular materials science who will be the first users of the new methods.\\n\\nThe first experiment, 2D electronic spectroscopy(ES), is a unique tool for the study of electronic coupling and energy transport in (bio-)molecular assemblies. These processes are central to the collection and utilization of solar energy and in the operation of photoactivated nanomaterials. The experiment measures the correlation of the coherent excitation and emission frequencies in the visible region of the spectrum in a three pulse four wave mixing experiment. The measurement can be thought of as the optical analog of 2D NMR, in that it reveals couplings between electronic transitions that are obscured in the linear absorption spectrum. Such couplings are the underlying mechanism for energy and charge transport in both natural and artificial solar energy collectors, and thus need to be characterised and understood. In addition the same experiment resolves the temporal evolution of the energy flow in the molecular assembly with femtosecond resolution by varying the inter-pulse timings. An extension of this experiment to include polarization resolved data, will introduce a correlation between 2D spectra and molecular structure, and thus reveal the spatial arrangement of the chromophores. We will apply 2DES to elucidate excitation dynamics in multi-heme proteins and artificial porphyrin arrays, both of which figure prominently in solar energy conversion schemes and the latter can act as molecular wires in molecular electronics. The 2DES will provide the first direct measurement of the route and mechanism of energy transport in these molecular materials. How this correlates with structure will inform future designs strategies. In addition many heme proteins have unknown or disputed structures, so 2DES will provide new structural data. In short, 2DES has the power do for electronic structure what 2D NMR has done for nuclear structure.\\n\\nThe next two experiments report Raman and IR spectra of electronically excited molecules as a function of time after excitation. Excited state dynamics are a critical component of photoactivated molecular devices, where they act as transducer between optical and mechanical energy, by means of changes in shape or charge. Vibrational spectroscopy yields a detailed picture of the nuclear structure, and such measurements in real time allow us to track the structural changes which act as the driving force for motion in molecular machines. The time resolved coherent Raman experiment (FSRS) is well established. The transient IR measurement we will develop will permit IR detection in the visible region, using the same detection apparatus as Raman. This new method overcomes the limited spectral resolution of traditional IR detectors, and will permit the observation of subtle changes in bond lengths and angles which accompany structural change on a single electronic surface. These tools will be applied to investigate the mechanism of operation of molecular motors and molecular switches in a variety of environments.","4":"028AA73E-33F8-4B14-A616-D446A3C9CA86"},{"1":"EP/J009148/1","2":"Ultrafast Multidimensional Spectroscopy for Photomolecular Science","3":"Ultrafast laser technology has advanced to the extent that experiments of a complexity which was unimaginable only a few years ago now fall within the realms of the possible, and have the potential to become routine. Modern solid state laser sources produce ultrastable pulses a few million billionths of a second wide with extreme stability over most of the electromagnetic spectrum. This opens up almost any atomic or molecular process to real time interrogation. In this proposal we describe three experiments at the cutting edge of advanced laser spectroscopy. Our objective is to develop and apply these experiments to important problems in molecular and biomolecular science, with a view to demonstrating their utility in, and with the objective of establishing them as important new tools for, materials characterisation. To this end we have established collaborations with world leading laboratories in molecular and biomolecular materials science who will be the first users of the new methods.\\n\\nThe first experiment, 2D electronic spectroscopy(ES), is a unique tool for the study of electronic coupling and energy transport in (bio-)molecular assemblies. These processes are central to the collection and utilization of solar energy and in the operation of photoactivated nanomaterials. The experiment measures the correlation of the coherent excitation and emission frequencies in the visible region of the spectrum in a three pulse four wave mixing experiment. The measurement can be thought of as the optical analog of 2D NMR, in that it reveals couplings between electronic transitions that are obscured in the linear absorption spectrum. Such couplings are the underlying mechanism for energy and charge transport in both natural and artificial solar energy collectors, and thus need to be characterised and understood. In addition the same experiment resolves the temporal evolution of the energy flow in the molecular assembly with femtosecond resolution by varying the inter-pulse timings. An extension of this experiment to include polarization resolved data, will introduce a correlation between 2D spectra and molecular structure, and thus reveal the spatial arrangement of the chromophores. We will apply 2DES to elucidate excitation dynamics in multi-heme proteins and artificial porphyrin arrays, both of which figure prominently in solar energy conversion schemes and the latter can act as molecular wires in molecular electronics. The 2DES will provide the first direct measurement of the route and mechanism of energy transport in these molecular materials. How this correlates with structure will inform future designs strategies. In addition many heme proteins have unknown or disputed structures, so 2DES will provide new structural data. In short, 2DES has the power do for electronic structure what 2D NMR has done for nuclear structure.\\n\\nThe next two experiments report Raman and IR spectra of electronically excited molecules as a function of time after excitation. Excited state dynamics are a critical component of photoactivated molecular devices, where they act as transducer between optical and mechanical energy, by means of changes in shape or charge. Vibrational spectroscopy yields a detailed picture of the nuclear structure, and such measurements in real time allow us to track the structural changes which act as the driving force for motion in molecular machines. The time resolved coherent Raman experiment (FSRS) is well established. The transient IR measurement we will develop will permit IR detection in the visible region, using the same detection apparatus as Raman. This new method overcomes the limited spectral resolution of traditional IR detectors, and will permit the observation of subtle changes in bond lengths and angles which accompany structural change on a single electronic surface. These tools will be applied to investigate the mechanism of operation of molecular motors and molecular switches in a variety of environments.","4":"B1DABD3A-B20A-4ABD-9A45-2D0E185D695A"},{"1":"NE/N005724/1","2":"Understanding the ecological relevance of eDNA in freshwater lotic ecosystems","3":"Understanding the impacts of environmental change and changing land use on biodiversity and how ecosystems work require comprehensive knowledge of communities and their ecology. Molecular biodiversity identification is emerging as a high throughput and cost effective alternative to traditional approaches and in particular, the analysis of environmental DNA (eDNA) provides an opportunity to measure biodiversity in space and time at unprecedented scales. Unlike DNA obtained through direct analysis of communities, eDNA refers to shed cells or free-DNA from organisms as they pass through an environment, or die and decay. eDNA is being applied for various uses such as identification and monitoring of endangered/invasive species and analysis of biodiversity. It is very clear that researchers can detect eDNA from a variety of natural environments and in particular, freshwater environments. However, understanding how those sources of eDNA relate to living biodiversity and associated ecological function in ecologically and socio-economically important river ecosystems is at the heart of the eDNA:LOFRESH proposal.\\nFocusing on a range of exemplar experimental semi-natural and natural freshwater catchment systems from local to national scales, we will (a.) improve understanding of the movement, and persistence of lotic eDNA, (b.) quantify the relationship between lotic eDNA and the in situ community using different combinations of genetic and genomic approaches, (c.) improve methodological approaches for eDNA data acquisition and interpreting eDNA data using novel ecological and phylogenetic algorithms, (e.) develop and test new models relating lotic eDNA to stream biodiversity and ecosystem function and their variation in response to land use pressures. Over a 4 year period, five work packages (WPs) will be delivered by the Universities of Bangor, Birmingham, Cardiff and the Centre for Ecology and Hydrology. In WP1, we will use artificial stream channels in a series of experiments to assess the effects of a range of physical and chemical drivers on the loss of lotic eDNA and to compare and contrast genetic and genomic approaches for assessing known sources of lotic eDNA. In WP2, we will test our experimental findings from WP1 by tracking natural lentic (i.e. lake) and experimentally introduced control lotic eDNA through the natural stream network of the intensely studied Conwy River research catchment in north Wales. WP2 will also assess relationships between observed lotic eDNA and the in situ community in selected tributaries of the Conwy displaying a range of physicochemical characteristics and experiencing different land use pressures. WP3 will sample lotic eDNA in coordination with an on-going national survey in Wales to up-scale the experimental and catchment-scale findings of WP1 and WP2 to the Welsh landscape and national scales. WP4 will provide informatics support, but specifically, develop workflows to identify species level diversity in eDNA datasets. Finally, in WP5 we will further test our model findings, by manipulating the experimental stream systems with emulated land use pressures, quantify the ecosystem functions of decomposition and food web structure and test linkages with eDNA signals. Effective engagement with a broad range of stakeholder groups (government, end-users, environmental agencies) and project partners (research institutions and academic partners specialising in eDNA, sequencing and informatics) will optimise impact and research synergies of potentially transformative science throughout the consortium network.","4":"566AD65A-3655-4128-9214-2BF904F80D8E"},{"1":"NE/N006216/1","2":"Understanding the ecological relevance of eDNA in freshwater lotic ecosystems","3":"Understanding the impacts of environmental change and changing land use on biodiversity and how ecosystems work require comprehensive knowledge of communities and their ecology. Molecular biodiversity identification is emerging as a high throughput and cost effective alternative to traditional approaches and in particular, the analysis of environmental DNA (eDNA) provides an opportunity to measure biodiversity in space and time at unprecedented scales. Unlike DNA obtained through direct analysis of communities, eDNA refers to shed cells or free-DNA from organisms as they pass through an environment, or die and decay. eDNA is being applied for various uses such as identification and monitoring of endangered/invasive species and analysis of biodiversity. It is very clear that researchers can detect eDNA from a variety of natural environments and in particular, freshwater environments. However, understanding how those sources of eDNA relate to living biodiversity and associated ecological function in ecologically and socio-economically important river ecosystems is at the heart of the eDNA:LOFRESH proposal.\\nFocusing on a range of exemplar experimental semi-natural and natural freshwater catchment systems from local to national scales, we will (a.) improve understanding of the movement, and persistence of lotic eDNA, (b.) quantify the relationship between lotic eDNA and the in situ community using different combinations of genetic and genomic approaches, (c.) improve methodological approaches for eDNA data acquisition and interpreting eDNA data using novel ecological and phylogenetic algorithms, (e.) develop and test new models relating lotic eDNA to stream biodiversity and ecosystem function and their variation in response to land use pressures. Over a 4 year period, five work packages (WPs) will be delivered by the Universities of Bangor, Birmingham, Cardiff and the Centre for Ecology and Hydrology. In WP1, we will use artificial stream channels in a series of experiments to assess the effects of a range of physical and chemical drivers on the loss of lotic eDNA and to compare and contrast genetic and genomic approaches for assessing known sources of lotic eDNA. In WP2, we will test our experimental findings from WP1 by tracking natural lentic (i.e. lake) and experimentally introduced control lotic eDNA through the natural stream network of the intensely studied Conwy River research catchment in north Wales. WP2 will also assess relationships between observed lotic eDNA and the in situ community in selected tributaries of the Conwy displaying a range of physicochemical characteristics and experiencing different land use pressures. WP3 will sample lotic eDNA in coordination with an on-going national survey in Wales to up-scale the experimental and catchment-scale findings of WP1 and WP2 to the Welsh landscape and national scales. WP4 will provide informatics support, but specifically, develop workflows to identify species level diversity in eDNA datasets. Finally, in WP5 we will further test our model findings, by manipulating the experimental stream systems with emulated land use pressures, quantify the ecosystem functions of decomposition and food web structure and test linkages with eDNA signals. Effective engagement with a broad range of stakeholder groups (government, end-users, environmental agencies) and project partners (research institutions and academic partners specialising in eDNA, sequencing and informatics) will optimise impact and research synergies of potentially transformative science throughout the consortium network.","4":"08335833-5CF6-4CA9-8AB2-E1FAE5AB824C"},{"1":"NE/N005678/1","2":"Understanding the ecological relevance of eDNA in freshwater lotic ecosystems","3":"Understanding the impacts of environmental change and changing land use on biodiversity and how ecosystems work require comprehensive knowledge of communities and their ecology. Molecular biodiversity identification is emerging as a high throughput and cost effective alternative to traditional approaches and in particular, the analysis of environmental DNA (eDNA) provides an opportunity to measure biodiversity in space and time at unprecedented scales. Unlike DNA obtained through direct analysis of communities, eDNA refers to shed cells or free-DNA from organisms as they pass through an environment, or die and decay. eDNA is being applied for various uses such as identification and monitoring of endangered/invasive species and analysis of biodiversity. It is very clear that researchers can detect eDNA from a variety of natural environments and in particular, freshwater environments. However, understanding how those sources of eDNA relate to living biodiversity and associated ecological function in ecologically and socio-economically important river ecosystems is at the heart of the eDNA:LOFRESH proposal.\\nFocusing on a range of exemplar experimental semi-natural and natural freshwater catchment systems from local to national scales, we will (a.) improve understanding of the movement, and persistence of lotic eDNA, (b.) quantify the relationship between lotic eDNA and the in situ community using different combinations of genetic and genomic approaches, (c.) improve methodological approaches for eDNA data acquisition and interpreting eDNA data using novel ecological and phylogenetic algorithms, (e.) develop and test new models relating lotic eDNA to stream biodiversity and ecosystem function and their variation in response to land use pressures. Over a 4 year period, five work packages (WPs) will be delivered by the Universities of Bangor, Birmingham, Cardiff and the Centre for Ecology and Hydrology. In WP1, we will use artificial stream channels in a series of experiments to assess the effects of a range of physical and chemical drivers on the loss of lotic eDNA and to compare and contrast genetic and genomic approaches for assessing known sources of lotic eDNA. In WP2, we will test our experimental findings from WP1 by tracking natural lentic (i.e. lake) and experimentally introduced control lotic eDNA through the natural stream network of the intensely studied Conwy River research catchment in north Wales. WP2 will also assess relationships between observed lotic eDNA and the in situ community in selected tributaries of the Conwy displaying a range of physicochemical characteristics and experiencing different land use pressures. WP3 will sample lotic eDNA in coordination with an on-going national survey in Wales to up-scale the experimental and catchment-scale findings of WP1 and WP2 to the Welsh landscape and national scales. WP4 will provide informatics support, but specifically, develop workflows to identify species level diversity in eDNA datasets. Finally, in WP5 we will further test our model findings, by manipulating the experimental stream systems with emulated land use pressures, quantify the ecosystem functions of decomposition and food web structure and test linkages with eDNA signals. Effective engagement with a broad range of stakeholder groups (government, end-users, environmental agencies) and project partners (research institutions and academic partners specialising in eDNA, sequencing and informatics) will optimise impact and research synergies of potentially transformative science throughout the consortium network.","4":"B032A218-E99D-4047-961F-922700549599"},{"1":"131980","2":"Artificial Neural Network Prediction of Coefficient Thermal Expansion of Tooling Board","3":"We propose to develop an Artificial Neural Network (ANN) for the prediction of polymer composite material properties of a novel tooling board system. The ANN should enable us to identify target formulations and process conditions thereby minimising the amount of experimentation and development required to tailor our product to customer’s requirements. The work will be conducted on data sets of materials properties already held within Base Materials and the performance of the ANN will be demonstrated on blind data sets.","4":"7F9F06F2-924E-4D0A-BBA6-59252FA5B669"},{"1":"132002","2":"Development of a high reliability, laser weld attched planar fibre coupling technique for Silicon Photonic devices","3":"We propose to develop an Artificial Neural Network (ANN) for the prediction of polymer composite material properties of a novel tooling board system. The ANN should enable us to identify target formulations and process conditions thereby minimising the amount of experimentation and development required to tailor our product to customer’s requirements. The work will be conducted on data sets of materials properties already held within Base Materials and the performance of the ANN will be demonstrated on blind data sets.","4":"7E005597-8C86-45CC-8C0C-31A242E06423"},{"1":"10037935","2":"Biodiversity Digital Twin","3":"With our planet facing an increasing reduction in biodiversity, it is of the utmost importance to understand the way climate, humans, pollution and other factors affect biodiversity. This need is urgent since biodiversity loss directly impacts our ability as humans to live. For example, this loss reduces the efficiency with which ecological communities capture essential resources, produce biomass, decompose organic matter and recycle nutrients. Furthermore, there is increasing evidence that a lack of contact with natural biodiversity impacts human health through negative effects in the microbiome and immune system. microbiome and immune system. Understanding the forces shaping biodiversity is the basis for any rational management of natural resources and will be a key enabler of the EU Biodiversity Strategy 2030 which aims to put biodiversity in Europe on the path to recovery by 2030 for the benefit of people, climate and the planet. In particular, we need to be able to better predict global biodiversity dynamics: how species interact with their environment and with each other. Our goal is to push the current boundaries of predictive understanding of biodiversity dynamics by developing a Digital Twin providing advanced modelling, simulation and prediction capabilities. By exploiting in new ways existing technologies and data available across relevant research infrastructures, the BioDT project will be able to more accurately model interaction between species and their environment. Scientists at Research Infrastructures will be able to use the BioDT to 1) better observe changes in biodiversity, 2) relate these changes to possible causes, and 3) better predict effects of changes based on influences on these causes by either climate or human intervention. Our consortium brings together a dynamic team of experts in biodiversity, high performance computing, artificial intelligence and FAIR data to realise the first biodiversity Digital Twin prototype.","4":"9D269829-FFBA-44FD-B930-BFD48B812D71"},{"1":"10039292","2":"BioDT - Biodiversity Digital Twin for Advanced Modelling, Simulation and Prediction Capabilities","3":"With our planet facing an increasing reduction in biodiversity, it is of the utmost importance to understand the way climate, humans, pollution and other factors affect biodiversity. This need is urgent since biodiversity loss directly impacts our ability as humans to live. For example, this loss reduces the efficiency with which ecological communities capture essential resources, produce biomass, decompose organic matter and recycle nutrients. Furthermore, there is increasing evidence that a lack of contact with natural biodiversity impacts human health through negative effects in the microbiome and immune system. microbiome and immune system. Understanding the forces shaping biodiversity is the basis for any rational management of natural resources and will be a key enabler of the EU Biodiversity Strategy 2030 which aims to put biodiversity in Europe on the path to recovery by 2030 for the benefit of people, climate and the planet. In particular, we need to be able to better predict global biodiversity dynamics: how species interact with their environment and with each other. Our goal is to push the current boundaries of predictive understanding of biodiversity dynamics by developing a Digital Twin providing advanced modelling, simulation and prediction capabilities. By exploiting in new ways existing technologies and data available across relevant research infrastructures, the BioDT project will be able to more accurately model interaction between species and their environment. Scientists at Research Infrastructures will be able to use the BioDT to 1) better observe changes in biodiversity, 2) relate these changes to possible causes, and 3) better predict effects of changes based on influences on these causes by either climate or human intervention. Our consortium brings together a dynamic team of experts in biodiversity, high performance computing, artificial intelligence and FAIR data to realise the first biodiversity Digital Twin prototype.","4":"56281E22-8460-474B-AD5C-69E1BD9FF048"},{"1":"2420816","2":"Building robust methods for model explainability for healthcare","3":"With the recent rise in medical applications of artificial intelligence (AI), decision makers are now demanding more transparency from predictive models. Explainability (XAI) is a complex challenge that stems from various technical fields. Its main goal is to build explanation model that make model decisions readily interpretable. We distinguish two types of XAI approaches: (i) local model approximations where a simple model, g(x), is fitted to predict the black box\\nmodel, f(x), in a neighborhood around the prediction point x, and (ii) additive feature attribution methods where a model-free estimator describes how the model outcome at a point x changes when some of its features are removed and sampled from a reference distribution. To this day, there is no consensus in the field of XAI when it comes to finding the right method for a use case. Further, methods are not reliable due to their instability and lack of robustness. Ultimately, a limited number of XAI methods are based on a causal reasoning, even though clinicians often seek causal explanations. Our aim is to find methodological improvements to bridge the gap between statisticians and clinicians who want to routinely use transparent, fair AI tools for prediction. We make use of statistical learning theory, robust statistics and knowledge of healthcare applications to build new, innovative approaches. The focus of our first research endeavor was to tackle the issue of locality in local explanation models and build a more robust approach to Shapley values that can resist adversarial attacks. In parallel, we have been developing methods for understanding poly-victimization in multi-outcome causal models. Working on both model explainability and causal inference will hopefully enable us to design causal XAI methods. Long term, we hope to bridge the gap between the two subfields. \\nSuch technical advances in model explainability and causal inference can have high social impact, as they can improve the methodology for developing risk scores, and in particularly medical risk scores. Such measures are built according to feature attributions in predictive, non-causal models. Ultimately, we aim to study various alternatives to evaluating XAI methods. This task is challenging by definition, as there is no ground truth for evaluating these methods. Our aim is to borrow ideas and concepts from unsupervised learning and adapt them to the purpose of model explainability. \\nThis project falls within two EPSRC research areas: &quot;Artificial Intelligence and robotics&quot; and &quot;Healthcare technologies&quot;.","4":"BD7065E3-49DD-4A24-9BC5-5DD3C99D71BA"},{"1":"2635642","2":"Building robust methods for model explainability for healthcare","3":"With the recent rise in medical applications of artificial intelligence (AI), decision makers are now demanding more transparency from predictive models. Explainability (XAI) is a complex challenge that stems from various technical fields. Its main goal is to build explanation model that make model decisions readily interpretable. We distinguish two types of XAI approaches: (i) local model approximations where a simple model, g(x), is fitted to predict the black box\\nmodel, f(x), in a neighborhood around the prediction point x, and (ii) additive feature attribution methods where a model-free estimator describes how the model outcome at a point x changes when some of its features are removed and sampled from a reference distribution. To this day, there is no consensus in the field of XAI when it comes to finding the right method for a use case. Further, methods are not reliable due to their instability and lack of robustness. Ultimately, a limited number of XAI methods are based on a causal reasoning, even though clinicians often seek causal explanations. Our aim is to find methodological improvements to bridge the gap between statisticians and clinicians who want to routinely use transparent, fair AI tools for prediction. We make use of statistical learning theory, robust statistics and knowledge of healthcare applications to build new, innovative approaches. The focus of our first research endeavor was to tackle the issue of locality in local explanation models and build a more robust approach to Shapley values that can resist adversarial attacks. In parallel, we have been developing methods for understanding poly-victimization in multi-outcome causal models. Working on both model explainability and causal inference will hopefully enable us to design causal XAI methods. Long term, we hope to bridge the gap between the two subfields. \\nSuch technical advances in model explainability and causal inference can have high social impact, as they can improve the methodology for developing risk scores, and in particularly medical risk scores. Such measures are built according to feature attributions in predictive, non-causal models. Ultimately, we aim to study various alternatives to evaluating XAI methods. This task is challenging by definition, as there is no ground truth for evaluating these methods. Our aim is to borrow ideas and concepts from unsupervised learning and adapt them to the purpose of model explainability. \\nThis project falls within two EPSRC research areas: &quot;Artificial Intelligence and robotics&quot; and &quot;Healthcare technologies&quot;.","4":"CB453D96-0828-4959-8405-363C28DE44B7"},{"1":"10039818","2":"CONVOLVE: Seamless design of Smart Edge Processors","3":"With the rise of deep learning (DL), our world braces for Artificial Intelligence (AI) in every edge device, creating an urgent need for edge AI processing hardware. Unlike existing solutions, this hardware needs to support high throughput, reliable, and secure AI processing at ultra-low power (ULP), with a very short time to market. With its strong legacy in edge solutions and open processing platforms, the EU is ideally positioned to become the leader in this edge-AI market. However, certain roadblocks keep the EU from assuming this leadership role: Edge processors need to become 100x more energy efficient; Their complexity demands automated design with 10x design time reduction; They must be secure and reliable to get accepted; Finally, they should be flexible and powerful to support the DL domain. CONVOLVE addresses these roadblocks and thereby enables EU leadership in Edge-AI. To that end, it will take a holistic approach with innovations at all design stack levels, including: 1.ULP memristive circuits for computation-in-memory 2.Fast compositional design of System-on-Chips (SoC) 3.Transparent compilers supporting automated code optimizations and domain-specific languages 4.Rethinking DL models through dynamic neural networks, event-based execution, and sparsity 5.On-edge continuous learning for improved accuracy, self-healing, and reliable adaptation to non-stationary environments 6.Holistic integration in SoCs supporting secure execution with real time guarantees The CONVOLVE consortium includes some of Europe's strongest research groups and industries, covering the whole design stack and value chain. In a community effort, we will demonstrate Edge-AI computing in real-life vision and audio domains. By combining these innovative ULP and fast design solutions, CONVOLVE will, for the first time, enable reliable, smart, and energy efficient edge-AI devices at a rapid time-to-market and low cost, and as such open the road for EU leadership in edge-processing.","4":"01135605-3E00-4560-9AF4-1ABF817C342B"},{"1":"10039252","2":"Seamless Design Of Smart Edge Processors","3":"With the rise of deep learning (DL), our world braces for Artificial Intelligence (AI) in every edge device, creating an urgent need for edge AI processing hardware. Unlike existing solutions, this hardware needs to support high throughput, reliable, and secure AI processing at ultra-low power (ULP), with a very short time to market. With its strong legacy in edge solutions and open processing platforms, the EU is ideally positioned to become the leader in this edge-AI market. However, certain roadblocks keep the EU from assuming this leadership role: Edge processors need to become 100x more energy efficient; Their complexity demands automated design with 10x design time reduction; They must be secure and reliable to get accepted; Finally, they should be flexible and powerful to support the DL domain. CONVOLVE addresses these roadblocks and thereby enables EU leadership in Edge-AI. To that end, it will take a holistic approach with innovations at all design stack levels, including: 1.ULP memristive circuits for computation-in-memory 2.Fast compositional design of System-on-Chips (SoC) 3.Transparent compilers supporting automated code optimizations and domain-specific languages 4.Rethinking DL models through dynamic neural networks, event-based execution, and sparsity 5.On-edge continuous learning for improved accuracy, self-healing, and reliable adaptation to non-stationary environments 6.Holistic integration in SoCs supporting secure execution with real time guarantees The CONVOLVE consortium includes some of Europe's strongest research groups and industries, covering the whole design stack and value chain. In a community effort, we will demonstrate Edge-AI computing in real-life vision and audio domains. By combining these innovative ULP and fast design solutions, CONVOLVE will, for the first time, enable reliable, smart, and energy efficient edge-AI devices at a rapid time-to-market and low cost, and as such open the road for EU leadership in edge-processing.","4":"8CE7F8C9-7AE1-4C28-A9FF-429D4F361BA1"},{"1":"10039007","2":"Youth-GEMs_Gene Environment interactions in Mental health trajectories of Youth","3":"Youth mental health is heavily burdened, with life-long enduring impact on European citizens and societies. Trajectories of mental health and illness in young people are assumed to be determined by interplay between genetic, epigenetic, and environmental risk impacting during development. However, direct evidence for this is sparse and scientific progress is challenged. We recently initiated substantial advances enabling us to create necessary breakthroughs at the most pressing needs and challenges. Aiming to significantly reduce mentalsuffering and illness among European youth within the next 5-10 years, we will provide 1) the world?sfirst, evidence-based knowledge base of functional (epi)genomics of the developing post-natal human brain in direct relation to developmental trajectories of trans-syndromal phenotypes of mental illness, providing improved risk markers and actionable biological targets, 2) reliable predictive models, while identifying gene-environment interplay, as well as actionable markers of trajectories of mental (ill)health in young people through the use of Artificial Intelligence (AI)-based and inference-based analyses of unprecedented sets of longitudinal general population datasets, 3) the first comprehensive, validated set of evidence-based behavioural, environmental, biological, and psychological-informed instruments for the robust quantitative clinical assessment of mental health for help-seeking young people aged 12-24 years, harmonised across European clinical settings, and 4) youth- and clinician-empowering AI-driven instruments for early (self)detection, prediction and monitoring of mental ill-health trajectories in youth. Our multidisciplinary consortium is uniquely equipped and positioned to enforce the necessary breakthroughs for significant reduction of mental illness and suffering of young people, and to translate our findings into clinical innovation and life-long impact in Europe and beyond.","4":"3CE8D4F8-006F-4C38-8387-AA1C342C778B"},{"1":"10038026","2":"Youth-GEMs: Gene Environment interactions in Mental health trajectories of Youth","3":"Youth mental health is heavily burdened, with life-long enduring impact on European citizens and societies. Trajectories of mental health and illness in young people are assumed to be determined by interplay between genetic, epigenetic, and environmental risk impacting during development. However, direct evidence for this is sparse and scientific progress is challenged. We recently initiated substantial advances enabling us to create necessary breakthroughs at the most pressing needs and challenges. Aiming to significantly reduce mentalsuffering and illness among European youth within the next 5-10 years, we will provide 1) the world?sfirst, evidence-based knowledge base of functional (epi)genomics of the developing post-natal human brain in direct relation to developmental trajectories of trans-syndromal phenotypes of mental illness, providing improved risk markers and actionable biological targets, 2) reliable predictive models, while identifying gene-environment interplay, as well as actionable markers of trajectories of mental (ill)health in young people through the use of Artificial Intelligence (AI)-based and inference-based analyses of unprecedented sets of longitudinal general population datasets, 3) the first comprehensive, validated set of evidence-based behavioural, environmental, biological, and psychological-informed instruments for the robust quantitative clinical assessment of mental health for help-seeking young people aged 12-24 years, harmonised across European clinical settings, and 4) youth- and clinician-empowering AI-driven instruments for early (self)detection, prediction and monitoring of mental ill-health trajectories in youth. Our multidisciplinary consortium is uniquely equipped and positioned to enforce the necessary breakthroughs for significant reduction of mental illness and suffering of young people, and to translate our findings into clinical innovation and life-long impact in Europe and beyond.","4":"7F13A52D-7388-492B-AAF5-D60B7EA035DA"},{"1":"73602","2":"Artificial Intelligence for Digital Breast Tomosynthesis (AI4DBT)","3":"no public description","4":"43B3FF63-ECB5-4100-A82B-FFA673D45312"},{"1":"72333","2":"Separation of Additive Layer Supports by Automation &amp; Artificial Intelligence","3":"no public description","4":"B2A9866C-29EB-4ECA-AB22-160985764D56"},{"1":"73884","2":"Understanding structures, contents and conditions of real-estate using Artificial Intelligence","3":"no public description","4":"AEE58BD0-A60C-4950-91C7-58964A30D3BE"},{"1":"71944","2":"Enabling rapid adoption of artificial intelligence through an anonymized data protocol and explainable models","3":"no public description","4":"DC22C46D-590F-4062-8129-38537A359051"},{"1":"73648","2":"Secure AID-GI - Artificial Intelligence-supported Diagnostics of Gastrointestinal diseases with video capsule endoscopy","3":"no public description","4":"2A4E42E5-32E2-4E0D-B9BA-3B00B1611C74"},{"1":"74115","2":"Modelling and artificial intelligence using sensor data to personalise rehabilitation following joint replacement","3":"no public description","4":"AFD6B9BC-0A87-4A9A-96FE-7304D03DA662"},{"1":"72339","2":"Improving Legal Services Productivity and Accessibility with Artificial Intelligence - Covid 19 necessities","3":"no public description","4":"235E3FA7-0432-40DD-AA7A-8DD375225EA9"},{"1":"72761","2":"Continuity Grant for The Development of an Artificial Intelligence Recommender System for Advisory Service Provision at Scale","3":"no public description","4":"EBBFB0C6-E1EF-42B1-A23D-539AA981A945"},{"1":"72067","2":"COVID-19 Continuity grant for: Multiple physiological inputs to optimise real-time biofeedback through artificial intelligence to improve sleep in insomniacs","3":"no public description","4":"14A6E57E-928E-441E-8321-638B819A6A85"},{"1":"72524","2":"Covid-19 Continuity Grant for Innersight Labs Ltd — partner organisation of the London Medical Imaging &amp; Artificial Intelligence Centre for Value-Based Healthcare","3":"no public description","4":"C3C6EA3A-C112-4E4C-B9E4-6ED12C8220A2"},{"1":"74599","2":"ALEAD - Artificial Learning Environments for Autonomous Driving","3":"no public description","4":"1B2FC28E-C3F1-4E2C-A84F-72DB1FF537F5"},{"1":"74492","2":"A Commercial Intelligence Platform to Optimise Farm Productivity","3":"no public description","4":"801B9A72-47FD-4E21-952F-CB1382C6EAF6"},{"1":"73482","2":"Development and characterisation of a novel artificial genome editing tool","3":"no public description","4":"9668836C-2663-4C3E-9822-3374A5342C05"},{"1":"73155","2":"COVID continuity grant in support of DfMA + Business Intelligence Toolkit - applicatio 44090","3":"no public description","4":"1C5EB947-108C-4F88-9304-66AB1AC9699B"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p>Even with project dropped, most of these still have little descriptions</p>
</section>
</section>
<section id="word-count-and-keyword-summary" class="level2">
<h2 class="anchored" data-anchor-id="word-count-and-keyword-summary">Word Count and Keyword Summary</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Take the repeated test one out for now.</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>analysis_prj <span class="ot">=</span> unique_prj <span class="sc">|&gt;</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(repeated_text, <span class="at">by=</span><span class="st">"ProjectId"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">year =</span> lubridate<span class="sc">::</span><span class="fu">year</span>(StartDate))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="do">## function for tokenise target fiel</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>tokenize_words_group <span class="ot">=</span> <span class="cf">function</span>(df, col, group_col) {</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  df <span class="sc">|&gt;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># preserve chained keywords</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">text_field =</span> <span class="fu">str_replace</span>({{col}}, </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>                                    <span class="st">'(A|a)rtificial (I|i)ntelligence'</span>, </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                                    <span class="st">'artificialintelligence'</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>             <span class="fu">str_replace</span>(<span class="st">"machine learning"</span>,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>                         <span class="st">'machinelearning'</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>           ) <span class="sc">|&gt;</span> </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unnest_tokens</span>(word,text_field, <span class="at">token=</span><span class="st">"words"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">anti_join</span>(stop_words,<span class="st">"word"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">word=</span><span class="fu">str_replace</span>(word,<span class="st">'artificialintelligence'</span>,<span class="st">'artificial-intelligence'</span>) <span class="sc">|&gt;</span> </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>             <span class="fu">str_replace</span>(<span class="st">'machinelearning'</span>,<span class="st">'machin-learning'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>           ) <span class="sc">|&gt;</span> </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(word, {{group_col}}) <span class="sc">|&gt;</span> </span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(<span class="at">n_prj =</span> <span class="fu">length</span>(<span class="fu">unique</span>(ProjectId)), <span class="at">.groups=</span><span class="st">"drop"</span> ) <span class="sc">|&gt;</span> </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">arrange</span>(<span class="fu">desc</span>({{group_col}}),<span class="fu">desc</span>(n_prj)) <span class="sc">|&gt;</span> </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>rank_words <span class="ot">=</span> <span class="cf">function</span>(word_token, group_col) {</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  word_token <span class="sc">|&gt;</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>({{group_col}}) <span class="sc">|&gt;</span> </span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">arrange</span>(<span class="fu">desc</span>({{group_col}}),<span class="fu">desc</span>(n_prj)) <span class="sc">|&gt;</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">rank =</span> <span class="fu">row_number</span>()) <span class="sc">|&gt;</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>()</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>plot_top_n <span class="ot">=</span> <span class="cf">function</span>(word_token, n, group_col) {</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>  top_n <span class="ot">=</span> <span class="fu">rank_words</span>(word_token, {{group_col}}) <span class="sc">|&gt;</span> </span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(rank <span class="sc">&lt;=</span> n)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>  top_n <span class="sc">|&gt;</span> </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="fu">arrange</span>({{group_col}},rank) <span class="sc">|&gt;</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>n_prj, <span class="at">y =</span> <span class="fu">reorder_within</span>(word, n_prj, {{group_col}} ))) <span class="sc">+</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_col</span>(<span class="at">fill=</span><span class="st">"midnightblue"</span>) <span class="sc">+</span> </span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_reordered</span>() <span class="sc">+</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> <span class="fu">eval</span>(<span class="fu">enquo</span>(group_col)), <span class="at">scales=</span><span class="st">"free_y"</span>) <span class="sc">+</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">"number of porject"</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">"word"</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>title_word_by_year <span class="ot">=</span> analysis_prj <span class="sc">|&gt;</span> </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokenize_words_group</span>(title, year)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>abstrc_word_by_year <span class="ot">=</span> analysis_prj <span class="sc">|&gt;</span> </span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokenize_words_group</span>(abstractText, year)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>title_word_by_year <span class="sc">|&gt;</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">&gt;=</span> <span class="dv">2016</span>) <span class="sc">|&gt;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">year =</span> <span class="fu">as.character</span>(year)) <span class="sc">|&gt;</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_top_n</span>(<span class="dv">15</span>, year)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>abstrc_word_by_year <span class="sc">|&gt;</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">&gt;=</span> <span class="dv">2016</span>) <span class="sc">|&gt;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">year =</span> <span class="fu">as.character</span>(year)) <span class="sc">|&gt;</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_top_n</span>(<span class="dv">15</span>, year)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="do">## track progression of top n over years</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt_style_change <span class="ot">=</span> <span class="cf">function</span>(text_by_year, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">fav_words=</span><span class="fu">c</span>(<span class="st">"artificial-intelligence"</span>,<span class="st">"ai"</span>,<span class="st">"health"</span>, <span class="st">"machine-learning"</span>,<span class="st">"learning"</span>),</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">limit_rank =</span> <span class="dv">10</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">weight =</span> <span class="fu">c</span>(<span class="st">"pcg"</span>, <span class="st">"absolute"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                            ) {</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(weight[<span class="dv">1</span>] <span class="sc">==</span> <span class="st">"absolute"</span>) {</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    weightQ <span class="ot">=</span> <span class="fu">quo</span>(n_prj)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> <span class="cf">if</span> (weight[<span class="dv">1</span>] <span class="sc">==</span> <span class="st">"pcg"</span>) {</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    weightQ <span class="ot">=</span> <span class="fu">quo</span>(pcg_prj)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stop</span>()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  word_ranked <span class="ot">=</span> text_by_year <span class="sc">|&gt;</span> </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># filter(year &gt; 2010) |&gt; </span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rank_words</span>(year) <span class="sc">|&gt;</span> </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(year) <span class="sc">|&gt;</span> </span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">pcg_prj =</span> n_prj<span class="sc">/</span><span class="fu">sum</span>(n_prj)) <span class="sc">|&gt;</span> </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>()</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  top_n <span class="ot">=</span> word_ranked <span class="sc">|&gt;</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(rank <span class="sc">&lt;=</span> limit_rank) <span class="sc">|&gt;</span> </span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="do">## join back words that were crop out in top n ranking</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>n_prj, <span class="sc">-</span>rank,<span class="sc">-</span>pcg_prj)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>  top_n_complete <span class="ot">=</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    tidyr<span class="sc">::</span><span class="fu">expand</span>(top_n, year, word) <span class="sc">|&gt;</span> </span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">left_join</span>(word_ranked, <span class="fu">c</span>(<span class="st">"year"</span>,<span class="st">"word"</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">c</span>(n_prj, pcg_prj), <span class="sc">~</span><span class="fu">coalesce</span>(.x,<span class="dv">0</span>))) <span class="sc">|&gt;</span> </span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">rank =</span> <span class="fu">coalesce</span>(rank, <span class="fu">max</span>(word_ranked<span class="sc">$</span>rank <span class="sc">+</span> <span class="dv">1</span>)))</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>  top_n_complete <span class="sc">|&gt;</span> </span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(year <span class="sc">&gt;=</span> <span class="dv">2010</span> <span class="sc">&amp;</span> year <span class="sc">&lt;=</span> <span class="dv">2023</span>) <span class="sc">|&gt;</span> </span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>year,<span class="at">y=</span> <span class="sc">!!</span> weightQ, <span class="at">color=</span>word)) <span class="sc">+</span> </span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>, <span class="at">axis.text.y.right =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>)) <span class="sc">+</span> </span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># scale_y_reverse() +</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    <span class="fu">gghighlight</span>(word <span class="sc">%in%</span> fav_words,</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>                <span class="at">unhighlighted_params=</span><span class="fu">list</span>(<span class="at">alpha=</span><span class="fl">0.2</span>),</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>                <span class="at">line_label_type =</span> <span class="st">"text_path"</span><span class="co">#"sec_axis"</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>                ) <span class="sc">+</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_brewer</span>(<span class="at">palette=</span><span class="st">"Set2"</span>) <span class="sc">+</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>INTERESTING_WORDS <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"artificial-intelligence"</span>,<span class="st">"ai"</span>,<span class="st">"health"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>title_word_by_year <span class="sc">|&gt;</span> </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plt_style_change</span>(</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    INTERESTING_WORDS</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"propotion of project"</span>) <span class="sc">+</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Keywords mention in project title"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>          stringr<span class="sc">::</span><span class="fu">str_wrap</span>(glue<span class="sc">::</span><span class="fu">glue</span>(<span class="st">"The mention of 'health' seen a sharp raise in 2020. "</span>,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>          <span class="st">"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'"</span>),<span class="dv">80</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>          )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>INTERESTING_WORDS <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"artificial-intelligence"</span>,<span class="st">"ai"</span>,<span class="st">"health"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>title_word_by_year <span class="sc">|&gt;</span> </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plt_style_change</span>(</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    INTERESTING_WORDS,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="st">"pcg"</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"absolute number of project"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>INTERESTING_WORDS<span class="ot">=</span><span class="fu">c</span>(<span class="st">"ai"</span>,<span class="st">"artificial-intelligence"</span>,<span class="st">"covid"</span>,<span class="st">"health"</span>,<span class="st">"artificial"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>abstrc_word_by_year <span class="sc">|&gt;</span> </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plt_style_change</span>(</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">fav_words=</span>INTERESTING_WORDS,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">limit_rank =</span> <span class="dv">30</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>INTERESTING_WORDS<span class="ot">=</span><span class="fu">c</span>(<span class="st">"ai"</span>,<span class="st">"artificial-intelligence"</span>,<span class="st">"covid"</span>,<span class="st">"health"</span>,<span class="st">"artificial"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>abstrc_word_by_year <span class="sc">|&gt;</span> </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plt_style_change</span>(</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">fav_words=</span>INTERESTING_WORDS,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">limit_rank =</span> <span class="dv">30</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="st">"pcg"</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>