[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Invest in AI Executive Summary",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)"
  },
  {
    "objectID": "01-get-data.html",
    "href": "01-get-data.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\nnote: code in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n1571\nEPSRC\nEP/T025077/1\nUniversity of Oxford\nOxford Physics\nFellowship\nJohnston\nMichael\nNaN\nNaN\nNaN\n...\n30/09/2025\n1855112.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n40CF41BC-3D3E-4E6D-9C2A-AA170047FBE2\n798CB33D-C79E-4578-83F2-72606407192C\n47649064-FCD1-4D7A-A5AA-31B69B9BDFFC\n0BFC0B2F-BC4E-4642-BDE9-2526263313E4\n\n\n3932\nBBSRC\nBB/P006027/2\nNewcastle University\nBiosciences Institute\nResearch Grant\nKraskov\nAlexander\nNaN\nhttp://orcid.org/0000-0002-3576-4719\nNaN\n...\n25/08/2023\n225904.0\nNaN\nNorth East\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n97D59AAA-1CCB-40F1-BF18-7A59FC039AB8\n2512EF1C-401B-4222-9869-A770D4C5FAC7\nAA74BEFD-ACAF-45CC-A5C5-18B751C8D0C5\n6C8CB6A6-59C3-4E74-87E1-83FAD67662D7\n\n\n1920\nInnovate UK\n10036158\nAINOSTICS LIMITED\nNaN\nCollaborative R&D\nAzadbakht\nHojjat\nNaN\nNaN\nNaN\n...\n31/05/2025\n263862.0\nNaN\nNorth West\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n308E962B-DC1A-413A-A5CF-30650A8FF135\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\n9754E6F0-A8EE-4C46-B548-11974C4F487A\n2ED090B4-37B5-483E-ADE2-30EA57785745\n\n\n\n\n3 rows × 25 columns\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n5\nBE53F3D3-46A3-4B6A-B992-9691C47FAD36\nA Platform for Responsive Conversational Agent...\nAbstracts are not currently available in GtR f...\nNone\nNone\n\n\n15\n2B95787E-B980-4A55-BDDB-DA447244FA33\nBlockDox - Better Journeys Through Unique Pass...\nWith implications for customer satisfaction, r...\nNone\nNone\n\n\n6\n461942AE-520B-4697-BFC9-4A7BE25E0990\nPlaying or being played?\nPlay and playfulness are increasingly conspicu...\nNone\nNone\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n15\nE492C41A-2E33-4C2C-8AC5-77510531C9A0\nSpeechWave\nSpeech recognition has made major advances in ...\nNone\nRobust speech recognition is a key technology ...\n\n\n17\n8D30B3A9-C19E-4FCA-9A93-B45E90829A85\nResponsible AI for Long-term Trustworthy Auton...\nSociety is seeing enormous growth in the devel...\nNone\nNone\n\n\n22\n9B3CFA3E-A7FE-4A24-BCFA-A0F90DCED950\nStable Prediction of Defect-Inducing Software ...\nContext: software systems have become ever lar...\nNone\nSPDISC's beneficiaries are the software indust...\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n1256\nMRC\n2884156\nUniversity of Oxford\nRDM Radcliffe Department of Medicine\nStudentship\nNaN\nNaN\nNaN\nNaN\nAtkinson\n...\n30/09/2027\n0.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nD221C894-68B7-4EB6-AB4C-FF1FA42F49CA\nC008C651-F5B0-4859-A334-5F574AB6B57C\n47649064-FCD1-4D7A-A5AA-31B69B9BDFFC\nNaN\n\n\n2981\nSTFC\nST/X005623/1\nUniversity of Leicester\nPhysics and Astronomy\nResearch Grant\nWilkinson\nMark\nNaN\nNaN\nNaN\n...\n31/03/2026\n370409.0\nNaN\nEast Midlands\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n1E3118BF-D3F2-4838-A286-F3357D71701B\n0FA7FA45-02C9-433E-A750-12A20140208F\nF9431CE1-F5FE-4418-A853-61549E962D88\n63F1B3A6-3F04-4517-B4BD-5C3F7B4BF404\n\n\n1657\nEPSRC\nEP/Y010477/1\nSwansea University\nCollege of Science\nFellowship\nJones\nMatt\nNaN\nNaN\nNaN\n...\n31/03/2029\n1830000.0\nNaN\nWales\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nBAFF2831-061B-4BDC-BE32-8E6F88B58287\n798CB33D-C79E-4578-83F2-72606407192C\nC11E4053-874C-4D80-A724-5687C5732465\nD9085C89-4C0A-4A89-94B0-A93E27894CDD\n\n\n\n\n3 rows × 25 columns"
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "Trend and Elite Project",
    "section": "",
    "text": "setup and load data\nrequire(readr)\nrequire(arrow)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(tidyr)\nrequire(scales)\nrequire(plotly)\nrequire(ggtext)\n\nsource(\"set_graphic.R\")\ncsv_export=read_csv('data/projectsearch-1709481069771.csv')\napi_export=arrow::read_parquet('data/gtr.parquet')\n## clean parse date\ngtr = csv_export |&gt; \n  mutate(across(c(StartDate, EndDate), ~as.Date(.x, '%d/%m/%Y')))\n\n\n\nExplore Trend\n\nAbsolute NumberAward\n\n\n\n\nCode\ngtr |&gt; plot_freq(n()) + \n  ylab(\"Number of Project\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngtr |&gt; \n  plot_freq(sum(AwardPounds,na.rm=T)) + \n  ylab(\"Total Award Sumed\") +\n  labs(caption=\"This plot may be useful if funding were allocated all at the start of the year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Elite (Most Expensive) Projects\n\n\nCode\nrequire(htmlwidgets)\n\n\nLoading required package: htmlwidgets\n\n\nCode\n## parameter\nSNOBYNESS=0.02 ## this is top n most expensive\nPRICE_FORMULAR=quo(\n  AwardPounds/as.numeric(duration, units='days') ## this is how you calculate expensiveness\n) \n\n## calculate award per year \nprice_gtr = gtr |&gt; \n  mutate(duration = EndDate - StartDate) |&gt;\n  mutate(duration_d = as.numeric(duration, units='days')) |&gt; \n  mutate(price = !! PRICE_FORMULAR) |&gt; \n  arrange(desc(price)) |&gt; \n  mutate(expensive_rank = row_number()) |&gt;\n  filter(expensive_rank &lt;= quantile(expensive_rank, SNOBYNESS))\n\ng &lt;- price_gtr |&gt;\n  ggplot(aes(y = price, label = LeadROName, label1= Title, label2 = ProjectReference)) + \n  geom_linerange(aes(xmin = StartDate, xmax = EndDate, y = price, alpha = duration_d), \n                 color = \"darkgrey\") +\n  geom_point(aes(x = StartDate), color = \"blue\") + \n  geom_point(aes(x = EndDate), color = \"navy\") +\n  ggtitle(glue::glue(\"Here are the top {SNOBYNESS * 100}% most expensive AI projects\")) +\n  scale_alpha_continuous(trans='log')\n\nggplotly(g) |&gt;\n  style(traces = 2, text = paste(price_gtr$StartDate,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 1, text = paste( price_gtr$PIFirstName, price_gtr$PISurname,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 3, customdata = price_gtr$GTRProjectUrl, text = paste(price_gtr$EndDate,\"\\n\", price_gtr$LeadROName, \"\\n\",price_gtr$Title)) |&gt;\n  onRender(\"\n    function(el) { \n      el.on('plotly_click', function(d) { \n        var url = d.points[0].customdata;\n        window.open(url);\n      });\n    }\n  \") #set up click event that open URL",
    "crumbs": [
      "Home",
      "Journey",
      "Trend and Elite Project"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\nnote: code in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n955\nEPSRC\n2435696\nNewcastle University\nSch of Engineering\nStudentship\nNaN\nNaN\nNaN\nNaN\nBurke\n...\n22/01/2025\n0.0\nNaN\nNorth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nC404C021-7732-4EA5-97E6-72C9A1E25A14\n798CB33D-C79E-4578-83F2-72606407192C\nAA74BEFD-ACAF-45CC-A5C5-18B751C8D0C5\nNaN\n\n\n1643\nAHRC\nAH/R004706/1\nQueen Mary University of London\nSch of Electronic Eng & Computer Science\nResearch Grant\nSturm\nBob\nNaN\nNaN\nNaN\n...\n31/10/2018\n70990.0\nNaN\nLondon\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nD67A7BDE-CF8B-43CF-962E-88F16578FB4C\n1291772D-DFCE-493A-AEE7-24F7EEAFE0E9\n1060F625-A74C-4896-9E14-A578B83AA015\nB3EB1954-0014-4555-B722-FAAF590F4C5D\n\n\n2087\nEPSRC\nEP/X039277/1\nImperial College London\nElectrical and Electronic Engineering\nResearch Grant\nQin\nChen\nNaN\nNaN\nNaN\n...\n30/11/2026\n485939.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n88FF7A74-819F-4B84-9913-E52A413FF131\n798CB33D-C79E-4578-83F2-72606407192C\n4FC4CBFD-9E7C-4518-BB16-852553236FE1\nE1B07EB3-5543-492B-8714-F101FB52FE87\n\n\n\n\n3 rows × 25 columns\n\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n11\n7D1CB9EF-309F-4666-ACA2-24380D8CF057\nAdvanced Analysis of Building Energy Performan...\nThe UK government aims to achieve a 60% reduct...\nNone\nNone\n\n\n21\nFC29C0D9-1BA3-4FCB-A4DD-F933B27D3A74\nThe development of artificial imine reductases\nThe project aims at the development of artific...\nNone\nNone\n\n\n20\nE08C6928-DA50-4488-BB0B-915C6B7281C7\nInvestigating the regulation by O2 of the nif ...\nNif and fix genes are responsible for diazotro...\nNone\nNone\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n13\n61B0A12D-1758-4F6B-8EA1-AA67F982950F\neNeMILP: Non-Monotonic Incremental Language Pr...\nResearch in natural language processing (NLP) ...\nNone\n- Economy\\n\\nThe two applications we will focu...\n\n\n3\nD78940B1-0C34-427B-B78C-AD460694E595\nCyber security solutions for smart traffic con...\nWe aim to develop a solution framework that ca...\nNone\nKnowledge: As our proposed research is the fir...\n\n\n16\n82A32E0C-87A1-40BA-B74F-C7357EA7B959\nExaggeration, cohesion, and fragmentation in o...\nOn-line forums can support the formation of so...\nNone\nThe impact of the project will be realised in ...\n\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n4348\nEPSRC\nEP/X034542/1\nUniversity of Strathclyde\nElectronic and Electrical Engineering\nResearch Grant\nHaas\nHarald\nNaN\nNaN\nNaN\n...\n30/04/2026\n311087.0\nNaN\nScotland\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nFE1E1118-3ADC-4137-8CB9-8E1FD0FBED2C\n798CB33D-C79E-4578-83F2-72606407192C\n562804F3-F71E-459D-9769-23A632545BC0\nA71191C0-C00B-4FAE-9FDF-601FB176BA3F\n\n\n2399\nInnovate UK\n10057775\nPATTERN PROJECT LTD\nNaN\nGrant for R&D\nGrover\nShruti\nNaN\nNaN\nNaN\n...\n31/03/2024\n49953.0\nNaN\nSouth West\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nC4180695-6BF6-4031-817E-11F0DD91B1B7\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\n4493F1FA-912C-433D-B61A-94F60F1E4FB6\nD26234DF-A0F9-4C49-9335-2424F4DE76EF\n\n\n5054\nEPSRC\nEP/X030156/1\nUniversity of Surrey\nComputing Science\nResearch Grant\nTamaddoni-Nezhad\nAlireza\nNaN\nNaN\nNaN\n...\n14/06/2026\n887960.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n860AFBA2-6194-47F1-9897-3BD29893AD8E\n798CB33D-C79E-4578-83F2-72606407192C\nBE62EC9C-0D83-4697-802C-3B9892A3BDEB\n40682427-6EA9-4395-816A-6D1B0CB3A00F\n\n\n\n\n3 rows × 25 columns",
    "crumbs": [
      "Home",
      "Getting Data",
      "Getting Data"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#overview",
    "href": "03.html#overview",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#cleaning",
    "href": "03.html#cleaning",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Cleaning",
    "text": "Cleaning\noverview\n\nMajority of data and link can join\nDescription and Title contains duplication:\n\nThis is due to project migration when person of interests changes occupation;\nWhen this happens the ProjectReference will ends with a slash.\n\n\n\nHow well does the two data set join\n\n\nvalidation\nJOIN_META_KEY = c(\"id\"=\"ProjectId\")\n\ndesc_key = gtr_desc[[names(JOIN_META_KEY)]]\nmeta_key = gtr_meta[[JOIN_META_KEY[[1]]]]\n\nstopifnot(\n  !any(duplicated(desc_key)),\n  !any(duplicated(meta_key))\n)\ncan_join = intersect(desc_key, meta_key)\ndesc_cant_join = setdiff(desc_key, meta_key)\nmeta_cant_join= setdiff(meta_key, desc_key)\n\npcg = round(length(can_join) / nrow(gtr_desc) * 100)\n\nmessage(glue::glue(\n  \"{ pcg } % of description can find matching porject id in the csv export;\\n\",\n  \"\\nNumbers breakdown:\\n\",\n  \"\\t - {length(can_join)} can join;\\n\",\n  \"\\t - {length(desc_cant_join)} description will be taken out;\\n\",\n  \"\\t - {length(meta_cant_join)} from csv file will be taken out;\\n\"\n))\n\n\n86 % of description can find matching porject id in the csv export;\n\nNumbers breakdown:\n     - 4453 can join;\n     - 722 description will be taken out;\n     - 722 from csv file will be taken out;\n\n\n\n\nLinked/Duplicated project\n\n\npartial project or migrated project\n## migrated project consist of this pattern\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  relocate(ProjectReference, FundingOrgName, LeadROName, ProjectId, \n           is_partial,project_ref,part,occurance\n           )\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain null!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## a lot of these are false duplicate? or have they simply not been included in \n## the project? \ngtr_pj |&gt;\n  group_by(occurance) |&gt; \n  summarise(n_projects=n())\n\n\n\n  \n\n\n\npartial project or migrated project\n## actually majority of these project will false alert\n\n\n\n\nexamples of inheritance projects\nsmp=sample(1:94,1)# 9/3 + 183/2 \ngtr_pj |&gt; \n  filter(occurance !=1) |&gt; \n  group_by(project_ref) |&gt;\n  filter(cur_group_id() == smp) |&gt; \n  left_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\"))\n\n\n\n  \n\n\n\nAlthough the project will have different id. The content is actually the same. So it is important these are taken out before input for analysis.\nFor the analysis, we only need to take the first project which end with /1\nWe will need to know which rows to keep which rows to delete\n\n\nkeep only the first project\nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, ProjectId) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  inner_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\")) |&gt; \n  ungroup()\n\n\n\n\ncheck further duplication\nrepeated_text = unique_prj |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\nrepeated_text |&gt; \n  select(ProjectReference,title,abstractText, ProjectId)\n\n\n\n  \n\n\n\nEven with project dropped, most of these still have little descriptions",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#word-count-and-keyword-summary",
    "href": "03.html#word-count-and-keyword-summary",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Word Count and Keyword Summary",
    "text": "Word Count and Keyword Summary\n\n\nCode\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"ProjectId\") |&gt; \n  mutate(year = lubridate::year(StartDate))\n\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\nrank_words = function(word_token, group_col) {\n  word_token |&gt;\n    group_by({{group_col}}) |&gt; \n    dplyr::arrange(desc({{group_col}}),desc(n_prj)) |&gt;\n    dplyr::mutate(rank = row_number()) |&gt;\n    ungroup()\n}\n\nplot_top_n = function(word_token, n, group_col) {\n  top_n = rank_words(word_token, {{group_col}}) |&gt; \n    filter(rank &lt;= n)\n  top_n |&gt; \n    arrange({{group_col}},rank) |&gt;\n    ggplot(aes(x=n_prj, y = reorder_within(word, n_prj, {{group_col}} ))) +\n    geom_col(fill=\"midnightblue\") + \n    scale_y_reordered() +\n    facet_wrap(~ eval(enquo(group_col)), scales=\"free_y\") +\n    xlab(\"number of porject\") + ylab(\"word\")\n}\n\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n\nabstrc_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(abstractText, year)\n\n\n\n\nCode\ntitle_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nabstrc_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## track progression of top n over years\nplt_style_change = function(text_by_year, \n                            fav_words=c(\"artificial-intelligence\",\"ai\",\"health\", \"machine-learning\",\"learning\"),\n                            limit_rank = 10,\n                            weight = c(\"pcg\", \"absolute\")\n                            ) {\n  \n  if(weight[1] == \"absolute\") {\n    weightQ = quo(n_prj)\n  } else if (weight[1] == \"pcg\") {\n    weightQ = quo(pcg_prj)\n  } else {\n    stop()\n  }\n  \n  word_ranked = text_by_year |&gt; \n    # filter(year &gt; 2010) |&gt; \n    rank_words(year) |&gt; \n    group_by(year) |&gt; \n    mutate(pcg_prj = n_prj/sum(n_prj)) |&gt; \n    ungroup()\n  \n  top_n = word_ranked |&gt;\n    filter(rank &lt;= limit_rank) |&gt; \n    ## join back words that were crop out in top n ranking\n    select(-n_prj, -rank,-pcg_prj)\n  \n  top_n_complete =\n    tidyr::expand(top_n, year, word) |&gt; \n    left_join(word_ranked, c(\"year\",\"word\")) |&gt; \n    mutate(across(c(n_prj, pcg_prj), ~coalesce(.x,0))) |&gt; \n    mutate(rank = coalesce(rank, max(word_ranked$rank + 1)))\n    \n  top_n_complete |&gt; \n    filter(year &gt;= 2010 & year &lt;= 2023) |&gt; \n    ggplot(aes(x=year,y= !! weightQ, color=word)) + \n    geom_line() +\n    geom_point() +\n    theme(legend.position = \"none\", axis.text.y.right = element_text(size = 20)) + \n    # scale_y_reverse() +\n    gghighlight(word %in% fav_words,\n                unhighlighted_params=list(alpha=0.2),\n                line_label_type = \"text_path\"#\"sec_axis\"\n                ) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_minimal()\n}\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight = \"pcg\"\n  ) +\n  ylab(\"propotion of project\") +\n  ggtitle(\"Keywords mention in project title\",\n          stringr::str_wrap(glue::glue(\"The mention of 'health' seen a sharp raise in 2020. \",\n          \"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'\"),80)\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight=\"absolute\"\n  ) +\n  ylab(\"absolute number of project\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight = \"pcg\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight=\"absolute\"\n  )",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(igraph)\nlibrary(tidytext)\nset up and load data\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n# gtr_pj |&gt;\n#   group_by(occurance) |&gt; \n#   summarise(n_projects=n())\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")",
    "crumbs": [
      "Home",
      "Journey",
      "Text Analysis - Use N-Gram Graph"
    ]
  },
  {
    "objectID": "04.html#explore-n-gram",
    "href": "04.html#explore-n-gram",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "explore n-gram",
    "text": "explore n-gram\n\n\nCode\n## break into bi-grame\nabstract_words = analysis_prj |&gt;\n  unnest_tokens(word, abstractText, \"ngrams\",n=2, drop=T) |&gt; \n  count(word,id, sort=T)\n\n## try this `bind_tf_idf` function\nword_distinct = abstract_words |&gt; \n  bind_tf_idf(word,id, n)\n\n## convert bi-graph into network graph\npharases = word_distinct |&gt; \n  arrange(desc(tf_idf)) |&gt; \n  tidyr::separate(word, into=c(\"word1\",\"word2\"), sep=\" \") |&gt; \n  anti_join(stop_words,c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  filter(if_all(c(word1,word2), ~!stringr::str_detect(.x, \"\\\\d+\"))) |&gt; \n  mutate(pharase = paste(word1, word2)) |&gt; \n  filter(!word1 |&gt; str_detect(\"^_\"))\n\nrequire(tidygraph)\n\n## try find graphical centroid of biggest graph\ncatch_pharase = pharases |&gt; \n  group_by(word1, word2) |&gt; \n  summarise(\n    occurance = n()\n  ) |&gt; arrange(-occurance)\n\n\n`summarise()` has grouped output by 'word1'. You can override using the\n`.groups` argument.\n\n\nCode\ncatch_pharase\n\n\n\n  \n\n\n\nbi-graph truns out to be very useful…. you gets to understand\nEigen Centrality Run every node through to\n\n\nCode\nword_graph = catch_pharase |&gt; \n  as_tbl_graph() |&gt; \n  morph(to_components) |&gt; \n  as_tibble() |&gt; \n  mutate(dim = map_int(graph, ~length(.x))) |&gt; \n  arrange(desc(dim))\n\nbiggest_g=word_graph |&gt; purrr::pluck(\"graph\",1)\n\n## eigen centrality\n\ncentroid_score = biggest_g |&gt; eigen_centrality() |&gt; pluck(\"vector\")\ncentroid = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  convert(to_local_neighborhood, centroid, 1) |&gt;  \n  mutate(score = centroid_score[name]) |&gt;\n  convert(to_subgraph, score &gt; quantile(score, 0.99)) |&gt; \n  activate(edges) |&gt; \n  arrange(occurance) |&gt; \n  filter(occurance &gt; 10) |&gt; \n  ggraph(layout=\"gem\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\",trans=\"log\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  ggtitle(\"By Eigenvector Centrality\") +\n  theme_void()\n\n\nSubsetting by nodes\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), color =\n\"cyan4\", : Ignoring unknown parameters: `trans`\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nAnygraphical based algorithmn is interesting here.\n\n\nCode\n## harmonic_centrality\ncache_file = \"cache/04-betweeness_score.RDS\"\nif(interactive()) {\n  betweeness_score = biggest_g |&gt; \n  activate(edges) |&gt;\n  betweenness(weights=E(biggest_g)$\"occurance\")\n  saveRDS(centroid_score, cache_file)\n} else {\n  betweeness_score=readRDS(cache_file)\n}\nbtw_centre = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  # convert(to_local_neighborhood, btw_centre) |&gt; \n  mutate(score = betweeness_score[name]) |&gt; \n  convert(to_subgraph, score &gt; quantile(score, 0.95)) |&gt; \n  arrange(desc(score)) |&gt; \n  filter(row_number() &lt; 50) |&gt; \n  activate(edges) |&gt; \n  filter(occurance &gt; 100) |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  theme_void() +\n  ggtitle(\"By Edge Betweeness, visualise top 0.01 % \")\n\n\nSubsetting by nodes\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\n\n\n\n\n\n\n\nThis results actually makes sense if you are looking at quot is a wild card that can get about a lot of things.\nUp to interpretation.\nOkay other then key terms. Seems like the edge betweeness is good for point out adjusant words rather than identify patterns.",
    "crumbs": [
      "Home",
      "Journey",
      "Text Analysis - Use N-Gram Graph"
    ]
  },
  {
    "objectID": "04.html#suggestions",
    "href": "04.html#suggestions",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Suggestions",
    "text": "Suggestions\nIt maybe usefull to use n-gram to extract combined terms from title and bind this back into abstract n-graph. Then when we look at graph centrality again we remove any waild card.\nPharases may high high frequency of occurance but this does not means that they have a higher “graphic degree”.",
    "crumbs": [
      "Home",
      "Journey",
      "Text Analysis - Use N-Gram Graph"
    ]
  },
  {
    "objectID": "04.html#follow-up",
    "href": "04.html#follow-up",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Follow up",
    "text": "Follow up\nThe better way is actually filter a few top occuring terms.\n\n\nCode\ncatch_pharase |&gt; \n  filter(occurance &gt; 100) |&gt; \n  as_tbl_graph() |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),trans=\"log\",edge_colour =\"cyan4\") +\n  geom_node_text(aes(label=name),repel = T) + \n  geom_node_point(size=5) +\n  theme_void()\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), trans =\n\"log\", : Ignoring unknown parameters: `trans`",
    "crumbs": [
      "Home",
      "Journey",
      "Text Analysis - Use N-Gram Graph"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "Explore Topic Modeling",
    "section": "",
    "text": "Code\nlibrary(topicmodels)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#conclusion",
    "href": "05.html#conclusion",
    "title": "Explore Topic Modeling",
    "section": "Conclusion",
    "text": "Conclusion\nSo we may want to filter down to only research grant by itself. Before we do so let’s refine some visualization tools to help explore topics.\n\n\nCode\n#' Calculate bigram given document and field\n#' @param doc_df document number\n#' @param field column of text field\n#' @param doc_id id indicating text comes from in fact\ncompute_bi_gram = function(doc_df,field) {\n  .gvars = group_vars(doc_df)\n  if(length(.gvars)==0) {\n    Gvars=quo(NULL)\n  } else {\n    Gvars=quo({any_of(.gvars)})\n  }\n  doc_df |&gt; \n    # select({{field}},!!Gvars ) |&gt; \n    unnest_tokens(pharse,{{field}},'ngrams',n=2) |&gt; \n    separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n    ## clear up stopwords\n    anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n    anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n    filter(!is.na(word1) & !is.na(word2))\n}\n\n#' customer counting function that also\ncount_bi_gram = function(bi_gram,...) {\n  bi_gram |&gt; \n    group_by(word1,word2, .add=T) |&gt; \n    group_by(..., .add=T) |&gt; \n    summarise(n=n(),.groups=\"drop\") |&gt; \n    arrange(desc(n))\n}\n\n#' Graphical Plotting\nplot_beta=function(lda_model,ki,max_n=15) {\n  lda_model |&gt; \n    tidy(\"beta\") |&gt; \n    filter(topic == ki) |&gt; \n    group_by(topic) |&gt; \n    slice_max(beta,n=max_n) |&gt; \n    ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n    geom_col(fill=\"royalblue3\") + \n    scale_y_reordered() +\n    theme_minimal()\n}\n\nplot_word_graph=function(bi_gram_tokens,ki,top_n=15,model=NULL) {\n  if(!is.null(model)) {\n    beta=tidy(model,\"beta\") |&gt; \n      filter(topic == ki)\n    .add_beta = function(x) {\n      activate(x,nodes) |&gt; \n        left_join(beta, by=c(\"name\"=\"term\"))\n    }\n    .add_node_marker = function() {\n      geom_node_point(aes(size=beta))\n    }\n  } else {\n    .add_beta = \\(x) x\n    .add_node_marker = \\() geom_node_point(size=5)\n  }\n  typical_graph=bi_gram_tokens |&gt; \n    count_bi_gram(topic) |&gt; \n    relocate(word1,word2) |&gt; \n    as_tbl_graph()\n  ## filter bi-gram graph and plot\n  g = typical_graph |&gt; \n    .add_beta() |&gt; \n    activate(edges) |&gt;\n    filter(topic == ki) |&gt; \n    arrange(desc(n)) |&gt; \n    filter(row_number() &lt;= top_n) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  ## plot one single graph\n  g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),color= \"cyan4\") +\n    geom_node_text(aes(label=name),repel = T) +\n    .add_node_marker() +\n    theme_void() +\n    ggtitle(paste(\"topic\",ki))\n}\n\n\n\n\nCode\n## data ----------------------\n## bind topic result \ntopic_binded=topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntypical_docs = topic_binded |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,abstractText)\n\n## compute a graphic exploration\n## combine to bi-gram this is usually the expensive one\nbi_gram_tok = topic_binded |&gt;\n  compute_bi_gram(abstractText)\n\nplot_topics = function(model,reps_tk,reps_docs,topic_ki) {\n  g1 = reps_tk |&gt;\n    plot_word_graph(topic_ki,top_n = 20,model=topics) +\n    ggtitle(paste0(\"Topic\", topic_ki,\"\"))\n  g2 = model |&gt; \n    plot_beta(topic_ki,max_n = 20) +\n    ggtitle(\"Beta, words post representative of topics\")\n  foot_notes = reps_docs |&gt; \n    filter(topic == topic_ki) |&gt; \n    pull(abstractText) |&gt; \n    stringr::str_wrap(160) |&gt; \n    stringr::str_trunc(500)\n  g3 = ggplot() + theme_void() + geom_text(aes(x=0,y=0,label = foot_notes)) +\n    ggtitle(\"Example\")\n  ggpubr::ggarrange(g1,g2) |&gt; \n    ggpubr::ggarrange(g3,nrow=2, heights=c(8,5))\n}\n# topic_ki = 5\n## filter bi-gram to graph\n\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  },
  {
    "objectID": "index.html#intro",
    "href": "index.html#intro",
    "title": "Invest in AI Executive Summary",
    "section": "Intro",
    "text": "Intro\nWelcome!\n\n\n\nData Source?\n\n\nData usage - What’s used? What has been taken out?\n\n\n\n\n\nOkay, now what does the word look like\n\n\n\nTopic Modeling"
  },
  {
    "objectID": "index.html#other-findings",
    "href": "index.html#other-findings",
    "title": "Invest in AI Executive Summary",
    "section": "Other Findings",
    "text": "Other Findings\n\nWord Trend and Time Event\n\n\nResearcher Migration"
  },
  {
    "objectID": "05.html#thoughts-so-far",
    "href": "05.html#thoughts-so-far",
    "title": "Explore Topic Modeling",
    "section": "Thoughts so far…",
    "text": "Thoughts so far…\nFocus on research grant type of projects\nThe histogram shows us that So we may want to filter down to only research grant by itself. Before we do so let’s refine some visualization tools to help explore topics.\n\n\nCode\n#' Calculate bigram given document and field\n#' @param doc_df document number\n#' @param field column of text field\n#' @param doc_id id indicating text comes from in fact\ncompute_bi_gram = function(doc_df,field) {\n  .gvars = group_vars(doc_df)\n  if(length(.gvars)==0) {\n    Gvars=quo(NULL)\n  } else {\n    Gvars=quo({any_of(.gvars)})\n  }\n  doc_df |&gt; \n    # select({{field}},!!Gvars ) |&gt; \n    unnest_tokens(pharse,{{field}},'ngrams',n=2) |&gt; \n    separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n    ## clear up stopwords\n    anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n    anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n    filter(!is.na(word1) & !is.na(word2))\n}\n\n#' customer counting function that also\ncount_bi_gram = function(bi_gram,...) {\n  bi_gram |&gt; \n    group_by(word1,word2, .add=T) |&gt; \n    group_by(..., .add=T) |&gt; \n    summarise(n=n(),.groups=\"drop\") |&gt; \n    arrange(desc(n))\n}\n\n#' Graphical Plotting\nplot_beta=function(lda_model,ki,max_n=15) {\n  lda_model |&gt; \n    tidy(\"beta\") |&gt; \n    filter(topic == ki) |&gt; \n    group_by(topic) |&gt; \n    slice_max(beta,n=max_n) |&gt; \n    ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n    geom_col(fill=\"royalblue3\") + \n    scale_y_reordered() +\n    theme_minimal()\n}\n\nplot_word_graph=function(bi_gram_tokens,ki,top_n=15,model=NULL) {\n  if(!is.null(model)) {\n    beta=tidy(model,\"beta\") |&gt; \n      filter(topic == ki)\n    .add_beta = function(x) {\n      activate(x,nodes) |&gt; \n        left_join(beta, by=c(\"name\"=\"term\"))\n    }\n    .add_node_marker = function() {\n      geom_node_point(aes(size=beta))\n    }\n  } else {\n    .add_beta = \\(x) x\n    .add_node_marker = \\() geom_node_point(size=5)\n  }\n  typical_graph=bi_gram_tokens |&gt; \n    count_bi_gram(topic) |&gt; \n    relocate(word1,word2) |&gt; \n    as_tbl_graph()\n  ## filter bi-gram graph and plot\n  g = typical_graph |&gt; \n    .add_beta() |&gt; \n    activate(edges) |&gt;\n    filter(topic == ki) |&gt; \n    arrange(desc(n)) |&gt; \n    filter(row_number() &lt;= top_n) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  ## plot one single graph\n  g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),color= \"cyan4\") +\n    geom_node_text(aes(label=name),repel = T) +\n    .add_node_marker() +\n    theme_void() +\n    ggtitle(paste(\"topic\",ki))\n}\n\n\n\n\nCode\n## data ----------------------\n## bind topic result \ntopic_binded=topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntypical_docs = topic_binded |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,abstractText)\n\n## compute a graphic exploration\n## combine to bi-gram this is usually the expensive one\nbi_gram_tok = topic_binded |&gt;\n  compute_bi_gram(abstractText)\n\nplot_topics = function(model,reps_tk,reps_docs,topic_ki) {\n  g1 = reps_tk |&gt;\n    plot_word_graph(topic_ki,top_n = 20,model=topics) +\n    ggtitle(paste0(\"Topic\", topic_ki,\"\"))\n  g2 = model |&gt; \n    plot_beta(topic_ki,max_n = 20) +\n    ggtitle(\"Beta, words post representative of topics\")\n  foot_notes = reps_docs |&gt; \n    filter(topic == topic_ki) |&gt; \n    pull(abstractText) |&gt; \n    stringr::str_wrap(160) |&gt; \n    stringr::str_trunc(500)\n  g3 = ggplot() + theme_void() + geom_text(aes(x=0,y=0,label = foot_notes)) +\n    ggtitle(\"Example\")\n  ggpubr::ggarrange(g1,g2) |&gt; \n    ggpubr::ggarrange(g3,nrow=2, heights=c(8,5))\n}\n# topic_ki = 5\n## filter bi-gram to graph\n\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#appendix",
    "href": "05.html#appendix",
    "title": "Explore Topic Modeling",
    "section": "Appendix",
    "text": "Appendix\n\nDeveloping a Visualization for Topic Modeling**\nI found bi-gram graph compensate traditional beta count graph. Instead of “reading tea leafs”, you can try read along the edge.\n\n\ndraft script\nbi_gram = topic_binded |&gt; \n  select(id,abstractText,topic) |&gt; \n  unnest_tokens(pharse,abstractText,'ngrams',n=2) |&gt; \n  separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n  anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  count(topic,word1,word2,sort=T)\n\ntypical_graph = bi_gram |&gt; \n  filter(!is.na(word1) & !is.na(word2)) |&gt;\n  group_by(topic) |&gt;\n  slice_max(n,n=20) |&gt; \n  relocate(word1,word2) |&gt; \n  as_tbl_graph()\n\nG = list()\nfor (i in c(2,10,8,9)) {\n  g = typical_graph |&gt; \n    activate(edges) |&gt;\n    filter(topic == i) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  if(length(g)==0) next()\n  plot_g = g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),trans=\"log\",color=\"royalblue\") +\n    geom_node_text(aes(label=name),repel = T) +\n    geom_node_point(size=5) +\n    theme_void() +\n    ggtitle(paste(\"topic\",i))\n  G = append(G, list(plot_g))\n}\nggpubr::ggarrange(plotlist=G,ncol=2, nrow=2)\n\n\n\n\nList of all Topics From the Frist Extraction\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)\n\n\n\n\n\n\n\n\n\n\n\nList of All Topic Extraction for Research Grant Only\nThe topic read\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=res_lda,\n                        reps_tk=res_bi_gram, \n                        reps_docs=res_reps_top,\n                        topic_ki=.x,\n                        color= \"coral2\"\n                        ))\nggpubr::ggarrange(plotlist=G,nrow=10)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  }
]