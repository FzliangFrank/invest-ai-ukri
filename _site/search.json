[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Invest in AI Executive Summary",
    "section": "",
    "text": "Code\nprint(\"Hello Panel!\")\n\n\n[1] \"Hello Panel!\""
  },
  {
    "objectID": "01-get-data.html",
    "href": "01-get-data.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\nnote: code in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n1571\nEPSRC\nEP/T025077/1\nUniversity of Oxford\nOxford Physics\nFellowship\nJohnston\nMichael\nNaN\nNaN\nNaN\n...\n30/09/2025\n1855112.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n40CF41BC-3D3E-4E6D-9C2A-AA170047FBE2\n798CB33D-C79E-4578-83F2-72606407192C\n47649064-FCD1-4D7A-A5AA-31B69B9BDFFC\n0BFC0B2F-BC4E-4642-BDE9-2526263313E4\n\n\n3932\nBBSRC\nBB/P006027/2\nNewcastle University\nBiosciences Institute\nResearch Grant\nKraskov\nAlexander\nNaN\nhttp://orcid.org/0000-0002-3576-4719\nNaN\n...\n25/08/2023\n225904.0\nNaN\nNorth East\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n97D59AAA-1CCB-40F1-BF18-7A59FC039AB8\n2512EF1C-401B-4222-9869-A770D4C5FAC7\nAA74BEFD-ACAF-45CC-A5C5-18B751C8D0C5\n6C8CB6A6-59C3-4E74-87E1-83FAD67662D7\n\n\n1920\nInnovate UK\n10036158\nAINOSTICS LIMITED\nNaN\nCollaborative R&D\nAzadbakht\nHojjat\nNaN\nNaN\nNaN\n...\n31/05/2025\n263862.0\nNaN\nNorth West\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n308E962B-DC1A-413A-A5CF-30650A8FF135\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\n9754E6F0-A8EE-4C46-B548-11974C4F487A\n2ED090B4-37B5-483E-ADE2-30EA57785745\n\n\n\n\n3 rows × 25 columns\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n5\nBE53F3D3-46A3-4B6A-B992-9691C47FAD36\nA Platform for Responsive Conversational Agent...\nAbstracts are not currently available in GtR f...\nNone\nNone\n\n\n15\n2B95787E-B980-4A55-BDDB-DA447244FA33\nBlockDox - Better Journeys Through Unique Pass...\nWith implications for customer satisfaction, r...\nNone\nNone\n\n\n6\n461942AE-520B-4697-BFC9-4A7BE25E0990\nPlaying or being played?\nPlay and playfulness are increasingly conspicu...\nNone\nNone\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n15\nE492C41A-2E33-4C2C-8AC5-77510531C9A0\nSpeechWave\nSpeech recognition has made major advances in ...\nNone\nRobust speech recognition is a key technology ...\n\n\n17\n8D30B3A9-C19E-4FCA-9A93-B45E90829A85\nResponsible AI for Long-term Trustworthy Auton...\nSociety is seeing enormous growth in the devel...\nNone\nNone\n\n\n22\n9B3CFA3E-A7FE-4A24-BCFA-A0F90DCED950\nStable Prediction of Defect-Inducing Software ...\nContext: software systems have become ever lar...\nNone\nSPDISC's beneficiaries are the software indust...\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n1256\nMRC\n2884156\nUniversity of Oxford\nRDM Radcliffe Department of Medicine\nStudentship\nNaN\nNaN\nNaN\nNaN\nAtkinson\n...\n30/09/2027\n0.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nD221C894-68B7-4EB6-AB4C-FF1FA42F49CA\nC008C651-F5B0-4859-A334-5F574AB6B57C\n47649064-FCD1-4D7A-A5AA-31B69B9BDFFC\nNaN\n\n\n2981\nSTFC\nST/X005623/1\nUniversity of Leicester\nPhysics and Astronomy\nResearch Grant\nWilkinson\nMark\nNaN\nNaN\nNaN\n...\n31/03/2026\n370409.0\nNaN\nEast Midlands\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n1E3118BF-D3F2-4838-A286-F3357D71701B\n0FA7FA45-02C9-433E-A750-12A20140208F\nF9431CE1-F5FE-4418-A853-61549E962D88\n63F1B3A6-3F04-4517-B4BD-5C3F7B4BF404\n\n\n1657\nEPSRC\nEP/Y010477/1\nSwansea University\nCollege of Science\nFellowship\nJones\nMatt\nNaN\nNaN\nNaN\n...\n31/03/2029\n1830000.0\nNaN\nWales\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nBAFF2831-061B-4BDC-BE32-8E6F88B58287\n798CB33D-C79E-4578-83F2-72606407192C\nC11E4053-874C-4D80-A724-5687C5732465\nD9085C89-4C0A-4A89-94B0-A93E27894CDD\n\n\n\n\n3 rows × 25 columns"
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "Trend and Elite Project",
    "section": "",
    "text": "setup and load data\nrequire(readr)\nrequire(arrow)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(tidyr)\nrequire(scales)\nrequire(plotly)\nrequire(ggtext)\n\nsource(\"set_graphic.R\")\ncsv_export=read_csv('data/projectsearch-1709481069771.csv')\napi_export=arrow::read_parquet('data/gtr.parquet')\n## clean parse date\ngtr = csv_export |&gt; \n  mutate(across(c(StartDate, EndDate), ~as.Date(.x, '%d/%m/%Y')))\n\n\n\nExplore Trend\n\nAbsolute NumberAward\n\n\n\n\nCode\ngtr |&gt; plot_freq(n()) + \n  ylab(\"Number of Project\")\n\n\n\n\n\n\n\n\n\nCode\ngtr |&gt; \n  plot_freq(sum(AwardPounds,na.rm=T)) + \n  ylab(\"Total Award Sumed\") +\n  labs(caption=\"This plot may be useful if funding were allocated all at the start of the year\")\n\n\n\n\n\n\n\n\n\n\nExplore Elite (Most Expensive) Projects\n\n\nCode\nrequire(htmlwidgets)\n\n\nLoading required package: htmlwidgets\n\n\nCode\n## parameter\nSNOBYNESS=0.02 ## this is top n most expensive\nPRICE_FORMULAR=quo(\n  AwardPounds/as.numeric(duration, units='days') ## this is how you calculate expensiveness\n) \n\n## calculate award per year \nprice_gtr = gtr |&gt; \n  mutate(duration = EndDate - StartDate) |&gt;\n  mutate(duration_d = as.numeric(duration, units='days')) |&gt; \n  mutate(price = !! PRICE_FORMULAR) |&gt; \n  arrange(desc(price)) |&gt; \n  mutate(expensive_rank = row_number()) |&gt;\n  filter(expensive_rank &lt;= quantile(expensive_rank, SNOBYNESS))\n\ng &lt;- price_gtr |&gt;\n  ggplot(aes(y = price, label = LeadROName, label1= Title, label2 = ProjectReference)) + \n  geom_linerange(aes(xmin = StartDate, xmax = EndDate, y = price, alpha = duration_d), \n                 color = \"darkgrey\") +\n  geom_point(aes(x = StartDate), color = \"blue\") + \n  geom_point(aes(x = EndDate), color = \"navy\") +\n  ggtitle(glue::glue(\"Here are the top {SNOBYNESS * 100}% most expensive AI projects\")) +\n  scale_alpha_continuous(trans='log')\n\nggplotly(g) |&gt;\n  style(traces = 2, text = paste(price_gtr$StartDate,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 1, text = paste( price_gtr$PIFirstName, price_gtr$PISurname,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 3, customdata = price_gtr$GTRProjectUrl, text = paste(price_gtr$EndDate,\"\\n\", price_gtr$LeadROName, \"\\n\",price_gtr$Title)) |&gt;\n  onRender(\"\n    function(el) { \n      el.on('plotly_click', function(d) { \n        var url = d.points[0].customdata;\n        window.open(url);\n      });\n    }\n  \") #set up click event that open URL"
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\nnote: code in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n362\nInnovate UK\n104751\nURBANTIDE LIMITED\nNaN\nFeasibility Studies\nRevill\nSteven\nNaN\nNaN\nNaN\n...\n30/09/2019\n72414.0\nNaN\nLondon\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n398A1E16-BD48-426F-ABF2-7588ABBCDB7C\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\nE391A44E-FD59-435D-8BA1-AE0AD3D6B75E\n4537E93F-C687-4707-8932-624104CF50FE\n\n\n768\nEPSRC\nEP/M026884/1\nUniversity College London\nSurgery\nResearch Grant\nSong\nWenhui\nNaN\nNaN\nNaN\n...\n31/05/2019\n870392.0\nNaN\nLondon\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nEDE57B67-2E92-4F2A-855A-49C76AA76675\n798CB33D-C79E-4578-83F2-72606407192C\nE7BE255A-5E4B-4E40-9A39-F1458486B92F\n46357C3B-37B6-4A91-84DD-D21F2D8486D3\n\n\n257\nHorizon Europe Guarantee\nEP/X036782/1\nUniversity College London\nStructural Molecular Biology\nResearch Grant\nHansen\nFlemming\nNaN\nNaN\nNaN\n...\n31/03/2028\n2153217.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nB6ACF925-2CC7-497A-8C06-7E5F0849F175\n240CEBFD-1052-4EAC-88DF-D88A163D61C8\nE7BE255A-5E4B-4E40-9A39-F1458486B92F\n63C902E1-D1AF-4169-84B1-40AB974B7427\n\n\n\n\n3 rows × 25 columns\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n21\nCE19CB43-9168-4ED5-9ED3-F4291578B1D4\nMachine Learning, Robust Optimisation, and Ver...\nThe need for better support to deal with the t...\nNone\nEach project site will host a workshop. These ...\n\n\n10\nCBA02AE8-BB0C-4B6F-8D3E-932BE96957B2\nLearned Exascale Computational Imaging (LEXCI)\nThe emerging era of exascale computing that wi...\nNone\nNone\n\n\n15\n3082E97C-AC07-469F-ADD6-A2A44FF5D210\nDeveloping the Caf1 polymer technology into a ...\nPharmaceuticals have developed via three manuf...\nNone\nNone\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n0\n58482BBD-02C6-4DA4-B422-D59103E84113\nAlgebraic Invariants for Phylogenetic Network ...\nThe key goal in phylogenetics is to be able to...\nNone\nNone\n\n\n4\n1CFE4F38-669E-49BD-8BC2-0A5275FC855D\nREMORA - REndezvous Mission for Orbital Recons...\nAs of January 2021, our solar system is popula...\nNone\nNone\n\n\n6\n6B756B0A-C146-4976-8C97-47FC6F550882\nA Neuromorphic Control System for Agile Biped ...\nRush-hour in a London mainline railway station...\nNone\nOur project could have academic and societal i...\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n2539\nAHRC\nAH/X011585/1\nDe Montfort University\nLMS - Leicester Media School\nResearch Grant\nXambó Sedó\nAnna\nNaN\nhttp://orcid.org/0000-0003-2333-6941\nNaN\n...\n31/08/2025\n207479.0\nNaN\nEast Midlands\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n623A9EA5-4C55-4A72-9CCA-8BFF68EDB430\n1291772D-DFCE-493A-AEE7-24F7EEAFE0E9\n75DEDCF0-99C0-4867-B29A-7737A4F77A1F\n6CD3D5E8-FD85-4B63-95F6-6DA0AF91D7EF\n\n\n1584\nHorizon Europe Guarantee\n10103529\nUNIVERSITY OF LEEDS\nNaN\nEU-Funded\nDunning\nKeri\nNaN\nNaN\nNaN\n...\n31/12/2026\n383564.0\nNaN\nYorkshire and The Humber\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n32037942-2A41-447D-9539-DA9F63A1054F\n240CEBFD-1052-4EAC-88DF-D88A163D61C8\n38CF3AB0-B06C-4A19-ACD0-386A7C74160F\n1234746C-0B5D-4CD0-AAFE-543056B28DAF\n\n\n1963\nInnovate UK\n10073837\nBRAITHWATE LTD\nNaN\nCollaborative R&D\nVajala\nArjun\nNaN\nNaN\nNaN\n...\n31/10/2025\n330922.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n156E495C-9FE1-4243-A4AA-C94208E0129A\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\nF6949C7D-1BE6-462E-84E7-A9B331A25110\n9EDEEB84-86FE-4705-A1F0-0C3DF4FD3A0B\n\n\n\n\n3 rows × 25 columns"
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "Topic Modeling - Part 1",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Analysis",
      "Topic Modeling - Part 1"
    ]
  },
  {
    "objectID": "03.html#overview",
    "href": "03.html#overview",
    "title": "Topic Modeling - Part 1",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Analysis",
      "Topic Modeling - Part 1"
    ]
  },
  {
    "objectID": "03.html#cleaning",
    "href": "03.html#cleaning",
    "title": "Topic Modeling - Part 1",
    "section": "Cleaning",
    "text": "Cleaning\noverview\n\nMajority of data and link can join\nDescription and Title contains duplication:\n\nThis is due to project migration when person of interests changes occupation;\nWhen this happens the ProjectReference will ends with a slash.\n\n\n\nHow well does the two data set join\n\n\nCode\nJOIN_META_KEY = c(\"id\"=\"ProjectId\")\n\ndesc_key = gtr_desc[[names(JOIN_META_KEY)]]\nmeta_key = gtr_meta[[JOIN_META_KEY[[1]]]]\n\nstopifnot(\n  !any(duplicated(desc_key)),\n  !any(duplicated(meta_key))\n)\ncan_join = intersect(desc_key, meta_key)\ndesc_cant_join = setdiff(desc_key, meta_key)\nmeta_cant_join= setdiff(meta_key, desc_key)\n\npcg = round(length(can_join) / nrow(gtr_desc) * 100)\n\nmessage(glue::glue(\n  \"{ pcg } % of description can find matching porject id in the csv export;\\n\",\n  \"\\nNumbers breakdown:\\n\",\n  \"\\t - {length(can_join)} can join;\\n\",\n  \"\\t - {length(desc_cant_join)} description will be taken out;\\n\",\n  \"\\t - {length(meta_cant_join)} from csv file will be taken out;\\n\"\n))\n\n\n86 % of description can find matching porject id in the csv export;\n\nNumbers breakdown:\n     - 4453 can join;\n     - 722 description will be taken out;\n     - 722 from csv file will be taken out;\n\n\n\n\nLinked/Duplicated project\n\n\npartial project or migrated project\n## migrated project consist of this pattern\nPARTIAL_PTN_DCT='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTN_DCT),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTN_DCT,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTN_DCT) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  relocate(ProjectReference, FundingOrgName, LeadROName, ProjectId, \n           is_partial,project_ref,part,occurance\n           )\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain null!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## a lot of these are false duplicate? or have they simply not been included in \n## the project? \ngtr_pj |&gt;\n  group_by(occurance) |&gt; \n  summarise(n_projects=n())\n\n\n\n  \n\n\n\npartial project or migrated project\n## actually majority of these project will false alert\n\n\n\n\nexamples of inheritance projects\nsmp=sample(1:94,1)# 9/3 + 183/2 \ngtr_pj |&gt; \n  filter(occurance !=1) |&gt; \n  group_by(project_ref) |&gt;\n  filter(cur_group_id() == smp) |&gt; \n  left_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\"))\n\n\n\n  \n\n\n\nAlthough the project will have different id. The content is actually the same. So it is important these are taken out before input for analysis.\nFor the analysis, we only need to take the first project which end with /1\nWe will need to know which rows to keep which rows to delete\n\n\nkeep only the first project\nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, ProjectId) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  inner_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\")) |&gt; \n  ungroup()\n\n\n\n\ncheck further duplication\nrepeated_text = unique_prj |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\nrepeated_text |&gt; \n  select(ProjectReference,title,abstractText, ProjectId)\n\n\n\n  \n\n\n\nEven with project dropped, most of these still have little descriptions",
    "crumbs": [
      "Home",
      "Analysis",
      "Topic Modeling - Part 1"
    ]
  },
  {
    "objectID": "03.html#word-count-and-keyword-summary",
    "href": "03.html#word-count-and-keyword-summary",
    "title": "Topic Modeling - Part 1",
    "section": "Word Count and Keyword Summary",
    "text": "Word Count and Keyword Summary\n\n\nCode\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"ProjectId\") |&gt; \n  mutate(year = lubridate::year(StartDate))\n\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\nrank_words = function(word_token, group_col) {\n  word_token |&gt;\n    group_by({{group_col}}) |&gt; \n    dplyr::arrange(desc({{group_col}}),desc(n_prj)) |&gt;\n    dplyr::mutate(rank = row_number()) |&gt;\n    ungroup()\n}\n\nplot_top_n = function(word_token, n, group_col) {\n  top_n = rank_words(word_token, {{group_col}}) |&gt; \n    filter(rank &lt;= n)\n  top_n |&gt; \n    arrange({{group_col}},rank) |&gt;\n    ggplot(aes(x=n_prj, y = reorder_within(word, n_prj, {{group_col}} ))) +\n    geom_col(fill=\"midnightblue\") + \n    scale_y_reordered() +\n    facet_wrap(~ eval(enquo(group_col)), scales=\"free_y\") +\n    xlab(\"number of porject\") + ylab(\"word\")\n}\n\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n\nabstrc_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(abstractText, year)\n\n\n\n\nCode\ntitle_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year)\n\n\n\n\n\n\n\n\n\n\n\nCode\nabstrc_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year)\n\n\n\n\n\n\n\n\n\n\n\nCode\n## track progression of top n over years\nplt_style_change = function(text_by_year, \n                            fav_words=c(\"artificial-intelligence\",\"ai\",\"health\", \"machine-learning\",\"learning\"),\n                            limit_rank = 10,\n                            weight = c(\"pcg\", \"absolute\")\n                            ) {\n  \n  if(weight[1] == \"absolute\") {\n    weightQ = quo(n_prj)\n  } else if (weight[1] == \"pcg\") {\n    weightQ = quo(pcg_prj)\n  } else {\n    stop()\n  }\n  \n  word_ranked = text_by_year |&gt; \n    # filter(year &gt; 2010) |&gt; \n    rank_words(year) |&gt; \n    group_by(year) |&gt; \n    mutate(pcg_prj = n_prj/sum(n_prj)) |&gt; \n    ungroup()\n  \n  top_n = word_ranked |&gt;\n    filter(rank &lt;= limit_rank) |&gt; \n    ## join back words that were crop out in top n ranking\n    select(-n_prj, -rank,-pcg_prj)\n  \n  top_n_complete =\n    tidyr::expand(top_n, year, word) |&gt; \n    left_join(word_ranked, c(\"year\",\"word\")) |&gt; \n    mutate(across(c(n_prj, pcg_prj), ~coalesce(.x,0))) |&gt; \n    mutate(rank = coalesce(rank, max(word_ranked$rank + 1)))\n    \n  top_n_complete |&gt; \n    filter(year &gt;= 2010 & year &lt;= 2023) |&gt; \n    ggplot(aes(x=year,y= !! weightQ, color=word)) + \n    geom_line() +\n    geom_point() +\n    theme(legend.position = \"none\", axis.text.y.right = element_text(size = 20)) + \n    # scale_y_reverse() +\n    gghighlight(word %in% fav_words,\n                unhighlighted_params=list(alpha=0.2),\n                line_label_type = \"text_path\"#\"sec_axis\"\n                ) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_minimal()\n}\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS\n  ) +\n  ylab(\"propotion of project\") +\n  ggtitle(\"Keywords mention in project title\",\n          stringr::str_wrap(glue::glue(\"The mention of 'health' seen a sharp raise in 2020. \",\n          \"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'\"),80)\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight=\"pcg\"\n  ) +\n  ylab(\"absolute number of project\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight=\"pcg\"\n  )",
    "crumbs": [
      "Home",
      "Analysis",
      "Topic Modeling - Part 1"
    ]
  }
]