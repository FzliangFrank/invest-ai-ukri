[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Welcome",
    "section": "Welcome!",
    "text": "Welcome!\nAbout\nThis project is dedicated for UK Research & Innovation in application of the role “Senior Analyst”.\nI come across UKRI data during my master degree. My “business project” use Gateway to Research to try extract insight about research funding about “Artificial Intelligence” for University of Exeter. This sample work is my rework at this project.\nMain Methodology\nThis works use “Topic Modeling” (LDA) to extract interesting “funding abstract” related to “Artifical Intelligence”.\nProgramming Language\nThe API call using Python (this is the only code section use that). All other analysis use R.\nThe web page is a static html written in quarto.",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(igraph)\nlibrary(tidytext)\nset up and load data\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n# gtr_pj |&gt;\n#   group_by(occurance) |&gt; \n#   summarise(n_projects=n())\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")"
  },
  {
    "objectID": "04.html#explore-n-gram",
    "href": "04.html#explore-n-gram",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "explore n-gram",
    "text": "explore n-gram\n\n\nCode\n## break into bi-grame\nabstract_words = analysis_prj |&gt;\n  unnest_tokens(word, abstractText, \"ngrams\",n=2, drop=T) |&gt; \n  count(word,id, sort=T)\n\n## try this `bind_tf_idf` function\nword_distinct = abstract_words |&gt; \n  bind_tf_idf(word,id, n)\n\n## convert bi-graph into network graph\npharases = word_distinct |&gt; \n  arrange(desc(tf_idf)) |&gt; \n  tidyr::separate(word, into=c(\"word1\",\"word2\"), sep=\" \") |&gt; \n  anti_join(stop_words,c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  filter(if_all(c(word1,word2), ~!stringr::str_detect(.x, \"\\\\d+\"))) |&gt; \n  mutate(pharase = paste(word1, word2)) |&gt; \n  filter(!word1 |&gt; str_detect(\"^_\"))\n\nrequire(tidygraph)\n\n## try find graphical centroid of biggest graph\ncatch_pharase = pharases |&gt; \n  group_by(word1, word2) |&gt; \n  summarise(\n    occurance = n()\n  ) |&gt; arrange(-occurance)\n\n\n`summarise()` has grouped output by 'word1'. You can override using the\n`.groups` argument.\n\n\nCode\ncatch_pharase\n\n\n\n  \n\n\n\nbi-graph truns out to be very useful…. you gets to understand\nEigen Centrality Run every node through to\n\n\nCode\nword_graph = catch_pharase |&gt; \n  as_tbl_graph() |&gt; \n  morph(to_components) |&gt; \n  as_tibble() |&gt; \n  mutate(dim = map_int(graph, ~length(.x))) |&gt; \n  arrange(desc(dim))\n\nbiggest_g=word_graph |&gt; purrr::pluck(\"graph\",1)\n\n## eigen centrality\n\ncentroid_score = biggest_g |&gt; eigen_centrality() |&gt; pluck(\"vector\")\ncentroid = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  convert(to_local_neighborhood, centroid, 1) |&gt;  \n  mutate(score = centroid_score[name]) |&gt;\n  convert(to_subgraph, score &gt; quantile(score, 0.99)) |&gt; \n  activate(edges) |&gt; \n  arrange(occurance) |&gt; \n  filter(occurance &gt; 10) |&gt; \n  ggraph(layout=\"gem\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\",trans=\"log\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  ggtitle(\"By Eigenvector Centrality\") +\n  theme_void()\n\n\nSubsetting by nodes\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), color =\n\"cyan4\", : Ignoring unknown parameters: `trans`\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nAnygraphical based algorithmn is interesting here.\n\n\nCode\n## harmonic_centrality\ncache_file = \"cache/04-betweeness_score.RDS\"\nif(interactive()) {\n  betweeness_score = biggest_g |&gt; \n  activate(edges) |&gt;\n  betweenness(weights=E(biggest_g)$\"occurance\")\n  saveRDS(centroid_score, cache_file)\n} else {\n  betweeness_score=readRDS(cache_file)\n}\nbtw_centre = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  # convert(to_local_neighborhood, btw_centre) |&gt; \n  mutate(score = betweeness_score[name]) |&gt; \n  convert(to_subgraph, score &gt; quantile(score, 0.95)) |&gt; \n  arrange(desc(score)) |&gt; \n  filter(row_number() &lt; 50) |&gt; \n  activate(edges) |&gt; \n  filter(occurance &gt; 100) |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  theme_void() +\n  ggtitle(\"By Edge Betweeness, visualise top 0.01 % \")\n\n\nSubsetting by nodes\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\n\n\n\n\n\n\n\nThis results actually makes sense if you are looking at quot is a wild card that can get about a lot of things.\nUp to interpretation.\nOkay other then key terms. Seems like the edge betweeness is good for point out adjusant words rather than identify patterns."
  },
  {
    "objectID": "04.html#suggestions",
    "href": "04.html#suggestions",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Suggestions",
    "text": "Suggestions\nIt maybe usefull to use n-gram to extract combined terms from title and bind this back into abstract n-graph. Then when we look at graph centrality again we remove any waild card.\nPharases may high high frequency of occurance but this does not means that they have a higher “graphic degree”."
  },
  {
    "objectID": "04.html#follow-up",
    "href": "04.html#follow-up",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Follow up",
    "text": "Follow up\nThe better way is actually filter a few top occuring terms.\n\n\nCode\ncatch_pharase |&gt; \n  filter(occurance &gt; 100) |&gt; \n  as_tbl_graph() |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),trans=\"log\",edge_colour =\"cyan4\") +\n  geom_node_text(aes(label=name),repel = T) + \n  geom_node_point(size=5) +\n  theme_void()\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), trans =\n\"log\", : Ignoring unknown parameters: `trans`"
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\ncode in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n1155\nISCF\n103700\nFIVE AI LIMITED\nNaN\nCollaborative R&D\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n31/03/2020\n12777085.0\nNaN\nSouth West\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n3CF9E6DD-F50F-4EAB-B493-64F907FF3C4D\n082B386F-6FF5-40C0-B99E-1B7F867318BE\n47046C28-E30D-4365-89FC-4CB11AE246A0\nNaN\n\n\n3981\nSTFC\nPP/F000146/1\nUniversity of Manchester\nPhysics and Astronomy\nResearch Grant\nO'Brien\nTimothy\nNaN\nNaN\nNaN\n...\n08/04/2011\n103763.0\nNaN\nNorth West\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nA8E80E2A-2BD4-4501-BEB1-C79A3E258544\n0FA7FA45-02C9-433E-A750-12A20140208F\nBDFEBC9A-F2A6-4D58-9440-8482180BEA69\nA7569121-A036-47B1-8D6B-4A527D14B06A\n\n\n2900\nEPSRC\nEP/S024050/1\nUniversity of Oxford\nEngineering Science\nTraining Grant\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n31/03/2028\n5532025.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n51FF873D-51B5-4191-B2A5-03F8760F0D78\n798CB33D-C79E-4578-83F2-72606407192C\n47649064-FCD1-4D7A-A5AA-31B69B9BDFFC\nNaN\n\n\n\n\n3 rows × 25 columns\n\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n23\nA5F04B81-7AF4-40FB-8145-7CAA4D0BB0D9\nHORIZON-CL5-2021-D5-05-01 - BeCoM - Better Con...\nAviation contributes to about 5% of the total ...\nNone\nNone\n\n\n11\n9CEC52F6-697B-4275-8C3B-E7C9127DB389\nPrison Reading Group Development\nThe project aims to increase the provision of ...\nNone\nThe immediate beneficiaries will be the partic...\n\n\n21\nB07BABCF-29BB-4321-9500-7E99EDA70F22\nACTION on cancer\nDeath from cancer is typically both slow and p...\nNone\nImpact Summary\\nThe proposal is an ambitious o...\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n1\nE3D0F535-D1B5-424D-9204-290AC82FF2A4\nACTION on Cancer\nAbstracts are not currently available in GtR f...\nNone\nNone\n\n\n21\n45767C16-8832-4D5C-9AEE-05B1CFD05070\nSmart Sensing for Structural Health Monitoring...\nThe cost of unplanned maintenance for structur...\nNone\nNone\n\n\n7\nEBE45FBD-1A65-4EC2-87C7-26E2B731A5EC\nThe White Rose Grid e-Science Centre\nThis proposal is in support of the continuatio...\nNone\nNone\n\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n5132\nEPSRC\nEP/W027879/1\nCardiff University\nSch of Engineering\nResearch Grant\nAhmadian\nReza\nNaN\nNaN\nNaN\n...\n31/12/2024\n1158210.0\nNaN\nUnknown\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nF4DD1899-669C-480F-899E-E78A4DE5E37F\n798CB33D-C79E-4578-83F2-72606407192C\n052DE517-F298-4593-869D-03FE4A87442A\n3AAE1916-A49F-4391-9297-5B8B03A1F155\n\n\n4920\nNERC\nNE/Y001028/1\nUniversity of St Andrews\nEarth and Environmental Sciences\nResearch Grant\nBurke\nAndrea\nNaN\nhttp://orcid.org/0000-0002-3754-1498\nNaN\n...\n31/10/2026\n797040.0\nNaN\nScotland\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n37AE839D-E152-4CC2-997C-C75FB8FADDD7\n8A03ED41-E67D-4F4A-B5DD-AAFB272B6471\n09D41C19-6E04-4937-902C-C3CD52E0683F\nA0A95DBE-E790-4D9E-BB5B-9D01F83F1EB6\n\n\n602\nEPSRC\nEP/X015661/1\nImperial College London\nPhysics\nResearch Grant\nBranford\nWilliam\nNaN\nNaN\nNaN\n...\n31/03/2026\n859944.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nCFCF4C23-F603-44E1-92B7-375C3AB98ABE\n798CB33D-C79E-4578-83F2-72606407192C\n4FC4CBFD-9E7C-4518-BB16-852553236FE1\n2717DEC3-FCBB-4B93-B7DF-2267FFB40B6E\n\n\n\n\n3 rows × 25 columns",
    "crumbs": [
      "Home",
      "Getting Data",
      "Getting Data"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "Trend and Elite Project",
    "section": "",
    "text": "setup and load data\nrequire(readr)\nrequire(arrow)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(tidyr)\nrequire(scales)\nrequire(plotly)\nrequire(ggtext)\n\nsource(\"set_graphic.R\")\ncsv_export=read_csv('data/projectsearch-1709481069771.csv')\napi_export=arrow::read_parquet('data/gtr.parquet')\n## clean parse date\ngtr = csv_export |&gt; \n  mutate(across(c(StartDate, EndDate), ~as.Date(.x, '%d/%m/%Y')))\n\n\n\nExplore Trend\n\nAbsolute NumberAward\n\n\n\n\nCode\ngtr |&gt; plot_freq(n()) + \n  ylab(\"Number of Project\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngtr |&gt; \n  plot_freq(sum(AwardPounds,na.rm=T)) + \n  ylab(\"Total Award Sumed\") +\n  labs(caption=\"This plot may be useful if funding were allocated all at the start of the year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Elite (Most Expensive) Projects\n\n\nCode\nrequire(htmlwidgets)\n\n\nLoading required package: htmlwidgets\n\n\nCode\n## parameter\nSNOBYNESS=0.02 ## this is top n most expensive\nPRICE_FORMULAR=quo(\n  AwardPounds/as.numeric(duration, units='days') ## this is how you calculate expensiveness\n) \n\n## calculate award per year \nprice_gtr = gtr |&gt; \n  mutate(duration = EndDate - StartDate) |&gt;\n  mutate(duration_d = as.numeric(duration, units='days')) |&gt; \n  mutate(price = !! PRICE_FORMULAR) |&gt; \n  arrange(desc(price)) |&gt; \n  mutate(expensive_rank = row_number()) |&gt;\n  filter(expensive_rank &lt;= quantile(expensive_rank, SNOBYNESS))\n\ng &lt;- price_gtr |&gt;\n  ggplot(aes(y = price, label = LeadROName, label1= Title, label2 = ProjectReference)) + \n  geom_linerange(aes(xmin = StartDate, xmax = EndDate, y = price, alpha = duration_d), \n                 color = \"darkgrey\") +\n  geom_point(aes(x = StartDate), color = \"blue\") + \n  geom_point(aes(x = EndDate), color = \"navy\") +\n  ggtitle(glue::glue(\"Here are the top {SNOBYNESS * 100}% most expensive AI projects\")) +\n  scale_alpha_continuous(trans='log')\n\nggplotly(g) |&gt;\n  style(traces = 2, text = paste(price_gtr$StartDate,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 1, text = paste( price_gtr$PIFirstName, price_gtr$PISurname,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 3, customdata = price_gtr$GTRProjectUrl, text = paste(price_gtr$EndDate,\"\\n\", price_gtr$LeadROName, \"\\n\",price_gtr$Title)) |&gt;\n  onRender(\"\n    function(el) { \n      el.on('plotly_click', function(d) { \n        var url = d.points[0].customdata;\n        window.open(url);\n      });\n    }\n  \") #set up click event that open URL",
    "crumbs": [
      "Home",
      "Other Findings",
      "Trend and Elite Project"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Other Findings",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#overview",
    "href": "03.html#overview",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Other Findings",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#cleaning",
    "href": "03.html#cleaning",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Cleaning",
    "text": "Cleaning\noverview\n\nMajority of data and link can join\nDescription and Title contains duplication:\n\nThis is due to project migration when person of interests changes occupation;\nWhen this happens the ProjectReference will ends with a slash.\n\n\n\nHow well does the two data set join\n\n\nvalidation\nJOIN_META_KEY = c(\"id\"=\"ProjectId\")\n\ndesc_key = gtr_desc[[names(JOIN_META_KEY)]]\nmeta_key = gtr_meta[[JOIN_META_KEY[[1]]]]\n\nstopifnot(\n  !any(duplicated(desc_key)),\n  !any(duplicated(meta_key))\n)\ncan_join = intersect(desc_key, meta_key)\ndesc_cant_join = setdiff(desc_key, meta_key)\nmeta_cant_join= setdiff(meta_key, desc_key)\n\npcg = round(length(can_join) / nrow(gtr_desc) * 100)\n\nmessage(glue::glue(\n  \"{ pcg } % of description can find matching porject id in the csv export;\\n\",\n  \"\\nNumbers breakdown:\\n\",\n  \"\\t - {length(can_join)} can join;\\n\",\n  \"\\t - {length(desc_cant_join)} description will be taken out;\\n\",\n  \"\\t - {length(meta_cant_join)} from csv file will be taken out;\\n\"\n))\n\n\n86 % of description can find matching porject id in the csv export;\n\nNumbers breakdown:\n     - 4453 can join;\n     - 722 description will be taken out;\n     - 722 from csv file will be taken out;\n\n\n\n\nLinked/Duplicated project\n\n\npartial project or migrated project\n## migrated project consist of this pattern\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  relocate(ProjectReference, FundingOrgName, LeadROName, ProjectId, \n           is_partial,project_ref,part,occurance\n           )\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain null!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## a lot of these are false duplicate? or have they simply not been included in \n## the project? \ngtr_pj |&gt;\n  group_by(occurance) |&gt; \n  summarise(n_projects=n())\n\n\n\n  \n\n\n\npartial project or migrated project\n## actually majority of these project will false alert\n\n\n\n\nexamples of inheritance projects\nsmp=sample(1:94,1)# 9/3 + 183/2 \ngtr_pj |&gt; \n  filter(occurance !=1) |&gt; \n  group_by(project_ref) |&gt;\n  filter(cur_group_id() == smp) |&gt; \n  left_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\"))\n\n\n\n  \n\n\n\nAlthough the project will have different id. The content is actually the same. So it is important these are taken out before input for analysis.\nFor the analysis, we only need to take the first project which end with /1\nWe will need to know which rows to keep which rows to delete\n\n\nkeep only the first project\nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, ProjectId) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  inner_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\")) |&gt; \n  ungroup()\n\n\n\n\ncheck further duplication\nrepeated_text = unique_prj |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\nrepeated_text |&gt; \n  select(ProjectReference,title,abstractText, ProjectId)\n\n\n\n  \n\n\n\nEven with project dropped, most of these still have little descriptions",
    "crumbs": [
      "Home",
      "Other Findings",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#word-count-and-keyword-summary",
    "href": "03.html#word-count-and-keyword-summary",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Word Count and Keyword Summary",
    "text": "Word Count and Keyword Summary\n\n\nCode\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"ProjectId\") |&gt; \n  mutate(year = lubridate::year(StartDate))\n\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\nrank_words = function(word_token, group_col) {\n  word_token |&gt;\n    group_by({{group_col}}) |&gt; \n    dplyr::arrange(desc({{group_col}}),desc(n_prj)) |&gt;\n    dplyr::mutate(rank = row_number()) |&gt;\n    ungroup()\n}\n\nplot_top_n = function(word_token, n, group_col) {\n  top_n = rank_words(word_token, {{group_col}}) |&gt; \n    filter(rank &lt;= n)\n  top_n |&gt; \n    arrange({{group_col}},rank) |&gt;\n    ggplot(aes(x=n_prj, y = reorder_within(word, n_prj, {{group_col}} ))) +\n    geom_col(fill=\"midnightblue\") + \n    scale_y_reordered() +\n    facet_wrap(~ eval(enquo(group_col)), scales=\"free_y\") +\n    xlab(\"number of porject\") + ylab(\"word\")\n}\n\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n\nabstrc_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(abstractText, year)\n\n\n\n\nCode\ntitle_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nabstrc_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## track progression of top n over years\nplt_style_change = function(text_by_year, \n                            fav_words=c(\"artificial-intelligence\",\"ai\",\"health\", \"machine-learning\",\"learning\"),\n                            limit_rank = 10,\n                            weight = c(\"pcg\", \"absolute\")\n                            ) {\n  \n  if(weight[1] == \"absolute\") {\n    weightQ = quo(n_prj)\n  } else if (weight[1] == \"pcg\") {\n    weightQ = quo(pcg_prj)\n  } else {\n    stop()\n  }\n  \n  word_ranked = text_by_year |&gt; \n    # filter(year &gt; 2010) |&gt; \n    rank_words(year) |&gt; \n    group_by(year) |&gt; \n    mutate(pcg_prj = n_prj/sum(n_prj)) |&gt; \n    ungroup()\n  \n  top_n = word_ranked |&gt;\n    filter(rank &lt;= limit_rank) |&gt; \n    ## join back words that were crop out in top n ranking\n    select(-n_prj, -rank,-pcg_prj)\n  \n  top_n_complete =\n    tidyr::expand(top_n, year, word) |&gt; \n    left_join(word_ranked, c(\"year\",\"word\")) |&gt; \n    mutate(across(c(n_prj, pcg_prj), ~coalesce(.x,0))) |&gt; \n    mutate(rank = coalesce(rank, max(word_ranked$rank + 1)))\n    \n  top_n_complete |&gt; \n    filter(year &gt;= 2010 & year &lt;= 2023) |&gt; \n    ggplot(aes(x=year,y= !! weightQ, color=word)) + \n    geom_line() +\n    geom_point() +\n    theme(legend.position = \"none\", axis.text.y.right = element_text(size = 20)) + \n    # scale_y_reverse() +\n    gghighlight(word %in% fav_words,\n                unhighlighted_params=list(alpha=0.2),\n                line_label_type = \"text_path\"#\"sec_axis\"\n                ) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_minimal()\n}\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight = \"pcg\"\n  ) +\n  ylab(\"propotion of project\") +\n  ggtitle(\"Keywords mention in project title\",\n          stringr::str_wrap(glue::glue(\"The mention of 'health' seen a sharp raise in 2020. \",\n          \"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'\"),80)\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight=\"absolute\"\n  ) +\n  ylab(\"absolute number of project\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight = \"pcg\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight=\"absolute\"\n  )",
    "crumbs": [
      "Home",
      "Other Findings",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "Appendix - Experiment of Zipf’s Law",
    "section": "",
    "text": "Code\nrequire(janeaustenr)\n\n\nLoading required package: janeaustenr\n\n\nCode\nrequire(tidytext)\n\n\nLoading required package: tidytext\n\n\nCode\nrequire(tidyverse)\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3147 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "10.html#word-frequency-experiment---zipfs-law",
    "href": "10.html#word-frequency-experiment---zipfs-law",
    "title": "Appendix - Experiment of Zipf’s Law",
    "section": "",
    "text": "Code\nrequire(janeaustenr)\n\n\nLoading required package: janeaustenr\n\n\nCode\nrequire(tidytext)\n\n\nLoading required package: tidytext\n\n\nCode\nrequire(tidyverse)\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3147 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "Code\nlibrary(topicmodels)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)",
    "crumbs": [
      "Home",
      "Topic Model",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#overview",
    "href": "05.html#overview",
    "title": "Topic Modeling",
    "section": "Overview",
    "text": "Overview\nTopic modeling is usefully for categorize long text into themes without reading full text.\n\nLoad Data\n\n\nset up and load data\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")\n\n\n\n\nReview Abstract Word Count of Abstract\n\n\nCode\nanalysis_prj |&gt; \n  count(ProjectCategory,sort=T)\n\n\n\n  \n\n\n\n\n\nCode\n## word count histogram\nabstract_words = analysis_prj |&gt; \n  unnest_tokens(word,abstractText) |&gt; \n  anti_join(stop_words)\n\n\nJoining with `by = join_by(word)`\n\n\nCode\nabstract_words |&gt; \n  count(id,ProjectCategory) |&gt; \n  ggplot() +\n  geom_histogram( fill=\"midnightblue\",aes(x=n) ) +\n  theme_minimal() +\n  ggtitle(\"Abstract Length\") +\n  facet_wrap(~ ProjectCategory)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nResearch Grant and Studentship have “rich” abstract we can extract on. You probably what to filter down to Research Grant before modeling",
    "crumbs": [
      "Home",
      "Topic Model",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#topic-model",
    "href": "05.html#topic-model",
    "title": "Topic Modeling",
    "section": "Topic Model",
    "text": "Topic Model\n\nTopic Modeling using LDA\nKey matrix from topic models are:\n\nGemma: is proportion of topics for each document\nBeta: is weight of words for each topic.\n\n\n\ncode for creating topic modelings\nabstract_word_count = abstract_words |&gt; \n  count(word,id,sort=T)\n\n## this is very expensive process so cached this result for saving rendering time\nif(interactive()) {\n  gtr_dtm = abstract_word_count |&gt; \n    cast_dtm(id,word,n)\n  topics = gtr_dtm |&gt; topicmodels::LDA(k=10)\n  saveRDS(topics, \"cache/05-topics.rds\")\n} else {\n  topics=readRDS(\"cache/05-topics.rds\")\n}\n\n\n\n\ncode for creating topic modelings\ntopics |&gt; \n  tidy(\"beta\") |&gt; \n  group_by(topic) |&gt; \n  slice_max(beta,n=15) |&gt; \n  ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n  geom_col() + \n  scale_y_reordered() +\n  facet_wrap(~ topic,scales=\"free_y\") +\n  theme_minimal() +\n  xlab(\"beta (higher indicate more relevant to topic)\") +\n  ylab(\"term\") +\n  ggtitle(\"LDA model output result (beta)\")\n\n\n\n\n\n\n\n\n\nThis is a typical graph used to visualize LDA output. beta indicate importance of words to extracted topic.\nHowever, it is still not easy to tell what these topic actually are. To explore topic, n-gram to link most frequently linked words, which will give us a picture of how “these key words” links together.\n\n\nCode\n## examples of top 10 topics.\ntop10_gamma = topics |&gt; \n  tidy(\"gamma\") |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma) |&gt; \n  rename(id=document)\ntop10_gamma |&gt; left_join(gtr_desc,by=\"id\")\n\n\n\n  \n\n\n\n\n\nTopic against Project Category\n\n\nCode\n## gamma score over the years\n## top gamma's n-gram\ntopic_binded = topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntopic_binded |&gt;\n  select(topic, ProjectCategory) |&gt;\n  mutate(topic_chr=fct_inorder(as.character(topic)) ) |&gt; \n  complete(topic_chr,ProjectCategory) |&gt; \n  ggplot() + \n  geom_bin2d(aes(x=topic_chr, y = ProjectCategory)) +\n  theme_minimal() +\n  scale_fill_viridis_c(option = \"F\",trans=\"sqrt\") +\n  ggtitle(\"Topic Against Porject Category\") +\n  coord_equal()\n\n\n\n\n\n\n\n\n\n\n\nuseful function create specific plot\n## For compute bigram ---------------------------------------------\n#' Calculate bigram given document and field\n#' @param doc_df document number\n#' @param field column of text field\n#' @param doc_id id indicating text comes from in fact\ncompute_bi_gram = function(doc_df,field) {\n  .gvars = group_vars(doc_df)\n  if(length(.gvars)==0) {\n    Gvars=quo(NULL)\n  } else {\n    Gvars=quo({any_of(.gvars)})\n  }\n  doc_df |&gt; \n    # select({{field}},!!Gvars ) |&gt; \n    unnest_tokens(pharse,{{field}},'ngrams',n=2) |&gt; \n    separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n    ## clear up stopwords\n    anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n    anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n    filter(!is.na(word1) & !is.na(word2))\n}\n#' customer counting function that also\ncount_bi_gram = function(bi_gram,...) {\n  bi_gram |&gt; \n    group_by(word1,word2, .add=T) |&gt; \n    group_by(..., .add=T) |&gt; \n    summarise(n=n(),.groups=\"drop\") |&gt; \n    arrange(desc(n))\n}\n## plot functions --------------------------------\nplot_beta=function(lda_model,ki,max_n=15) {\n  lda_model |&gt; \n    tidy(\"beta\") |&gt; \n    filter(topic == ki) |&gt; \n    group_by(topic) |&gt; \n    slice_max(beta,n=max_n) |&gt; \n    ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n    geom_col(fill=\"royalblue3\") + \n    scale_y_reordered() +\n    theme_minimal()\n}\nplot_word_graph=function(bi_gram_tokens,\n                         ki,\n                         top_n=15,\n                         model=NULL,\n                         color= \"cyan4\"\n                         ) {\n  if(!is.null(model)) {\n    beta=tidy(model,\"beta\") |&gt; \n      filter(topic == ki)\n    .add_beta = function(x) {\n      activate(x,nodes) |&gt; \n        left_join(beta, by=c(\"name\"=\"term\"))\n    }\n    .add_node_marker = function() {\n      geom_node_point(aes(size=beta))\n    }\n  } else {\n    .add_beta = \\(x) x\n    .add_node_marker = \\() geom_node_point(size=5)\n  }\n  typical_graph=bi_gram_tokens |&gt; \n    count_bi_gram(topic) |&gt; \n    relocate(word1,word2) |&gt; \n    as_tbl_graph()\n  ## filter bi-gram graph and plot\n  g = typical_graph |&gt; \n    .add_beta() |&gt; \n    activate(edges) |&gt;\n    filter(topic == ki) |&gt; \n    arrange(desc(n)) |&gt; \n    filter(row_number() &lt;= top_n) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  ## plot one single graph\n  g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),color= color) +\n    geom_node_text(aes(label=name),repel = T) +\n    .add_node_marker() +\n    theme_void() +\n    ggtitle(paste(\"topic\",ki))\n}\n\n\n\n\npre compute expensive data\n## data ----------------------\n## bind topic result \ntopic_binded=topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntypical_docs = topic_binded |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,abstractText)\n\n## compute a graphic exploration\n## combine to bi-gram this is usually the expensive one\nbi_gram_tok = topic_binded |&gt;\n  compute_bi_gram(abstractText)\n## graph -----------------------\nplot_topics = function(model,reps_tk,reps_docs,topic_ki,color= \"cyan4\") {\n  ## topic graph\n  g1 = reps_tk |&gt;\n    plot_word_graph(topic_ki,top_n = 20,model=model,color=color) +\n    ggtitle(paste0(\"Topic\", topic_ki,\"\"))\n  \n  ## beta plot\n  g2 = model |&gt; \n    plot_beta(topic_ki,max_n = 20) +\n    ggtitle(\"Beta, words most represent topics\")\n  \n  ## add example texts\n  foot_notes = reps_docs |&gt; \n    filter(topic == topic_ki) |&gt; \n    pull(abstractText) |&gt; \n    stringr::str_wrap(160) |&gt; \n    stringr::str_trunc(500)\n  g3 = ggplot() + theme_void() + geom_text(aes(x=0,y=0,label = foot_notes)) +\n    ggtitle(\"Example\")\n  ggpubr::ggarrange(g1,g2) |&gt; \n    ggpubr::ggarrange(g3,nrow=2, heights=c(8,5))\n}\n\n\nThe topic modeling are in fact picking up different types of topics. Particularly “Studentship” is extracted as topic 2, “Collaborative R & D” is almost exclusively extracted as topic 10.\n\n\nCode\nG=map(c(2,10),~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nCode\nggpubr::ggarrange(plotlist=G,nrow=2)\n\n\n\n\n\n\n\n\n\nTopic 3 seems to be describe about “machine learning”. One of the hot words. It makes no surprise that some how related to student ship.\ntopic 8, topic 9 both are very scientific. As we can see the description are very sentific and specific to particular field.\n\n\nCode\nG=map(c(8,9),~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=2)\n\n\n\n\n\n\n\n\n\ntopic 8 probably sounds like biology. topic 9 sounds like material science.\nIt also interesting to see that although term “artificial intelligence” is frequently linked together, the “beta” score is actually low.\n\n\nTopic Against Institution\n\n\nCode\ntopics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.75) |&gt; \n  left_join(gtr_desc,c(\"document\"=\"id\"))\n\n\n\n  \n\n\n\n\n\nFocus on Research Grant\nThe histogram shows us that research grant is the biggest chunk. So let’s apply “research grant” by itself to see if there anything interesting.\n\n\nCode\n## filter to research project only\nres_prj = analysis_prj |&gt; \n  filter(ProjectCategory==\"Research Grant\")\n\n## research document term matrix\nres_dtm = res_prj |&gt; \n  select(id,abstractText) |&gt; \n  unnest_tokens(word,abstractText) |&gt; \n  anti_join(stop_words) |&gt; \n  count(id,word,sort=T) |&gt; \n  cast_dtm(id,word,n)\n\n\nJoining with `by = join_by(word)`\n\n\nCode\n## research lda models \nif(interactive() ) {\n  res_lda = res_dtm |&gt; topicmodels::LDA(k=10)\n  saveRDS(res_lda,\"cache/05-res_lda.rds\")\n} else {\n  res_lda=readRDS(\"cache/05-res_lda.rds\")\n}\n\n\n\n\nlda output for research fund abstract text only\nres_reps = res_lda |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.75) |&gt; # 0.75 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\n## compute three data sets\nres_betas = res_lda |&gt; tidy(\"beta\")\nres_bi_gram = res_reps |&gt; \n  compute_bi_gram(abstractText)\nres_reps_top = res_reps |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1)\n\n\n\n\nCode\n## a simple test of co-herence\nres_reps |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,LeadROName,Department,Title)\n\n\n\n  \n\n\n\nJust by looking at department of research topic, it seems that some of the topic perhaps not as coherent as we hopped. It is possible that we need less topic than 10 perhaps.\nLets do another sample:\n\n\nCode\nres_reps |&gt; \n  group_by(topic) |&gt; \n  slice_sample(n=1) |&gt; \n  select(topic,LeadROName,Department,Title)\n\n\n\n  \n\n\n\nSo actually this maybe doing alright. Lets analysis department fields.\n\n\nCode\nres_reps |&gt; \n  select(topic,LeadROName,Department,Title,id) |&gt; \n  unnest_tokens(word,Department) |&gt; \n  anti_join(stop_words,\"word\") |&gt; \n  # filter(!word %in% c(\"sch\",\"school\",'science')) |&gt; \n  count(id,word,topic,sort=T) |&gt; \n  bind_tf_idf(word,id,n) |&gt;\n  group_by(topic) |&gt; \n  slice_max(tf_idf,n=10) |&gt; \n  ggplot(aes(x = tf_idf, y = reorder_within(word, tf_idf, topic) )) + \n  geom_col(fill=\"black\") +\n  scale_y_reordered() +\n  facet_wrap(~ topic,scale=\"free_y\") +\n  theme_minimal() +\n  ggtitle(\"Topic Versus Research Insititution Deparatment\")\n\n\n\n\n\n\n\n\n\nSo 5 and 6 could be similar, 3 and 4 could stand one topic. But we cannot be too sure yet. So\n\n\nCode\n## this process also merge similarish topics\ntopic_guess = c(\n  \"AI & Public Wellfare\",\n  \"Health & Medical\",\n  \"Computer & Mathematic\",\n  \n  \"Applied AI\",\n  \"Environmental Biology\",\n  \"Cellor Biology\",\n  \n  \"AI & Public Wellfare\",\n  \"AI & Public Wellfare\",\n  \"Renewable Energy\",\n  \"Material Science\"\n)\n\n\nInterestingly, climate change is on longer on this.\nFor analysis purposes, we put number 1,7,8 all under one umberalla “AI & Public Well-fare’.\n\n\nTrending Research\n\n\nCode\ntopic_by_year = res_lda |&gt; \n  tidy(\"gamma\") |&gt; \n  left_join(gtr_meta,by=c(\"document\"=\"id\")) |&gt; \n  mutate(year = year(StartDate)) |&gt; \n  mutate(topic_chr = topic_guess[topic]) |&gt; \n  group_by(topic_chr,year) |&gt;\n  summarise(topic_value = sum(gamma * AwardPounds))\n\n\n`summarise()` has grouped output by 'topic_chr'. You can override using the\n`.groups` argument.\n\n\nCode\nCutoff_Year=2010\n\nfast_growing = topic_by_year |&gt; \n  filter(year &gt;= Cutoff_Year) |&gt; \n  group_by(topic_chr) |&gt; \n  arrange(topic_chr,year) |&gt; \n  mutate(growth = (topic_value - lag(topic_value))/lag(topic_value) ) |&gt; \n  summarise(avg_growth = mean(growth,na.rm=T)) |&gt; \n  slice_max(avg_growth, n=2)\n\nlatest_top = topic_by_year |&gt; \n  slice_max(year,n=1) |&gt; \n  ungroup() |&gt; \n  slice_max(topic_value,n=1)\n  \ntopic_by_year |&gt;\n  ggplot(aes(x=year,y=topic_value)) +\n  geom_line(aes(color=topic_chr,fill=topic_chr)) + \n  theme_minimal() +\n  ylab(\"topic multiply award\") +\n  gghighlight::gghighlight(\n      (  topic_chr %in% fast_growing$topic_chr \n       | topic_chr %in% latest_top$topic_chr )\n    & year &gt;= Cutoff_Year,\n    line_label_type=\"sec_axis\"\n  ) +\n  scale_color_brewer(palette=\"Set2\") +\n  geom_vline(aes(xintercept=Cutoff_Year),linetype=\"dashed\") + \n  ggtitle(sprintf(\"Growing Topics Since %s\", Cutoff_Year))\n\n\nWarning in geom_line(aes(color = topic_chr, fill = topic_chr)): Ignoring\nunknown aesthetics: fill\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: topic_chr",
    "crumbs": [
      "Home",
      "Topic Model",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#appendix",
    "href": "05.html#appendix",
    "title": "Topic Modeling",
    "section": "Appendix",
    "text": "Appendix\n\nDeveloping a Visualization for Topic Modeling**\nI found bi-gram graph compensate traditional beta count graph. Instead of “reading tea leafs”, you can try read along the edge.\n\n\ndraft script\nbi_gram = topic_binded |&gt; \n  select(id,abstractText,topic) |&gt; \n  unnest_tokens(pharse,abstractText,'ngrams',n=2) |&gt; \n  separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n  anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  count(topic,word1,word2,sort=T)\n\ntypical_graph = bi_gram |&gt; \n  filter(!is.na(word1) & !is.na(word2)) |&gt;\n  group_by(topic) |&gt;\n  slice_max(n,n=20) |&gt; \n  relocate(word1,word2) |&gt; \n  as_tbl_graph()\n\nG = list()\nfor (i in c(2,10,8,9)) {\n  g = typical_graph |&gt; \n    activate(edges) |&gt;\n    filter(topic == i) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  if(length(g)==0) next()\n  plot_g = g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),trans=\"log\",color=\"royalblue\") +\n    geom_node_text(aes(label=name),repel = T) +\n    geom_node_point(size=5) +\n    theme_void() +\n    ggtitle(paste(\"topic\",i))\n  G = append(G, list(plot_g))\n}\nggpubr::ggarrange(plotlist=G,ncol=2, nrow=2)\n\n\n\n\nList of all Topics From the Frist Extraction\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)\n\n\n\n\n\n\n\n\n\n\n\nList of All Topic Extraction for Research Grant Only\nThe topic read\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=res_lda,\n                        reps_tk=res_bi_gram, \n                        reps_docs=res_reps_top,\n                        topic_ki=.x,\n                        color= \"coral2\"\n                        ))\nggpubr::ggarrange(plotlist=G,nrow=10)",
    "crumbs": [
      "Home",
      "Topic Model",
      "Topic Modeling"
    ]
  }
]