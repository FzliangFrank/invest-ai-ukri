[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Invest in AI Executive Summary",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)"
  },
  {
    "objectID": "index.html#intro",
    "href": "index.html#intro",
    "title": "Invest in AI Executive Summary",
    "section": "Intro",
    "text": "Intro\nWelcome!\n\n\n\nData Source?\n\n\nData usage - What’s used? What has been taken out?\n\n\n\n\n\nOkay, now what does the word look like\n\n\n\nTopic Modeling"
  },
  {
    "objectID": "index.html#other-findings",
    "href": "index.html#other-findings",
    "title": "Invest in AI Executive Summary",
    "section": "Other Findings",
    "text": "Other Findings\n\nWord Trend and Time Event\n\n\nResearcher Migration"
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(igraph)\nlibrary(tidytext)\nset up and load data\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n# gtr_pj |&gt;\n#   group_by(occurance) |&gt; \n#   summarise(n_projects=n())\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")"
  },
  {
    "objectID": "04.html#explore-n-gram",
    "href": "04.html#explore-n-gram",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "explore n-gram",
    "text": "explore n-gram\n\n\nCode\n## break into bi-grame\nabstract_words = analysis_prj |&gt;\n  unnest_tokens(word, abstractText, \"ngrams\",n=2, drop=T) |&gt; \n  count(word,id, sort=T)\n\n## try this `bind_tf_idf` function\nword_distinct = abstract_words |&gt; \n  bind_tf_idf(word,id, n)\n\n## convert bi-graph into network graph\npharases = word_distinct |&gt; \n  arrange(desc(tf_idf)) |&gt; \n  tidyr::separate(word, into=c(\"word1\",\"word2\"), sep=\" \") |&gt; \n  anti_join(stop_words,c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  filter(if_all(c(word1,word2), ~!stringr::str_detect(.x, \"\\\\d+\"))) |&gt; \n  mutate(pharase = paste(word1, word2)) |&gt; \n  filter(!word1 |&gt; str_detect(\"^_\"))\n\nrequire(tidygraph)\n\n## try find graphical centroid of biggest graph\ncatch_pharase = pharases |&gt; \n  group_by(word1, word2) |&gt; \n  summarise(\n    occurance = n()\n  ) |&gt; arrange(-occurance)\n\n\n`summarise()` has grouped output by 'word1'. You can override using the\n`.groups` argument.\n\n\nCode\ncatch_pharase\n\n\n\n  \n\n\n\nbi-graph truns out to be very useful…. you gets to understand\nEigen Centrality Run every node through to\n\n\nCode\nword_graph = catch_pharase |&gt; \n  as_tbl_graph() |&gt; \n  morph(to_components) |&gt; \n  as_tibble() |&gt; \n  mutate(dim = map_int(graph, ~length(.x))) |&gt; \n  arrange(desc(dim))\n\nbiggest_g=word_graph |&gt; purrr::pluck(\"graph\",1)\n\n## eigen centrality\n\ncentroid_score = biggest_g |&gt; eigen_centrality() |&gt; pluck(\"vector\")\ncentroid = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  convert(to_local_neighborhood, centroid, 1) |&gt;  \n  mutate(score = centroid_score[name]) |&gt;\n  convert(to_subgraph, score &gt; quantile(score, 0.99)) |&gt; \n  activate(edges) |&gt; \n  arrange(occurance) |&gt; \n  filter(occurance &gt; 10) |&gt; \n  ggraph(layout=\"gem\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\",trans=\"log\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  ggtitle(\"By Eigenvector Centrality\") +\n  theme_void()\n\n\nSubsetting by nodes\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), color =\n\"cyan4\", : Ignoring unknown parameters: `trans`\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nAnygraphical based algorithmn is interesting here.\n\n\nCode\n## harmonic_centrality\ncache_file = \"cache/04-betweeness_score.RDS\"\nif(interactive()) {\n  betweeness_score = biggest_g |&gt; \n  activate(edges) |&gt;\n  betweenness(weights=E(biggest_g)$\"occurance\")\n  saveRDS(centroid_score, cache_file)\n} else {\n  betweeness_score=readRDS(cache_file)\n}\nbtw_centre = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  # convert(to_local_neighborhood, btw_centre) |&gt; \n  mutate(score = betweeness_score[name]) |&gt; \n  convert(to_subgraph, score &gt; quantile(score, 0.95)) |&gt; \n  arrange(desc(score)) |&gt; \n  filter(row_number() &lt; 50) |&gt; \n  activate(edges) |&gt; \n  filter(occurance &gt; 100) |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  theme_void() +\n  ggtitle(\"By Edge Betweeness, visualise top 0.01 % \")\n\n\nSubsetting by nodes\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\n\n\n\n\n\n\n\nThis results actually makes sense if you are looking at quot is a wild card that can get about a lot of things.\nUp to interpretation.\nOkay other then key terms. Seems like the edge betweeness is good for point out adjusant words rather than identify patterns."
  },
  {
    "objectID": "04.html#suggestions",
    "href": "04.html#suggestions",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Suggestions",
    "text": "Suggestions\nIt maybe usefull to use n-gram to extract combined terms from title and bind this back into abstract n-graph. Then when we look at graph centrality again we remove any waild card.\nPharases may high high frequency of occurance but this does not means that they have a higher “graphic degree”."
  },
  {
    "objectID": "04.html#follow-up",
    "href": "04.html#follow-up",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Follow up",
    "text": "Follow up\nThe better way is actually filter a few top occuring terms.\n\n\nCode\ncatch_pharase |&gt; \n  filter(occurance &gt; 100) |&gt; \n  as_tbl_graph() |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),trans=\"log\",edge_colour =\"cyan4\") +\n  geom_node_text(aes(label=name),repel = T) + \n  geom_node_point(size=5) +\n  theme_void()\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), trans =\n\"log\", : Ignoring unknown parameters: `trans`"
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\nnote: code in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\n\nlibrary and setup\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n137\nInnovate UK\n104505\nVIBTEK LTD\nNaN\nCollaborative R&D\nGill\nMatthew\nNaN\nNaN\nNaN\n...\n31/12/2020\n1471871.0\nNaN\nYorkshire and The Humber\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nDB2358DF-30B7-4440-8646-3D46D17AFB86\n1308DB9B-A8CB-43DC-8754-A44B2F23F8E8\n08A3870D-1DA7-4FA9-A590-9DAD05BCC5DC\nECD8C090-73EF-4809-8C09-2518692BB662\n\n\n2850\nEPSRC\n2599504\nUniversity of Liverpool\nCardiovascular and Metabolic Medicine\nStudentship\nNaN\nNaN\nNaN\nNaN\nHernandez\n...\n31/03/2025\n0.0\nNaN\nNorth West\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n7E2E07EE-1351-4F2F-986D-D030E14BBDC0\n798CB33D-C79E-4578-83F2-72606407192C\nCE1E540C-192A-4906-A744-796BA2B220A8\nNaN\n\n\n2908\nNERC\nNE/X008967/1\nUniversity of East Anglia\nEnvironmental Sciences\nResearch Grant\nTurner\nKerry\nNaN\nNaN\nNaN\n...\n31/03/2027\n269352.0\nNaN\nEast of England\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nE44D3DB0-4467-4D12-96BA-63C1A436421B\n8A03ED41-E67D-4F4A-B5DD-AAFB272B6471\nC131DAC0-7FB4-4E59-B253-C6557AC44486\n25E545B9-F32C-4EF2-8C30-79C55197E9A1\n\n\n\n\n3 rows × 25 columns\n\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n\ngathering project description\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\n\nSkipping intractive\n\n\n\n\nSample Data\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n22\nED9C3219-829C-4227-A180-E5B8A7C8DD4E\nQuantifying and Monitoring Potential Ecosystem...\nProposal to Research Councils Energy Program: ...\nNone\nNone\n\n\n13\nE6406D09-B809-48E0-B1E9-CCDEF96F0E1C\nBrains on Board: Neuromorphic Control of Flyin...\nWhat if we could design an autonomous flying r...\nNone\nPrimary beneficiaries of this research will be...\n\n\n19\nD14E2F60-82EF-4ADA-9D47-20F2E9FF3148\nEqually Safe Online\nWe address the timely topic of online gender-b...\nNone\nNone\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n19\nCC3F2C21-16D5-4C0C-ABF6-1CAADDB43372\nCARDyAL: Cooperative Aerodynamics and Radio-ba...\nThe primary scientific objectives of the CARDy...\nNone\nNone\n\n\n13\nDCEFA771-3BF8-40DA-AE57-28E6F4447761\nNorthwest European Seasonal Weather Prediction...\nThe atmospheric circulation and jet stream (gi...\nNone\nWays in which potential beneficiaries may make...\n\n\n22\nD2E20DC1-B499-46B2-B385-AEBD9F3D7306\nAutomating electron microscopy: machine learni...\nNanoparticles are of particular interest in th...\nNone\nNone\n\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n3847\nHorizon Europe Guarantee\n10077978\nEARLHAM INSTITUTE\nNaN\nEU-Funded\nDe Vega\nJose\nNaN\nNaN\nNaN\n...\n29/02/2028\n368936.0\nNaN\nUnknown\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n97F29FEC-C65D-4856-91AA-FDDEE6D7ABE1\n240CEBFD-1052-4EAC-88DF-D88A163D61C8\n5BBF9D07-B233-4FA6-B75B-6D13E1A7FCF0\n0000DA87-17A6-4F76-847A-A36E0E843C7A\n\n\n2667\nEPSRC\nEP/X012026/1\nImperial College London\nBioengineering\nResearch Grant\nBallester\nPedro\nNaN\nNaN\nNaN\n...\n30/11/2026\n620042.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n14695273-BD16-4F90-9C5B-3CEAF339953C\n798CB33D-C79E-4578-83F2-72606407192C\n4FC4CBFD-9E7C-4518-BB16-852553236FE1\nD7A65868-B497-44FC-BE00-5F9A8A532530\n\n\n277\nAHRC\n2890594\nUniversity of Southampton\nSch of Humanities\nStudentship\nNaN\nNaN\nNaN\nNaN\nMcKee\n...\n31/05/2027\n0.0\nNaN\nSouth East\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nDA053CDD-FBA8-49DE-8982-0029552E82C1\n1291772D-DFCE-493A-AEE7-24F7EEAFE0E9\n64EDCEB7-556D-4890-9582-FD894F98C10D\nNaN\n\n\n\n\n3 rows × 25 columns",
    "crumbs": [
      "Home",
      "Getting Data",
      "Getting Data"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "Trend and Elite Project",
    "section": "",
    "text": "setup and load data\nrequire(readr)\nrequire(arrow)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(tidyr)\nrequire(scales)\nrequire(plotly)\nrequire(ggtext)\n\nsource(\"set_graphic.R\")\ncsv_export=read_csv('data/projectsearch-1709481069771.csv')\napi_export=arrow::read_parquet('data/gtr.parquet')\n## clean parse date\ngtr = csv_export |&gt; \n  mutate(across(c(StartDate, EndDate), ~as.Date(.x, '%d/%m/%Y')))\n\n\n\nExplore Trend\n\nAbsolute NumberAward\n\n\n\n\nCode\ngtr |&gt; plot_freq(n()) + \n  ylab(\"Number of Project\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngtr |&gt; \n  plot_freq(sum(AwardPounds,na.rm=T)) + \n  ylab(\"Total Award Sumed\") +\n  labs(caption=\"This plot may be useful if funding were allocated all at the start of the year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Elite (Most Expensive) Projects\n\n\nCode\nrequire(htmlwidgets)\n\n\nLoading required package: htmlwidgets\n\n\nCode\n## parameter\nSNOBYNESS=0.02 ## this is top n most expensive\nPRICE_FORMULAR=quo(\n  AwardPounds/as.numeric(duration, units='days') ## this is how you calculate expensiveness\n) \n\n## calculate award per year \nprice_gtr = gtr |&gt; \n  mutate(duration = EndDate - StartDate) |&gt;\n  mutate(duration_d = as.numeric(duration, units='days')) |&gt; \n  mutate(price = !! PRICE_FORMULAR) |&gt; \n  arrange(desc(price)) |&gt; \n  mutate(expensive_rank = row_number()) |&gt;\n  filter(expensive_rank &lt;= quantile(expensive_rank, SNOBYNESS))\n\ng &lt;- price_gtr |&gt;\n  ggplot(aes(y = price, label = LeadROName, label1= Title, label2 = ProjectReference)) + \n  geom_linerange(aes(xmin = StartDate, xmax = EndDate, y = price, alpha = duration_d), \n                 color = \"darkgrey\") +\n  geom_point(aes(x = StartDate), color = \"blue\") + \n  geom_point(aes(x = EndDate), color = \"navy\") +\n  ggtitle(glue::glue(\"Here are the top {SNOBYNESS * 100}% most expensive AI projects\")) +\n  scale_alpha_continuous(trans='log')\n\nggplotly(g) |&gt;\n  style(traces = 2, text = paste(price_gtr$StartDate,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 1, text = paste( price_gtr$PIFirstName, price_gtr$PISurname,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 3, customdata = price_gtr$GTRProjectUrl, text = paste(price_gtr$EndDate,\"\\n\", price_gtr$LeadROName, \"\\n\",price_gtr$Title)) |&gt;\n  onRender(\"\n    function(el) { \n      el.on('plotly_click', function(d) { \n        var url = d.points[0].customdata;\n        window.open(url);\n      });\n    }\n  \") #set up click event that open URL",
    "crumbs": [
      "Home",
      "Journey",
      "Trend and Elite Project"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#overview",
    "href": "03.html#overview",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "set up and load data\nrequire(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#cleaning",
    "href": "03.html#cleaning",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Cleaning",
    "text": "Cleaning\noverview\n\nMajority of data and link can join\nDescription and Title contains duplication:\n\nThis is due to project migration when person of interests changes occupation;\nWhen this happens the ProjectReference will ends with a slash.\n\n\n\nHow well does the two data set join\n\n\nvalidation\nJOIN_META_KEY = c(\"id\"=\"ProjectId\")\n\ndesc_key = gtr_desc[[names(JOIN_META_KEY)]]\nmeta_key = gtr_meta[[JOIN_META_KEY[[1]]]]\n\nstopifnot(\n  !any(duplicated(desc_key)),\n  !any(duplicated(meta_key))\n)\ncan_join = intersect(desc_key, meta_key)\ndesc_cant_join = setdiff(desc_key, meta_key)\nmeta_cant_join= setdiff(meta_key, desc_key)\n\npcg = round(length(can_join) / nrow(gtr_desc) * 100)\n\nmessage(glue::glue(\n  \"{ pcg } % of description can find matching porject id in the csv export;\\n\",\n  \"\\nNumbers breakdown:\\n\",\n  \"\\t - {length(can_join)} can join;\\n\",\n  \"\\t - {length(desc_cant_join)} description will be taken out;\\n\",\n  \"\\t - {length(meta_cant_join)} from csv file will be taken out;\\n\"\n))\n\n\n86 % of description can find matching porject id in the csv export;\n\nNumbers breakdown:\n     - 4453 can join;\n     - 722 description will be taken out;\n     - 722 from csv file will be taken out;\n\n\n\n\nLinked/Duplicated project\n\n\npartial project or migrated project\n## migrated project consist of this pattern\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  relocate(ProjectReference, FundingOrgName, LeadROName, ProjectId, \n           is_partial,project_ref,part,occurance\n           )\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain null!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## a lot of these are false duplicate? or have they simply not been included in \n## the project? \ngtr_pj |&gt;\n  group_by(occurance) |&gt; \n  summarise(n_projects=n())\n\n\n\n  \n\n\n\npartial project or migrated project\n## actually majority of these project will false alert\n\n\n\n\nexamples of inheritance projects\nsmp=sample(1:94,1)# 9/3 + 183/2 \ngtr_pj |&gt; \n  filter(occurance !=1) |&gt; \n  group_by(project_ref) |&gt;\n  filter(cur_group_id() == smp) |&gt; \n  left_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\"))\n\n\n\n  \n\n\n\nAlthough the project will have different id. The content is actually the same. So it is important these are taken out before input for analysis.\nFor the analysis, we only need to take the first project which end with /1\nWe will need to know which rows to keep which rows to delete\n\n\nkeep only the first project\nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, ProjectId) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  inner_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\")) |&gt; \n  ungroup()\n\n\n\n\ncheck further duplication\nrepeated_text = unique_prj |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\nrepeated_text |&gt; \n  select(ProjectReference,title,abstractText, ProjectId)\n\n\n\n  \n\n\n\nEven with project dropped, most of these still have little descriptions",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "03.html#word-count-and-keyword-summary",
    "href": "03.html#word-count-and-keyword-summary",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Word Count and Keyword Summary",
    "text": "Word Count and Keyword Summary\n\n\nCode\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"ProjectId\") |&gt; \n  mutate(year = lubridate::year(StartDate))\n\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\nrank_words = function(word_token, group_col) {\n  word_token |&gt;\n    group_by({{group_col}}) |&gt; \n    dplyr::arrange(desc({{group_col}}),desc(n_prj)) |&gt;\n    dplyr::mutate(rank = row_number()) |&gt;\n    ungroup()\n}\n\nplot_top_n = function(word_token, n, group_col) {\n  top_n = rank_words(word_token, {{group_col}}) |&gt; \n    filter(rank &lt;= n)\n  top_n |&gt; \n    arrange({{group_col}},rank) |&gt;\n    ggplot(aes(x=n_prj, y = reorder_within(word, n_prj, {{group_col}} ))) +\n    geom_col(fill=\"midnightblue\") + \n    scale_y_reordered() +\n    facet_wrap(~ eval(enquo(group_col)), scales=\"free_y\") +\n    xlab(\"number of porject\") + ylab(\"word\")\n}\n\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n\nabstrc_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(abstractText, year)\n\n\n\n\nCode\ntitle_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nabstrc_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## track progression of top n over years\nplt_style_change = function(text_by_year, \n                            fav_words=c(\"artificial-intelligence\",\"ai\",\"health\", \"machine-learning\",\"learning\"),\n                            limit_rank = 10,\n                            weight = c(\"pcg\", \"absolute\")\n                            ) {\n  \n  if(weight[1] == \"absolute\") {\n    weightQ = quo(n_prj)\n  } else if (weight[1] == \"pcg\") {\n    weightQ = quo(pcg_prj)\n  } else {\n    stop()\n  }\n  \n  word_ranked = text_by_year |&gt; \n    # filter(year &gt; 2010) |&gt; \n    rank_words(year) |&gt; \n    group_by(year) |&gt; \n    mutate(pcg_prj = n_prj/sum(n_prj)) |&gt; \n    ungroup()\n  \n  top_n = word_ranked |&gt;\n    filter(rank &lt;= limit_rank) |&gt; \n    ## join back words that were crop out in top n ranking\n    select(-n_prj, -rank,-pcg_prj)\n  \n  top_n_complete =\n    tidyr::expand(top_n, year, word) |&gt; \n    left_join(word_ranked, c(\"year\",\"word\")) |&gt; \n    mutate(across(c(n_prj, pcg_prj), ~coalesce(.x,0))) |&gt; \n    mutate(rank = coalesce(rank, max(word_ranked$rank + 1)))\n    \n  top_n_complete |&gt; \n    filter(year &gt;= 2010 & year &lt;= 2023) |&gt; \n    ggplot(aes(x=year,y= !! weightQ, color=word)) + \n    geom_line() +\n    geom_point() +\n    theme(legend.position = \"none\", axis.text.y.right = element_text(size = 20)) + \n    # scale_y_reverse() +\n    gghighlight(word %in% fav_words,\n                unhighlighted_params=list(alpha=0.2),\n                line_label_type = \"text_path\"#\"sec_axis\"\n                ) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_minimal()\n}\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight = \"pcg\"\n  ) +\n  ylab(\"propotion of project\") +\n  ggtitle(\"Keywords mention in project title\",\n          stringr::str_wrap(glue::glue(\"The mention of 'health' seen a sharp raise in 2020. \",\n          \"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'\"),80)\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight=\"absolute\"\n  ) +\n  ylab(\"absolute number of project\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight = \"pcg\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight=\"absolute\"\n  )",
    "crumbs": [
      "Home",
      "Journey",
      "Basic Text Ananlysis - Word Occurance"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "Appendix - Experiment of Zipf’s Law",
    "section": "",
    "text": "Code\nrequire(janeaustenr)\n\n\nLoading required package: janeaustenr\n\n\nCode\nrequire(tidytext)\n\n\nLoading required package: tidytext\n\n\nCode\nrequire(tidyverse)\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3214 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "10.html#word-frequency-experiment---zipfs-law",
    "href": "10.html#word-frequency-experiment---zipfs-law",
    "title": "Appendix - Experiment of Zipf’s Law",
    "section": "",
    "text": "Code\nrequire(janeaustenr)\n\n\nLoading required package: janeaustenr\n\n\nCode\nrequire(tidytext)\n\n\nLoading required package: tidytext\n\n\nCode\nrequire(tidyverse)\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3214 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "Explore Topic Modeling",
    "section": "",
    "text": "Code\nlibrary(topicmodels)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  },
  {
    "objectID": "05.html#appendix",
    "href": "05.html#appendix",
    "title": "Explore Topic Modeling",
    "section": "Appendix",
    "text": "Appendix\n\nDeveloping a Visualization for Topic Modeling**\nI found bi-gram graph compensate traditional beta count graph. Instead of “reading tea leafs”, you can try read along the edge.\n\n\ndraft script\nbi_gram = topic_binded |&gt; \n  select(id,abstractText,topic) |&gt; \n  unnest_tokens(pharse,abstractText,'ngrams',n=2) |&gt; \n  separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n  anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  count(topic,word1,word2,sort=T)\n\ntypical_graph = bi_gram |&gt; \n  filter(!is.na(word1) & !is.na(word2)) |&gt;\n  group_by(topic) |&gt;\n  slice_max(n,n=20) |&gt; \n  relocate(word1,word2) |&gt; \n  as_tbl_graph()\n\nG = list()\nfor (i in c(2,10,8,9)) {\n  g = typical_graph |&gt; \n    activate(edges) |&gt;\n    filter(topic == i) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  if(length(g)==0) next()\n  plot_g = g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),trans=\"log\",color=\"royalblue\") +\n    geom_node_text(aes(label=name),repel = T) +\n    geom_node_point(size=5) +\n    theme_void() +\n    ggtitle(paste(\"topic\",i))\n  G = append(G, list(plot_g))\n}\nggpubr::ggarrange(plotlist=G,ncol=2, nrow=2)\n\n\n\n\nList of all Topics From the Frist Extraction\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)\n\n\n\n\n\n\n\n\n\n\n\nList of All Topic Extraction for Research Grant Only\nThe topic read\n\n\nplot all topics trained so far\nG=map(1:10,~plot_topics(model=res_lda,\n                        reps_tk=res_bi_gram, \n                        reps_docs=res_reps_top,\n                        topic_ki=.x,\n                        color= \"coral2\"\n                        ))\nggpubr::ggarrange(plotlist=G,nrow=10)",
    "crumbs": [
      "Home",
      "Journey",
      "Explore Topic Modeling"
    ]
  }
]